[["index.html", "Análisis de encuestas de hogares con R Prefacio", " Análisis de encuestas de hogares con R Andrés Gutiérrez1, Cristian Téllez2, Stalyn Guerrero3 2024-03-11 Prefacio La versión online de este libro está licenciada bajo una Licencia Internacional de Creative Commons para compartir con atribución no comercial 4.0. Este libro es el resultado de un compendio de las experiencias internacionales prácticas adquiridas por los autores y será usado como insumo principal en un documento de CEPAL. Se agradecen los comentarios que puedan surgir, escribiendo al correo electrónico del primer autor. Experto Regional en Estadísticas Sociales - Comisión Económica para América Latina y el Caribe (CEPAL) - andres.gutierrez@cepal.org↩︎ Profesor - Universidad Santo Tomás - cristiantellez@usta.edu.co↩︎ Consultor - Comisión Económica para América Latina y el Caribe (CEPAL), guerrerostalyn@gmail.com↩︎ "],["introducción.html", "Capítulo 1 Introducción", " Capítulo 1 Introducción Las encuestas de hogares son uno de los instrumentos más importantes para hacer seguimiento a los indicadores de los Objetivos de Desarrollo Sostenible (ODS, por sus siglas) en el marco de la agenda 2030 (CEPAL 2023). Dada la importancia que tienen estos levantamientos en la política púbica de cada país, es necesario que los resultados que se obtengan de ellos sean lo más precisos y confiables posibles. En este sentido, las herramientas estadísticas utilizadas para procesar la información de las encuestas debe ser apropiada. Particularmente, el diseño de muestreo utilizado, la mayoría de veces complejo, debería ser incorporado en el análisis de los datos, dado que la forma en como se seleccionan las unidades observacionales no sigue un proceso simple y directo sobre el marco de muestreo, sino que es necesario recurrir a procesos indirectos en más de una etapa, con estratificación, aglomeración, probabilidades de selección no uniformes, etc. No incorporar el diseño de muestreo complejo en el procesamiento de las estadísticas derivadas de levantamientos complejos es un error común que el investigador debe evitar a toda costa. El objetivo principal de este documento es presentar los conceptos necesarios para hacer un análisis apropiado de los datos recolecatados en las encuestas complejas, enfocado en las dinámicas particulares de las encuestas de hogares. Particularmente, se pretender entregar una guía práctica usando el software estadístico R (R Core Team 2022) como interfaz principal. En ese sentido, todos los ejemplos, tablas y gráficos que se presentan en este libro se producen con R, y los códigos computacionales usados se presentan en el docuemtno para que el lector pueda reproducirlos y replicarlos. R es un software de código abierto, ampliaente utilizado por la comunidad estadística en todo el mundo que permite que cualquier investigador o instituto de estadística tenga acceso a él sin importar la plataforma computacional (Windows, macOS, Linux) del ordenador. Los desarrollos estadísticos están en permanente evolución, surgiendo nuevas metodologías y nuevos enfoques en el análisis de encuestas. Estos desarrollos que parten de la academia, luego son adoptados por las empresas (privadas o estatales) y las entidades estatales, las cuales satisfacen sus necesidades con la inclusión de estos desarrollos en software estadísticos, proceso que puede llevar mucho tiempo. Algunos investigadores hacen la implementación de sus metodologías en paquetes estadísticos de código abierto como R o Python. Para efectos de este documento, todo el procesamiento de las encuestas se llevará en R , puesto que tiene un mayor número de desarrollos en el procesamiento de las encuestas de hogares. Como se mencionó anteriormente, dentro del software R se disponen de múltiples librerías para el procesamiento de encuestas complejas. Estas varían dependiendo del enfoque de programación desarrollado por el autor o la necesidad que se busque suplir. Como es el objetivo de este documento y como se ha venido trabajando en los capítulos anteriores nos centraremos en las librerías survey y srvyr. A medida que se requiera, se incluirán más librerías de acuerdo a las necesidades que se presenten. El lector encontrará en este texto la siguiente estructura. En el capítulo 2 se describen los conceptos básicos de una encuesta compleja fundamentales para la correcta definición del diseño muestral en el entorno de las encuestas de hogares. En el capítulo 3 y 4 se definen los conceptos de variables aleatoria continua y discretas respectivamente en el contexto del muestreo probabilístico y, en el capítulo 5 se muestra como ajustar modelos de regresión lineal utilizando variables discretas y continuas empleando las herramientas del muestreo probabilístico. En el capítulo 6 se presentan las herramientas para ajustar modelos de regresión logística los cuales son fundamentales en el análisis de encuestas de hogares. Ahora bien, en los análisis estadísticos no solo son requeridos los modelos de regresión lineales, también, por la misma naturaleza de las variables capturadas en una encuesta de hogares, es necesario el ajuste de modelos lineales generalizados y multiniveles, estos conceptos son trabajados en el capítulo 7 y 8 respectivamente. Con las complejidades que trajo la pandemia por COVID-19, es muy evidente que la falta de respuesta en las encuestas de hogares ha aumentado de manera importante en los últimos años por lo que es necesario recurrir a técnicas de imputación para paliar el sesgo que trae la información no capturada en el trabajo de campo. Esta temática es abordada en el capítulo 9. Por último, la presentación gráfica de los resultados en una encuesta de hogares se presenta en el capítulo 10. References "],["manejando-una-base-de-encuestas-de-hogares-con-r.html", "Capítulo 2 Manejando una base de encuestas de hogares con R", " Capítulo 2 Manejando una base de encuestas de hogares con R R fue creado en 1992 en Nueva Zelanda por Ross Ihaka y Robert Gentleman. A manera introductoria, R es un software diseñado para realizar análisis estadísticos tanto sencillos como complejos. Este software ha ganado popularidad entre los investigadores puesto que su manejo es sencillo y además, es de libre uso; es decir, no requiere de ninguna licencia para su utilización. Puede descargarse desde la página web https://www.r-project.org. R es un lenguaje de programación de libre distribución, bajo Licencia GNU, y mantiene un ambiente para el cómputo estadístico y gráfico. Este software está diseñado para utilizarse en distintos sistemas operativos como, Windows, MacOS o Linux. El concepto de ambiente se enfoca en caracterizarlo como un sistema totalmente planificado y coherente, en lugar de una acumulación gradual de herramientas muy específicas y poco flexibles, como suele ser con otro software de análisis de datos. R es un lenguaje de programación y, por ende, su interfaz puede ser poco amigable para los investigadores que inician sus primeros cálculos. Una solución a este incinveniente es RStudio, el cual es un entorno de desarrollo integrado (IDE, por sus siglas en inglés), lo que significa que RStudio es un software que permite manejar y utilizar R de manera más cómoda y agradable. "],["fundamentos-básicos-de-r-y-rstudio.html", "2.1 Fundamentos básicos de R y Rstudio", " 2.1 Fundamentos básicos de R y Rstudio R es un lenguaje colaborativo, el cual permite que la comunidad vaya haciendo aportes al desarrollo de funciones dentro de paquetes o librerías. Alguna de las librerías más usadas para el análisis de bases de datos son las siguientes: dplyr es un paquete enfocado en el trabajo con bases de datos rectangulares (Wickham et al. 2023). Entre sus principales propiedades están 1) la identificación de herramientas de manipulación de datos más importantes necesarias para el análisis de datos y hacerlas fáciles de usar desde R; 2) proporcionar un rendimiento rápido y eficiente para los datos almacenados en la memoria del computador; 3) utilizar la misma interfaz para trabajar con datos sin importar dónde o cómo estén almacenados, ya sea en un marco de datos, una tabla de datos o una base de datos. tidyverse es una colección de paquetes disponibles en R y orientados a la manipulación, importación, exploración y visualización de datos y que se utiliza exhaustivamente en ciencia de datos (Wickham et al. 2019). El uso de tidyverse permite facilitar el trabajo estadístico y la generación de procesamientos reproducibles. readstata13 permite leer y escribir todos los formatos de archivo de Stata (versión 17 y anteriores) en un marco de datos R (Garbuszus y Jeworutzki 2023). Se admiten las versiones de formato de archivo de datos 102 a 119. para leer las bases de datos de STATA. Además, el paquete admite muchas características del formato Stata dta, como conjuntos de etiquetas en diferentes idiomas o calendarios comerciales. survey ha sido elaborado por Thomas Lumley y nos proporciona funciones en R útiles para analizar datos provenientes de encuestas complejas (Lumley 2016). Alguno de los parámetros que se pueden estimar usando este paquete son medias, totales, razones, cuantiles, tablas de contingencias, modelos de regresión, modelos loglineales, entre otros. srvyr permite utilizar el operador pipeline (%&gt;%) en las consultas que se realizan con el paquete survey (Freedman Ellis y Schneider 2023). ggplot2 es un paquete de visualización de datos para el lenguaje R que implementa lo que se conoce como la gramática de los gráficos, que no es más que una representación esquemática y en capas de lo que se dibuja en dichos gráficos, como lo pueden ser los marcos y los ejes, el texto de los mismos, los títulos, así como, por supuesto, los datos o la información que se grafica, el tipo de gráfico que se utiliza, los colores, los símbolos y tamaños, entre otros (Wickham 2016). TeachingSampling permite al usuario extraer muestras probabilísticas y hacer inferencias a partir de una población finita basada en varios diseños de muestreo. Entre los diseño empleados en esta librería están: Muestreo Aleatorio Simple (MAS), Muestreo estratificado, Muestreo en varias etapas, muestreos proporcionales al tamaño, entre otros (Gutiérrez 2020b). samplesize4surveys permite calcular el tamaño de muestra requerido para la estimación de totales, medias y proporciones bajo diseños de muestreo complejos (Gutiérrez 2020a). Antes de poder utilizar las diferentes funciones que cada librería tiene, es necesario descargarlas de antemano de la web. El comando install.packages permite realizar esta tarea. Note que algunas librerías pueden depender de otras, así que para poder utilizarlas es necesario instalar también las dependencias. install.packages(&quot;dplyr&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;readstata13&quot;) install.packages(&quot;survey&quot;) install.packages(&quot;srvyr&quot;) install.packages(&quot;ggplot2&quot;) install.packages(&quot;TeachingSampling&quot;) install.packages(&quot;samplesize4surveys&quot;) Una vez instaladas las librerías hay que informarle al software que vamos a utilizarlas con el comando library. Nótese que es necesario haber instalado las librerías para poder utilizarlas. rm(list = ls()) library(&quot;dplyr&quot;) library(&quot;tidyverse&quot;) library(&quot;readstata13&quot;) library(&quot;survey&quot;) library(&quot;srvyr&quot;) library(&quot;ggplot2&quot;) library(&quot;TeachingSampling&quot;) library(&quot;samplesize4surveys&quot;) References "],["creación-de-proyectos-en-r.html", "2.2 Creación de proyectos en R", " 2.2 Creación de proyectos en R Una vez se descargan e instalan las librerías o paquetes en R, el paso recomendado es que todos los procesamientos que se realicen se hagan a través de un la creación de proyectos. Un proyecto de R se define como un archivo que contiene los archivos de origen y contenido asociados con el trabajo que se está realizando. Adicionalmente, contiene información que permite la compilación de cada archivo de R que se va a utilizar, mantiene la información para integrarse con sistemas de control de código fuente y ayuda a organizar la aplicación de los procesamientos en componentes lógicos. Ahora bien, atendiendo a una cultura de buenas practicas de programación, se recomienda crear un proyecto en donde se tenga disponible toda la información con la cual se va a trabajar. A continuación, se muestran los pasos para crear un proyecto dentro de RStudio. Paso 1: Abrir RStudio. Paso 2: ir a file -&gt; New Project Paso 3: Tipos de proyecto. Para este ejemplo se tomará New Directory Tipos de proyectos Un aspecto importante a tener en cuenta en este paso es que al hacer clic en New Directory, RStudio brinda una variedad de opciones dependiendo de las características del procesamiento que se desea realizar. Ahora bien, si se cuenta con algunos código previamente desarrollados y se desea continuar con ese proyecto, se debe tomar la opción Existing Directory. Además, es posible clonar repositorios existentes en Git a través de la opción Version Control. Paso 4: Seleccionar el tipo de proyecto. Seleccionar el tipo de proyecto Paso 5: Diligenciar el nombre del proyecto y la carpeta de destino. Nombre de proyecto Al realizar estos pasos todas las rutinas creadas dentro del proyecto estarán ancladas a la carpeta del proyecto. "],["lectura-de-las-bases-de-datos-y-procesamientos-básicos.html", "2.3 Lectura de las bases de datos y procesamientos básicos", " 2.3 Lectura de las bases de datos y procesamientos básicos Es muy usual que al trabajar proyectos en R sea necesario importar bases de datos con información relevante para un estudio en particular. Los formatos de bases de datos que R permite importar son diversos, entre ellos se tienen xlsx, csv, txt, STATA, etc. Particularmente, para la lectura de bases de datos provenientes de STATA 13, se deb utilizar la función read.dta13. Una vez leída la base de datos en el formato pertinente es recomendable transformarla al formato nativo de R, es decir .RDS. Este es un formato más eficiente y propio de R. Una vez se carga la base de datos se procede a utilizar las funciones en R para poder obtener resultados de los procesamientos agregados y gráficos de interés. Para ejemplificar el uso de funciones que permitan obtener resultados agregados, utilizaremos la base de datos BigCity del paquete TeachinSampling. Esta base corresponde a un conjunto de variables socioeconómicas de 150266 personas en un año en particular. library(TeachingSampling) data(BigCity) data1 &lt;- BigCity Una vez guardada la base en nuestros archivos de trabajo, la función head permite vizualizar rápidamente los primeros datos de la base. head(data1) ## HHID PersonID Stratum PSU Zone Sex Age MaritalST Income ## 1 idHH00001 idPer01 idStrt001 PSU0001 Rural Male 38 Married 555.00 ## 2 idHH00001 idPer02 idStrt001 PSU0001 Rural Female 40 Married 555.00 ## 3 idHH00001 idPer03 idStrt001 PSU0001 Rural Female 20 Single 555.00 ## 4 idHH00001 idPer04 idStrt001 PSU0001 Rural Male 19 Single 555.00 ## 5 idHH00001 idPer05 idStrt001 PSU0001 Rural Male 18 Single 555.00 ## 6 idHH00002 idPer01 idStrt001 PSU0001 Rural Male 35 Married 298.34 ## Expenditure Employment Poverty ## 1 488.33 Employed NotPoor ## 2 488.33 Employed NotPoor ## 3 488.33 Inactive NotPoor ## 4 488.33 Employed NotPoor ## 5 488.33 Inactive NotPoor ## 6 216.70 Employed Relative Una vez cargada la base de datos en R, se puede empezar a reliazar los procesamientos según las necesidades de cada investigador. En este sentido, una de las primeras revisiones que se realizan al cargar las bases de datos es revisar su dimensión; es decir, verificar la cantidad de filas y columnas que tiene la base. Lo anterior se puede hacer con la función nrow, que identifica el número de registros (unidades efectivamente observadas) en la base de datos y con la función ncol, que muestra el número de variables en la base de datos. Los códigos computacionales son los siguientes: nrow(data1) ## [1] 150266 ncol(data1) ## [1] 12 Una forma resumida de revisar la cantidad de filas y columnas que tiene la base de datos es con la función dim, la cual nos devuelve un vector cuya primera componente contiene la cantidad de filas y su segunda componente, la cantidad de columnas: dim(data1) ## [1] 150266 12 Es usual que en las encuestas de hogares las bases de datos sean extensas; es decir, que contengan una cantidad importante de variables observadas y que, por lo general, su tamaño de muestra sea grande. Es por lo anterior que, para poder visualizar apropiadamente dichas bases de datos, una vez cargadas en R sea necesario hacerlo de manera externa. Esto es, abrir una pestaña diferente en R y hacer la navegación de la base como un texto plano. Lo anterior se realiza con la función View como se muestra a continuación: View(data1) Visor de bases de datos de RStudio Otra verificación importante que se debe realizar al momento de cargar una base de datos en R es el reconocimiento de las variables que incluye. Esto se puede hacer utilizando la función names la cual identifica las variables de la base de datos. names(data1) La función names solo devuelve un vector con los nombres de las variables que contiene la base. Sin embargo, si se quiere profundizar en qué información contiene cada variable, La función str muestra de manera compacta la estructura de un objeto y sus componentes. Para nuestra base se utilizaría de la siguiente manera: str(data1) ## &#39;data.frame&#39;: 150266 obs. of 12 variables: ## $ HHID : chr &quot;idHH00001&quot; &quot;idHH00001&quot; &quot;idHH00001&quot; &quot;idHH00001&quot; ... ## $ PersonID : chr &quot;idPer01&quot; &quot;idPer02&quot; &quot;idPer03&quot; &quot;idPer04&quot; ... ## $ Stratum : chr &quot;idStrt001&quot; &quot;idStrt001&quot; &quot;idStrt001&quot; &quot;idStrt001&quot; ... ## $ PSU : chr &quot;PSU0001&quot; &quot;PSU0001&quot; &quot;PSU0001&quot; &quot;PSU0001&quot; ... ## $ Zone : chr &quot;Rural&quot; &quot;Rural&quot; &quot;Rural&quot; &quot;Rural&quot; ... ## $ Sex : chr &quot;Male&quot; &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; ... ## $ Age : int 38 40 20 19 18 35 29 14 13 6 ... ## $ MaritalST : Factor w/ 6 levels &quot;Partner&quot;,&quot;Married&quot;,..: 2 2 5 5 5 2 2 5 5 NA ... ## $ Income : num 555 555 555 555 555 ... ## $ Expenditure: num 488 488 488 488 488 ... ## $ Employment : Factor w/ 3 levels &quot;Unemployed&quot;,&quot;Inactive&quot;,..: 3 3 2 3 2 3 3 NA NA NA ... ## $ Poverty : Factor w/ 3 levels &quot;NotPoor&quot;,&quot;Extreme&quot;,..: 1 1 1 1 1 3 3 3 3 3 ... Como se puede observar en la salida anterior, por ejemplo, la variable HHID es de tipo caracter al igual que la variable Sex, mientras que la variable Income es de tipo numérico. Todos los demás atributos de las variables se encuentran en la salida del código. Esta función es muy útil al momento de querer tener un panorama amplio del contenido y clase de cada variable en una base de datos, particularmente en una encuesta de hogares en donde se tiene, por la misma estructura del estudio, muchas clases o tipos de variables observadas. "],["el-operador-pipeline.html", "2.4 El operador pipeline (%&gt;%)", " 2.4 El operador pipeline (%&gt;%) El software estadístico R es un lenguaje de programación creado por estadísticos y para estadísticos. Una de las contribuciones recientes a este software es el desarrollo de los pipelines (tuberías) que permiten de una forma intuitiva generar consultas y objetos desde una base de datos. El operador pipeline, %&gt;%, es nativo del paquete Bache y Wickham (2022) y está cargado automáticamente en los paquetes del tidyverse. El objetivo del operador es ayudar a escribir el código de una manera que sea más fácil de leer y entender. En este sentido, el operador %&gt;% permite encadenar operaciones para que el resultado de una operación anterior se convierta en el argumento de la siguiente operación. A continuación, se presenta un ejemplo sencillo del uso del operador %&gt;% con la base de datos BigLucy haciendo el conteo del total de elementos que contiene la base de datos utilizando la función count. data1 %&gt;% count() ## n ## 1 150266 La anterior línea de código se entiende así: con la base de datos realice un conteo. Por otro lado, existe una gama amplia de funciones que se pueden utilizar con el operador %&gt;%. A continuación, se enlistan una serie de funciones muy útiles al momento de hacer análisis con bases de datos provenientes de encuestas de hogares: filter: mantiene un criterio de filtro sobre alguna variable o mezcla de variables. select: selecciona columnas por nombres. arrange: ordena las filas de la base de datos. mutate: añade nuevas variables a la base de datos. summarise: reduce variables a valores agrupados y los presenta en una tabla. group_by: ejecuta funciones y agrupa el resultado por las variables de interés. Una de las primeras consultas que se pueden realizar en las encuestas de hogares es saber el número de entrevistas efectivamente realizadas. Usando %&gt;% se realiza de la siguiente manera: data1 %&gt;% nrow() ## [1] 150266 Otro de los ejercicios que se hacen usualmente con las encuestas de hogares está relacionado con hacer descriptivos de la base por sexo. Una de las formas más sencillas de hacer esta revisión es usar la función filter, para generar dos bases de datos. datasexH &lt;- data1 %&gt;% filter(Sex == &quot;Male&quot;) datasexM &lt;- data1 %&gt;% filter(Sex == &quot;Female&quot;) Por otro lado, si el interés ahora es filtrar la base de datos por aquellas personas que no son pobres, se realiza de la siguiente manera: datanopobre &lt;- data1 %&gt;% filter(Poverty == &quot;NotPoor&quot;) En este mismo sentido, es posible también crear nuevas bases de datos filtradas por algunos ingresos particulares mensuales, es posible realizarlo de la siguiente manera: dataingreso1 &lt;- data1 %&gt;% filter(Income %in% c(265, 600)) dataingreso2 &lt;- data1 %&gt;% filter(Income %in% c(1000, 2000)) Dado que la base del ejemplo no cuenta con regiones geográficas definidas, se pueden construir agrupando algunos estratos, para ello se usa el siguiente código: Region &lt;- as.numeric(gsub( pattern = &quot;\\\\D&quot;, replacement = &quot;&quot;, x = data1$Stratum )) data1$Region &lt;- cut( Region, breaks = 5, labels = c(&quot;Norte&quot;, &quot;Sur&quot;, &quot;Centro&quot;, &quot;Occidente&quot;, &quot;Oriente&quot;) ) La mayoría de veces, para los análisis con las bases de hogares, es necesario recodificar los niveles de los factores. El siguiente código permite generar los nombres de las regiones, junto con sus respectivos rótulos. data1$IDRegion &lt;- factor( data1$Region, levels = c(&quot;Norte&quot;, &quot;Sur&quot;, &quot;Centro&quot;, &quot;Occidente&quot;, &quot;Oriente&quot;), labels = c(&quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;) ) Por último, para efectos de visualización en tablas y gráficos es conviene codificar los nombres de las variables como sigue: data1$Nom_corto &lt;- factor( data1$Region, levels = c(&quot;Norte&quot;, &quot;Sur&quot;, &quot;Centro&quot;, &quot;Occidente&quot;, &quot;Oriente&quot;), labels = c(&quot;N&quot;, &quot;S&quot;, &quot;C&quot;, &quot;O&quot;, &quot;E&quot;) ) Otra función muy útil en los procesamientos descriptivos de las encuestas de hogares es la función select, la cual permite seleccionar un grupo de variables de interés que se quieren analizar. Si, por ejemplo, se desea seleccionar de la base de ejemplo solo las variables que identifican al hogar (HHID), a las unidades primarias de muestreo (PSU), la Zona (Zone), el sexo de la persona (Sex) y sus ingresos (Income), se realiza de la siguiente manera: datared &lt;- data1 %&gt;% select(HHID, PSU, Zone, Sex, Income) La función select no solo sirve para seleccionar variables de una base de datos, también se puede utilizar para eliminar algunas variables de la base de datos que ya no son de interés para el análisis o que, simplemente, se generaron en la manipulación de la base de datos como variables temporales para realizar algunos cálculos de interés. Por ejemplo, si se desea eliminar de la base de datos de ejemplo las variables de identificación del hogar y de identificación de las personas, se podría realizar introduciendo un signo “menos” (-) delante del nombre de la variable como sigue: datagrey &lt;- data1 %&gt;% select(-HHID, -PersonID) Por otro lado, si el objetivo fuese ordenar las filas de la base por alguna variable en particular, se utilizaría la función arrange para realizar esta operación. A continuación, se ejemplifica con la base de datos dataingreso1, cómo se ordena la base de acuerdo con la variable Income: datadog &lt;- dataingreso1 %&gt;% arrange(Income) datadog %&gt;% head() ## HHID PersonID Stratum PSU Zone Sex Age MaritalST Income ## 1 idHH02245 idPer01 idStrt017 PSU0166 Rural Male 73 Widowed 265 ## 2 idHH02245 idPer02 idStrt017 PSU0166 Rural Male 38 Married 265 ## 3 idHH02245 idPer03 idStrt017 PSU0166 Rural Female 37 Married 265 ## 4 idHH02245 idPer04 idStrt017 PSU0166 Rural Male 16 Single 265 ## 5 idHH02245 idPer05 idStrt017 PSU0166 Rural Male 12 Single 265 ## 6 idHH02245 idPer06 idStrt017 PSU0166 Rural Male 11 &lt;NA&gt; 265 ## Expenditure Employment Poverty ## 1 281.42 Inactive Relative ## 2 281.42 Employed Relative ## 3 281.42 Inactive Relative ## 4 281.42 Employed Relative ## 5 281.42 &lt;NA&gt; Relative ## 6 281.42 &lt;NA&gt; Relative Es posible utilizar la función arrange para hacer ordenamientos más complicados. Por ejemplo, ordenar por más de una variable. A modo de ejemplo, ordenemos la base de datos dataingreso1 de acuerdo con las variables Sexo y Edad. dataingreso1 %&gt;% arrange(Sex, Age) %&gt;% head() ## HHID PersonID Stratum PSU Zone Sex Age MaritalST Income ## 1 idHH02586 idPer05 idStrt018 PSU0193 Urban Female 2 &lt;NA&gt; 600 ## 2 idHH23231 idPer05 idStrt018 PSU0193 Urban Female 2 &lt;NA&gt; 600 ## 3 idHH19448 idPer05 idStrt115 PSU1571 Urban Female 2 &lt;NA&gt; 600 ## 4 idHH40093 idPer05 idStrt115 PSU1571 Urban Female 2 &lt;NA&gt; 600 ## 5 idHH14415 idPer05 idStrt091 PSU1182 Urban Female 3 &lt;NA&gt; 600 ## 6 idHH35060 idPer05 idStrt091 PSU1182 Urban Female 3 &lt;NA&gt; 600 ## Expenditure Employment Poverty ## 1 379.15 &lt;NA&gt; NotPoor ## 2 379.15 &lt;NA&gt; NotPoor ## 3 411.02 &lt;NA&gt; NotPoor ## 4 411.02 &lt;NA&gt; NotPoor ## 5 399.55 &lt;NA&gt; NotPoor ## 6 399.55 &lt;NA&gt; NotPoor También es posible utilizar la función arrange junto con la opción desc() para que el ordenamiento sea descendente. dataingreso1 %&gt;% arrange(desc(Age)) %&gt;% head() ## HHID PersonID Stratum PSU Zone Sex Age MaritalST Income ## 1 idHH06945 idPer01 idStrt047 PSU0541 Urban Female 88 Separated 600 ## 2 idHH27590 idPer01 idStrt047 PSU0541 Urban Female 88 Separated 600 ## 3 idHH05798 idPer01 idStrt039 PSU0438 Urban Female 87 Widowed 600 ## 4 idHH26443 idPer01 idStrt039 PSU0438 Urban Female 87 Widowed 600 ## 5 idHH13013 idPer01 idStrt085 PSU1074 Urban Female 87 Widowed 600 ## 6 idHH33658 idPer01 idStrt085 PSU1074 Urban Female 87 Widowed 600 ## Expenditure Employment Poverty ## 1 543.59 Inactive NotPoor ## 2 543.59 Inactive NotPoor ## 3 1026.00 Inactive NotPoor ## 4 1026.00 Inactive NotPoor ## 5 266.00 Inactive NotPoor ## 6 266.00 Inactive NotPoor References "],["creación-de-variables-y-resumen-de-conteos.html", "2.5 Creación de variables y resumen de conteos", " 2.5 Creación de variables y resumen de conteos Las funciones mutate, summarise y group_by están cargadas en el paquete tidyverse y son muy importantes al momento de realizar análisis descriptivos simples con las bases de datos de las encuestas de hogares. En primer lugar, la función mutate permite realizar transformaciones de variables en una base de datos. Usualmente, en las encuestas de hogares es necesario crear nuevas variables, por ejemplo, si el hogar está en estado de pobreza extrema o no, la cual se calcula a partir de los ingresos del hogar. La función mutate proporciona una interfaz clara para realizar este tipo de operaciones. A modo de ejemplo, utilizaremos la base de ejemplo para crear una nueva variable llamada logingreso, la cual corresponde al logaritmo natural de los ingresos de la persona dentro del hogar. Los códigos computacionales se muestran a continuación: datablue2 &lt;- dataingreso1 %&gt;% mutate(Logincome = log(Income)) datablue2 %&gt;% select(PersonID, Income, Logincome) %&gt;% head() ## PersonID Income Logincome ## 1 idPer01 600 6.39693 ## 2 idPer02 600 6.39693 ## 3 idPer03 600 6.39693 ## 4 idPer01 600 6.39693 ## 5 idPer02 600 6.39693 ## 6 idPer03 600 6.39693 Si fuera necesario, se puede crear más de una variable en la base de datos. Cabe recalcar que la función mutate reconoce sistemáticamente las variables que van siendo creadas de manera ordenada. A continuación, se presenta un ejemplo de cómo crear más de una nueva variable en la base de datos: datacat &lt;- datablue2 %&gt;% mutate(Income2 = 2 * Income, Income4 = 2 * Income2) datacat %&gt;% select(PersonID, Income2, Income4) %&gt;% head() ## PersonID Income2 Income4 ## 1 idPer01 1200 2400 ## 2 idPer02 1200 2400 ## 3 idPer03 1200 2400 ## 4 idPer01 1200 2400 ## 5 idPer02 1200 2400 ## 6 idPer03 1200 2400 Ahora bien, la función summarise crea un nuevo archivo de datos con información resumida de la base de datos. Como se mencionó anteriormente, esta función sirve para resumir o colapsar la información de las filas; toma un grupo de valores como argumento y devuelve un conjunto de valores que resumen la información de los datos en la base. Por ejemplo, con summarise es posible hallar la media de los ingresos, sus percentiles o algunas medidas de dispersión en la base de datos. En el siguiente código computacional se muestra cómo se utiliza la función para conocer el número de envuestas de la base. data1 %&gt;% summarise(n = n()) ## n ## 1 150266 Por otro lado, la función group_by permite agrupar información de acuerdo con una o varias variables de interés. Usadas en conjunto, estas dos funciones proporcionan un medio poderoso para obterner infomración desagregada. El siguiente código permite generar el número de registros en cada una de las regiones de BigCity. El comando group_by agrupa los datos por región, el comando summarise hace los cálculos requeridos y el comando arrange ordena los resultados: data1 %&gt;% group_by(Region) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## # A tibble: 5 × 2 ## Region n ## &lt;fct&gt; &lt;int&gt; ## 1 Oriente 39160 ## 2 Occidente 33868 ## 3 Centro 25944 ## 4 Sur 25898 ## 5 Norte 25396 Por ejemplo, para generar el número de encuestas efectivas discriminado por el sexo del respondiente, es posible utilizar el siguiente código computacional: data1 %&gt;% group_by(Sex) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## # A tibble: 2 × 2 ## Sex n ## &lt;chr&gt; &lt;int&gt; ## 1 Female 79190 ## 2 Male 71076 Si ahora se desea realizar la consulta del número de encuestas efectivas por área geográfica, se realiza de la siguiente manera: data1 %&gt;% group_by(Zone) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) ## # A tibble: 2 × 2 ## Zone n ## &lt;chr&gt; &lt;int&gt; ## 1 Urban 78164 ## 2 Rural 72102 Por último, resulta fundamental categorizar variables como la edad para agilizar y estandarizar la revisión de los datos en la base. En este escenario, optaremos por emplear la función case_when, la cual posibilita la evaluación de distintas condiciones para una variable particular. data1 &lt;- data1 %&gt;% mutate( CatAge = case_when( Age &lt;= 5 ~ &quot;0-5&quot;, Age &lt;= 15 ~ &quot;6-15&quot;, Age &lt;= 30 ~ &quot;16-30&quot;, Age &lt;= 45 ~ &quot;31-45&quot;, Age &lt;= 60 ~ &quot;46-60&quot;, TRUE ~ &quot;Más de 60&quot; ), CatAge = factor( CatAge, levels = c(&quot;0-5&quot;, &quot;6-15&quot;, &quot;16-30&quot;, &quot;31-45&quot;, &quot;46-60&quot;, &quot;Más de 60&quot;), ordered = TRUE ) ) "],["algunas-estadísticas-descriptivas-sobre-la-base-de-datos.html", "2.6 Algunas estadísticas descriptivas sobre la base de datos", " 2.6 Algunas estadísticas descriptivas sobre la base de datos En este apartado se mostrará cómo se pueden utilizar las funciones de tidyverse para obtener estadísticas descriptivas de la base de datos. Nótese que es muy importante recalcar que, los siguientes resultados no tienen ninguna interpretación poblacional y se realizan con el único propósito de ilustrar el manejo de las bases de datos de las encuestas. La función summarise permite conocer el total de los ingresos en la base de datos y la media de los ingresos sobre los respondientes de la muestra. data1 %&gt;% summarise(total.ing = sum(Income), media.ing = mean(Income)) ## total.ing media.ing ## 1 87893117 584.9169 También se puede calcular medias de manera agrupada. Particularmente, si se desea calcular la media de los ingresos por región, junto con el total de respondientes, se hace de la siguiente manera: data1 %&gt;% group_by(Region) %&gt;% summarise(n = n(), media = mean(Income)) ## # A tibble: 5 × 3 ## Region n media ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Norte 25396 509. ## 2 Sur 25898 644. ## 3 Centro 25944 701. ## 4 Occidente 33868 571. ## 5 Oriente 39160 530. Si ahora el análisis de los ingresos se desea hacer por sexo se realiza de la siguiente manera: data1 %&gt;% group_by(Sex) %&gt;% summarise(n = n(), media = mean(Income)) ## # A tibble: 2 × 3 ## Sex n media ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Female 79190 579. ## 2 Male 71076 591. La función summarise también permite conocer algunas medidas de localización de los ingresos en la base de datos. data1 %&gt;% summarise( mediana = median(Income), decil1 = quantile(Income, 0.1), decil9 = quantile(Income, 0.9), rangodecil = decil9 - decil1 ) ## mediana decil1 decil9 rangodecil ## 1 449.17 165.33 1126.33 961 Utilizando la función summarise también podemos conocer también el comportamiento variacional de los ingresos sobre los respondientes. A continuación, se calcula la varianza, la desviación estandar, el mínimo, el máximo, el rango y el rango intercuartílico de la variable ingreso en la base de datos. data1 %&gt;% summarise( varianza = var(Income), desv = sd(Income), mini = min(Income), maxi = max(Income), rango = maxi - mini, rangoiq = IQR(Income) ) ## varianza desv mini maxi rango rangoiq ## 1 332463.4 576.5964 10 32920 32910 468.33 Por último, si se desea realizar el cálculo de la media, la desviación estándar y el rango de los ingresos por condición de ocupación se procede con el siguiente código: data1 %&gt;% group_by(Employment) %&gt;% summarise( n = n(), media = mean(Income), desv = sd(Income), rangoiq = IQR(Income) ) ## # A tibble: 4 × 5 ## Employment n media desv rangoiq ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Unemployed 4630 429. 375. 392. ## 2 Inactive 44104 532. 553. 439. ## 3 Employed 62188 661. 606. 529. ## 4 &lt;NA&gt; 39344 541. 558. 407. Como se ha podido ejemplificar en este capítulo, son muchas las ventajas que ofrece R a la hora de realizar procesamientos con bases de datos. Más aún, el entorno de tidyverse hace que el cómputo en las bases sea muy eficiente. Además, su sintaxis es coherente y consistente en todos sus paquetes, lo que facilita la lectura y escritura de código. El uso de los operadores %&gt;%, permite encadenar fácilmente una serie de operaciones, mejorando la claridad y la legibilidad del procesamiento. Este ambiente también cuenta con una comunidad activa de usuarios y desarrolladores, así como una amplia documentación, lo que facilita la resolución de problemas y la mejora continua del análisis de datos. Asimismo, se integra bien con otras herramientas y paquetes en el ecosistema de R, lo que permite combinar diferentes enfoques para análisis de datos de manera efectiva. Este enfoque será usado durante el resto del documento para poder analizar de forma apropiada los datos que provienen de encuestas complejas. Por último, este conjunto de librearía está en constante desarrollo y mejora, lo que garantiza que los usuarios tengan acceso a las últimas herramientas y técnicas para el análisis de datos en R. "],["conceptos-básicos-en-encuestas-de-hogares.html", "Capítulo 3 Conceptos básicos en encuestas de hogares", " Capítulo 3 Conceptos básicos en encuestas de hogares En este capítulo se presentan algunos de los conceptos más importantes y necesarios para el correcto entendimiento, definición y posterior análisis de una encuesta de hogares. en general, estos principios son tomadas de Särndal, Swensson, y Wretman (2003) y Gutiérrez (2009). Alguno de los conceptos que se encontrarán en este capítulo están relacionados con la población objetivo, universo de estudio, marco muestral, etc. Para continuar con lo análisis de las encuestas de hogares es necesario que el lector tenga claro algunos conceptos básicos en el muestreo probabilístico. Según Groves et al. (2009), una encuesta es un método sistemático para recopilar información de una muestra de elementos con el propósito de construir descriptores cuantitativos de los parámetros de la población. Asimismo, Gutiérrez (2009) afirma que una muestra representativa es un modelo reducido de la población y de aquí se desprende un argumento de validez sobre la muestra. En pocas palabras, se desea que la muestra representativa tenga la cantidad de información suficiente para poder hacer una inferencia adecuada a la población. Si la muestra es representativa, las conclusiones que se obtienen de la población utilizando las técnicas de muestreo adecuadas, son correctas. Sin embargo, si se toma una muestra no representativa, no es correcto realizar inferencias dado que estas no representan la realidad de la población. A continuación se presentan algunas preguntas que motivan un uso cuidadoso de los estadísticas descriptivas en las bases de datos de las encuestas de hogares: Si se calcula el promedio de los ingresos en una base de datos de una encuesta, ¿qué significa esa cifra? Esta cifra representa los ingresos medios que reportaron las personas entrevistadas en el estudio. En ningún momento se puede hablar de que este valor representa a la población a la cual queremos hacer inferencia. Para poder realizar las conclusiones a nivel poblacional se deben utilizar los factores de expansión que se obtuvieron empleando el diseño muestral. Si se calcula el total de los ingresos en una base de datos de una encuesta, ¿qué significa esa cifra? Similar a lo anterior, significa los ingresos totales que reportaron los entrevistados en el estudio. Se recalca que, bajo ninguna circunstancia se puede inferir que este valor muestral representa a la población de estudio. ¿Qué se necesita para que la inferencia sea precisa y exacta? Se requiere de un buen diseño de muestreo, que la muestra que se recolecte sea representativa de la población en el estudio y que el tamaño de muestra sea suficiente para poder inferir en todas las desagregaciones, tanto geográficas como temáticas que se plantearon en el diseño muestral. La representatividad es la característica más importante de una muestra probabilística, y se define como la capacidad que tiene una muestra de poder representar a la población a la cual se desea hacer inferencia. En este sentido, el muestreo adquiere todo su sentido en cuanto se garantice que las características que se quieren medir en la población quedan reflejadas adecuadamente en la muestra expandida con los pesos de muestreo. Cabe resaltar que, una muestra representativa no es aquella que se parece a la población, de tal forma que las categorías aparecen con las mismas proporciones que en la población dado que, en algunas ocasiones es fundamental sobre-representar algunas categorías o incluso seleccionar unidades con probabilidades desiguales para poderlas medir con precisión. Las medidas descriptivas permiten la presentación resumida de un conjunto de datos con el fin de poder describir apropiadamente las diversas características de interés presentes en la información de la muestra. La estadísticas descriptiva involucra cualquier labor o actividad para resumir y describir los datos univariados o multivariados sin tratar de hacer inferencia más allá de los mismos. Este tipo de análisis son primordiales en cualquier encuesta de hogares dado que, permiten tener una idea inicial del comportamiento de la población en ciertas variables de estudio. Dadas las definiciones hechas anteriormente, una encuesta de hogares requiere el análisis de todas las variables que dispuestas en la encuesta. Este proceso debe ser llevado a cabo por separado para asegurar la calidad y consistencia de los datos recolectados. Sin embargo, en este capítulo no vamos a adentrarnos en el análisis de las variables en la muestra, porque los datos muestrales no son de interés para el investigador. El interés se centra en lo que suceda a nivel poblacional y este análisis se debe abordar desde la teoría del muestreo. References "],["unidades-y-marcos-de-muestreo.html", "3.1 Unidades y marcos de muestreo", " 3.1 Unidades y marcos de muestreo El término encuesta se encuentra directamente relacionado con una población finita compuesta de individuos a los cuales es necesario entrevistar. El universo de estudio lo constituye el total de individuos o elementos que poseen las características que quieren ser estudiadas. Ahora bien, el conjunto de unidades de interés sobre los cuales se tendrán resultados poblacionales recibe el nombre de población objetivo. Por ejemplo, la Encuesta Nacional de Empleo y Desempleo de Ecuador define su población objetivo como todas las personas mayores de 10 años residentes en viviendas particulares en Ecuador. Asimismo los dominios de análisis corresponden a los diferentes niveles de desagregación establecidos para consolidar el diseño probabilístico y sobre los que se presentan los resultados de interés. Por ejemplo, en México, la Encuesta Nacional de Ingresos y Gastos de los Hogares define como unidades de análisis el ámbito al que pertenece la vivienda, urbano alto, complemento urbano y rural; la Gran Encuesta Integrada de Hogres de Colombia tiene cobertura nacional y sus unidades de análisis están definidas por 13 grandes ciudades junto con sus áreas metropolitanas. En general, para seleccionar una muestra de las unidades del universo de estudio, se debe acceder a un dispositivo que permita identificarlas y ubicarlas. Tales dispositivos se conocen como marcos de muestreo. En general, no existe un marco de muestreo actualizado de personas u hogares. Por tal razón, el diseño de una encuesta de hogares plantea la necesidad de seleccionar en varias etapas algunas unidades de muestreo que sirven como medio para seleccionar finalmente a los hogares que participarán de la muestra. A manera de ejemplo, la antigua Pesquisa Nacional por Amostra de Domicilios (PNAD) en Brasil se realiza por medio de una muestra de viviendas en tres etapas, cada etapa se define como una unidad de muestreo. Por ejemplo, las unidades de muestreo en PNAD son: Las unidades primarias de muestreo (UPM) son los municipios, Las unidades secundarias de muestreo (USM) son los sectores censales, que conforman una malla territorial conformada en el último Censo Demográfico. Las últimas unidades en ser seleccionadas son las viviendas. En este sentido esta encuesta contaba con tres unidades de muestreo que eran seleccionadas de manera sistemática para seleccionar finalmente a las viviedas hogares y personas: primero los municipios; luego, dentro de los municipios seleccionados, sectores censales; finalmente, dentro de los sectores seleccionados en los municipios seleccionados, viviendas. Para realizar el proceso de selección sistemática de los hogares es necesario contar con un marco de muestreo que sirva de vínculo entre los hogares o personas y las unidades de muestreo y que permita el acceso a la población de interés. En este sentido, el marco de muestreo es el conjunto sobre el cual se identifican a los elementos que componen la población objeto de estudio, del cual se selecciona la muestra. Los marcos de muestreo más utilizados en encuestas complejas son de áreas geográficas y se crean directamente después de cada levantamiento censal. A modo de ejemplo, la Encuesta Nacional de Hogares de Costa Rica utiliza un marco muestral construido a partir de los censos nacionales de población y vivienda de 2011. Dicho marco corresponde a uno de áreas en donde sus unidades son superficies geográficas asociadas con las viviendas. Este marco permite la definición de UPM con 150 viviendas en las zonas urbanas y 100 viviendas en las zonas rurales. Este marco está conformado por 10461 UPM (64.5% urbanas y 35.5% rurales). "],["la-base-de-datos-bigcity.html", "3.2 La base de datos BigCity", " 3.2 La base de datos BigCity Gambino y do Nascimento Silva (2009) afirman que, desde que se popularizaron las encuestas de hogares en 1940, se ha hecho evidente algunas tendencias que están ligadas a los avances tecnológicos en las agencias estadísticas y en la sociedad y, con la introducción del computador, se ha acelerado su uso. Detrás de cada encuesta hay una muestra, y detrás de cada muestra hay un esquema de selección por muestreo probabilístico. El muestreo es un procedimiento que responde a la necesidad de información estadística precisa sobre una población objetivo de estudio; Como lo menciona Gutiérrez (2009), el muestreo trata con investigaciones parciales sobre la población que apuntan a inferir a la población completa. Es así como en las últimas décadas ha tenido bastante desarrollo en diferentes campos principalmente en el sector gubernamental con la publicación de las estadísticas oficiales que permiten realizar un seguimiento a las metas del gobierno, en el sector académico, en el sector privado y de comunicaciones. Como se ha venido mencionando anteriormente, este documento está enfocado en el análisis de las encuestas de hogares. En ese sentido y para que el lector tenga un fundamento estándar a través de sus capítulos, en este documento se utilizará, para los ejemplos computacionales, la base de datos BigCity del paquete TeachingSampling. Esta base de datos corresponde a un conjunto de datos que contiene algunas variables socioeconómicas de \\(150266\\) personas de una ciudad en un año en particular. Algunas de las variables de esta base de datos son: HHID, que corresponde al identificador del hogar. PersonID, que hace referencia al identificador de la persona dentro del hogar. Stratum, que asigna el estrato geográfico del hogar. Son 119 estratos. PSU, que corresponde a las unidades primarias de muestreo. La base de datos cuenta con \\(1664\\) PSU. Zone, que identifica las áreas urbanas o rurales a lo largo de la ciudad. Sex, que establece el sexo del entrevistado. Income, que detalla los ingresos mensuales per cápita. Expenditure, que describe los gastos mensuales per cápita. Employment, que muestra la situación laboral de la persona entrevistada. Poverty, que indica si la persona es pobre o no. Depende de los ingresos. References "],["un-esquema-de-muestreo-para-bigcity.html", "3.3 Un esquema de muestreo para BigCity", " 3.3 Un esquema de muestreo para BigCity En esta sección se establecerán las condiciones de selección de una muestra probabilística correspondiente con un esquema tradicional en la región. Los diseños de muestreo en las encuestas de hogares se caracterizan por ser diseños complejos los cuales involucran, entre otras, más de una etapa en la selección de las unidades de observación, estratos y estimadores complejos. En su mayoría, las unidades primarias de muestreo son seleccionadas dentro de los estratos. Es muy común que, en las encuestas de hogares de la región, se formen estratos geográficos, socioeconómicos o una mezcla de ellos. Según la teoría de muestreo, los estratos corresponden con subgrupos poblacionales que abarcan grandes subdivisones de la población objetivo. Además la inferencia en los estratos se hace de forma independiente; esto es, las estimaciones del total, así como el cálculo y estimación de la varianza son el resultado de añadir o sumar para cada estrato la respectiva cantidad. Se asume que, dentro de cada estrato \\(U_h\\) (\\(h=1,\\ldots, H\\)) existen \\(N_{Ih}\\) unidades primarias de muestreo, de las cuales se selecciona una muestra \\(s_{Ih}\\) de tamaño \\(n_{Ih}\\) mediante un diseño de muestreo aleatorio simple. Estas muestras \\(s_{Ih}\\) corresponden con conglomerados o unidades primarias de muestreo (UPM) constituidas por unión o escisión de los sectores censales del país. Además, se supone que el sub-muestreo dentro de cada unidad primaria seleccionada es también aleatorio simple. En este sentido, para cada unidad primaria de muestreo seleccionada \\(i\\in s_{Ih}\\) de tamaño \\(N_i\\) se selecciona una muestra \\(s_i\\) de elementos de tamaño \\(n_i\\). En esta sección se utilizarán las funciones estudiadas en el capítulo anterior para la manipulación de la base de datos de ejemplo. Inicialmente, se cargarán las librerías ggplot2 que permitirá generar gráficos de alta calidad en R, TeachingSampling que permite tomar muestras probabilísticas utilizando los diseños de muestreo usuales, survey y srvyr que permitirán definir los diseños muestrales y por último dplyr que permite la manipulación de las bases de datos. library(ggplot2) library(TeachingSampling) library(dplyr) library(survey) library(srvyr) Una vez cargada las librerías, se procede a realizar una agregación de las 1664 unidades primarias de muestreo en esta base de datos y calcular, para cada una de ellas, la cantidad de personas, el total de ingresos y total de gastos, además de la identificación de su estrato de muestreo: data(&#39;BigCity&#39;) FrameI &lt;- BigCity %&gt;% group_by(PSU) %&gt;% summarise( Stratum = unique(Stratum), Persons = n(), Income = sum(Income), Expenditure = sum(Expenditure) ) attach(FrameI) El siguiente código computacional muestra la información de las unidades primarias de muestreo. head(FrameI, 10) PSU Stratum Persons Income Expenditure PSU0001 idStrt001 118 70911.72 44231.78 PSU0002 idStrt001 136 68886.60 38381.90 PSU0003 idStrt001 96 37213.10 19494.78 PSU0004 idStrt001 88 36926.46 24030.74 PSU0005 idStrt001 110 57493.88 31142.36 PSU0006 idStrt001 116 75272.06 43473.28 PSU0007 idStrt001 68 33027.84 21832.66 PSU0008 idStrt001 136 64293.02 47660.02 PSU0009 idStrt001 122 33156.14 23292.16 PSU0010 idStrt002 70 65253.78 37114.76 Ahora bien, se va a suponer que en cada estrato se tomará una muestra de dos UPM. Suponiendo que el muestreo dentro del estrato es aleatorio simple, el factor de expansión será la división entre el número total de UPM en el estrato y el tamaño de muestra. Para calcular los tamaños poblacionales de los estratos (NIh), definir los tamaños de muestra dentro de cada estrato (nIh), y calcular el factor de expansión, se realiza de la siguiente manera: sizes = FrameI %&gt;% group_by(Stratum) %&gt;% summarise(NIh = n(), nIh = 2, dI = NIh / nIh) NIh &lt;- sizes$NIh nIh &lt;- sizes$nIh La siguiente tabla ejemplifica el esquema de muestreo para los primeros 10 estratos de la población. head(sizes, 10) Stratum NIh nIh dI idStrt001 9 2 4.5 idStrt002 11 2 5.5 idStrt003 7 2 3.5 idStrt004 13 2 6.5 idStrt005 11 2 5.5 idStrt006 5 2 2.5 idStrt007 14 2 7.0 idStrt008 7 2 3.5 idStrt009 8 2 4.0 idStrt010 8 2 4.0 Como se desea extraer una muestra probabilística bajo un diseño aleatorio simple estratificado, se procede a utilizar la función S.STSI de la librería TeachingSampling como se muestra a continuación: samI &lt;- S.STSI(Stratum, NIh, nIh) UI &lt;- levels(as.factor(FrameI$PSU)) sampleI &lt;- UI[samI] En este caso, el objeto sampleI contendrá la información asociada a las UPM que fueron seleccionadas en esta primera etapa. Ahora bien, con la función left_join se procede a pegar los tamaños muestrales a aquellas UPM’s que fueron seleccionadas en la muestra: FrameII &lt;- left_join(sizes, BigCity[which(BigCity$PSU %in% sampleI),]) attach(FrameII) Una vez se tiene la base de datos con la muestra de UMP, se selecciona aquellas variables que son de interés para el estudio como sigue a continuación: head(FrameII, 10) %&gt;% select(Stratum:Zone) Stratum NIh nIh dI HHID PersonID PSU Zone idStrt001 9 2 4.5 idHH00082 idPer01 PSU0007 Rural idStrt001 9 2 4.5 idHH00082 idPer02 PSU0007 Rural idStrt001 9 2 4.5 idHH00082 idPer03 PSU0007 Rural idStrt001 9 2 4.5 idHH00082 idPer04 PSU0007 Rural idStrt001 9 2 4.5 idHH00083 idPer01 PSU0007 Rural idStrt001 9 2 4.5 idHH00083 idPer02 PSU0007 Rural idStrt001 9 2 4.5 idHH00084 idPer01 PSU0007 Rural idStrt001 9 2 4.5 idHH00084 idPer02 PSU0007 Rural idStrt001 9 2 4.5 idHH00084 idPer03 PSU0007 Rural idStrt001 9 2 4.5 idHH00084 idPer04 PSU0007 Rural Luego de tener la información muestral de la primera etapa, en la base FrameII se procede a calcular los tamaños de muestra dentro de cada UPM. En este caso, a modo de ejemplo, se tomará el 10% del tamaño de la UPM como tamaño de muestra en la segunda etapa y se utilizará la función ceiling la cual aproxima al siguiente entero. HHdb &lt;- FrameII %&gt;% group_by(PSU) %&gt;% summarise(Ni = length(unique(HHID)), ni = ceiling(Ni * 0.1)) Ni &lt;- as.numeric(HHdb$Ni) ni &lt;- ceiling(Ni * 0.1) sum(ni) ## [1] 691 Teniendo el vector de tamaños de hogares en cada UPM, junto con su correspondiente tamaño de muestra, se procede a realizar la selección mediante un muestreo aleatorio simple con la función S.SI de la librería TeachingSampling. A modo ilustrativo, la selección en la segunda etapa del diseño se realizará, inicialmente para la primera UPM. Posterior a eso, se realizará un ciclo con la función for para hacerlo con las demás UPM. Para la primera UPM se realiza de la siguiente manera: sam = S.SI(Ni[1], ni[1]) clusterII = FrameII[which(FrameII$PSU == sampleI[1]), ] sam.HH &lt;- data.frame(HHID = unique(clusterII$HHID)[sam]) clusterHH &lt;- left_join(sam.HH, clusterII, by = &quot;HHID&quot;) clusterHH$dki &lt;- Ni[1] / ni[1] clusterHH$dk &lt;- clusterHH$dI * clusterHH$dki sam_data = clusterHH De esta manera se ha seleccionado una muestra de unidades de la primera UPM seleccionada. A continuación se muestran los prime head(sam_data, 10) %&gt;% select(Stratum:Zone) Stratum NIh nIh dI PersonID PSU Zone idStrt001 9 2 4.5 idPer01 PSU0007 Rural idStrt001 9 2 4.5 idPer02 PSU0007 Rural idStrt001 9 2 4.5 idPer01 PSU0007 Rural idStrt001 9 2 4.5 idPer02 PSU0007 Rural idStrt001 9 2 4.5 idPer01 PSU0007 Rural idStrt001 9 2 4.5 idPer02 PSU0007 Rural idStrt001 9 2 4.5 idPer03 PSU0007 Rural Para las demás UPM seleccionadas en la primera etapa de muestreo, la selección de la muestra se automatiza con el siguiente código computacional. for (i in 2:length(Ni)) { sam = S.SI(Ni[i], ni[i]) clusterII = FrameII[which(FrameII$PSU == sampleI[i]),] sam.HH &lt;- data.frame(HHID = unique(clusterII$HHID)[sam]) clusterHH &lt;- left_join(sam.HH, clusterII, by = &quot;HHID&quot;) clusterHH$dki &lt;- Ni[i] / ni[i] clusterHH$dk &lt;- clusterHH$dI * clusterHH$dki data1 = clusterHH sam_data = rbind(sam_data, data1) } encuesta &lt;- sam_data attach(encuesta ) Nótese que la base de datos encuesta contiene una muestra probabilística de la base poblacional BigCity. Una vez se obtiene la muestra (como se mostró anteriormente), el paso siguiente es definir el diseño utilizado y guardarlo como un objeto en R para posteriormente poderlo utilizar y realizar el proceso de estimación de parámetros y cálculo de indicadores. Para realizar esta tarea, se utilizará el paquete srvyr el cual ya fue definido en el capítulo anterior. Para este ejemplo, el diseño de muestreo utilizado fue un estratificado-bietápico en el cual, los estratos correspondieron a la variable Stratum, las UPM correspondieron a la variable PSU, los factores de expansión4 están en la variable dk y por último, se le indica a la función as_survey_design que las UPM’s están dentro de los estrato con el argumento nest = T. A continuación, se presenta el código computacional: diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = dk, nest = T ) Ya definido el diseño de muestreo como un objeto de R se puede empezar a extraer información del mismo. Por ejemplo, se pueden extraer los pesos de muestreo de dicho diseño con la función weights y luego sumarlos para revisar hasta cuánto me está expandiendo mi muestra. El código es el siguiente: sum(weights(diseno)) ## [1] 146712.8 Como se puede observar, el tamaño poblacional estimado utilizando el diseño propuesto es de \\(140579.2\\). Sin embargo, el tamaño poblacional de la base BigCity es de \\(150266\\). Es normal que esto suceda pero debe ser corregido puesto que la suma de los factores de expansión debe sumar el total de la población. La solución para esto es calibrar los pesos de muestreo que se abordará a continuación. El factor de expansión es el número de elementos menos uno de la población (no incluidos en la muestra) representados por el elemento incluido. Está inducido por el inverso de la probabilidad de inclusión.↩︎ "],["calibración-de-los-factores-de-expansión.html", "3.4 Calibración de los factores de expansión", " 3.4 Calibración de los factores de expansión La calibración es un ajuste que se realiza a los pesos de muestreo con el propósito de que las estimaciones de algunas variables de control reproduzcan de forma perfecta los totales poblacionales de estas variables (Deville y Särndal 1992). Esta propiedad de consistencia es deseable en un sistema de ponderadores. En este sentido, cuando los estudios por muestreo están afectados por la ausencia de respuesta, como en muchos casos pasa en las encuestas de hogares, es deseable tener las siguientes propiedades en la estructura inferencial que sustenta el muestreo: Sesgo pequeño o nulo. Errores estándares pequeños. Un sistema de ponderación que reproduzca la información auxiliar disponible. Un sistema de ponderación que sea eficiente al momento de estimar cualquier característica de interés en un estudio multipropósito. La calibración es usualmente el último paso en el ajuste de los ponderadores. Hace uso de información auxiliar que reduce la varianza y corrige los problemas de cobertura que no pudieron ser corregidos en los pasos previos. Puesto que el estimador de calibración depende exclusivamente de la información auxiliar disponible, esta información puede aparecer en diversas formas: Puede estar de forma explícita en el marco de unidades. \\(x_k \\ (\\forall \\ k \\in U)\\) Puede ser un agregado poblacional proveniente de un censo o de registros administrativos. \\(t_x = \\sum_U x_k\\) Puede ser una estimación poblacional \\(\\hat{t}_x = \\sum_s w_kx_k\\) muy confiable. Particularmente, en encuestas de hogares, existen conteos de personas disponibles a nivel de desagregaciones de interés. Por ejemplo, número de personas por edad, raza y género que se permite utilizar como información auxiliar para calibrar las estimaciones. La necesidad de calibrar en las encuestas de hogares se da porque no todos los grupos de personas se cubren apropiadamente desde el diseño de muestreo. Además, las estimaciones del número de personas en estos subgrupos son menores a las proyecciones que se tienen desde los censos. Por último, al ajustar los pesos para que sumen exactamente la cifra de los conteos censales, se reduce el sesgo de subcobertura. Para ejemplificar el estimador de calibración en R usando la base de datos de ejemplo se utilizarán la función calibrate del paquete survey. En primer lugar, para poder calibrar se requiere construir la información poblacional a la cual se desea calibrar. En este ejemplo se calibrará a nivel de zona y sexo. Por tanto, los totales se obtienen como sigue: library(survey) totales &lt;- colSums(model.matrix( ~ -1 + Zone:Sex, BigCity)) En la salida anterior se puede observar que, por ejemplo, en la zona rural hay 37238 mujeres mientras que en la urbana hay 41952. De igual manera se puede leer para el caso de los hombres. Una vez obtenido estos totales, se procede a utilizar la función calibrate para calibrar los pesos de muestreo como sigue: diseno_cal &lt;- calibrate(diseno, ~ -1 + Zone:Sex, totales, calfun = &quot;linear&quot;) Luego de que se hayan calibrado los pesos se puede observar que, al sumar los pesos calibrados estos reproducen el total poblacional de la base de ejemplo. sum(weights(diseno_cal)) ## [1] 150266 encuesta$wk &lt;- weights(diseno_cal) Dado que uno de los principios de los pesos calibrados es que dichos pesos no sean muy diferentes a los pesos originales que provienen del diseño de muestreo, se puede observar a continuación, la distribución de los pesos, sin calibrar y calibrados respectivamente. A continuación se observa que los histogramas de los factores de expansión antes y después de la calibración es similar. par(mfrow = c(1, 2)) hist(encuesta$dk) hist(encuesta$wk) References "],["análisis-de-variables-continuas.html", "Capítulo 4 Análisis de variables continuas", " Capítulo 4 Análisis de variables continuas En encuestas de hogares una variable continua se puede definir como aquella que puede tomar cualquier valor dentro de un rango específico. Por lo general, se refieren a cantidades que pueden ser medidas con precisión y que tienen una gama infinita o muy amplia de valores posibles. Algunos ejemplos de variables continuas en encuestas de hogares podrían incluir los ingresos y los gastos familiares, que pueden variar en un rango continuo, ya que pueden tomar cualquier valor positivo. La edad de los miembros del hogar también puede considerarse continua porque puede expresarse en años y puede tener valores fraccionarios. En contraste, las variables categóricas en encuestas de hogares podrían incluir categorías de interés como el estado civil, el nivel educativo, la ubicación geográfica, entre otras, que se clasifican en categorías discretas y no pueden tomar valores continuos en un rango. "],["definición-del-diseño-de-muestreo.html", "4.1 Definición del diseño de muestreo", " 4.1 Definición del diseño de muestreo Las bases de datos (tablas de datos) pueden estar disponibles en una variedad de formatos (.xlsx, .dat, .cvs, .sav, .txt, etc.); sin embargo, al momento de analizar una base de datos de encuestas de hogares, es recomendable realizar la lectura de cualesquiera de estos formatos y proceder inmediatamente a guardarlo en un archivo de extensión .rds , la cual es nativa5 del software R y permiten almacenar cualquier objeto o información (como pueden ser marcos de datos, vectores, matrices, listas, entre otros). Los archivos .rds se carcaterizan por su flexibilidad a la hora de ser almacenados en memoria y por su perfecta compatibilidad con R. Para ejemplifcar las sintaxis que se utilizarán en R, se tomará la misma base del capítulo anterior la cual contiene una muestra de 2427 registros y proviene de la estrategia de muestreo compleja que se implementó anteriormente (estratificado, bietápico con calibración de los factores de expansión). A continuación, se muestra la sintaxis en R de cómo cargar un archivo con extensión .rds. library(tidyverse) encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) head(encuesta) ## HHID Stratum NIh nIh dI PersonID PSU Zone Sex Age MaritalST ## 1 idHH00031 idStrt001 9 2 4.5 idPer01 PSU0003 Rural Male 68 Married ## 2 idHH00031 idStrt001 9 2 4.5 idPer02 PSU0003 Rural Female 56 Married ## 3 idHH00031 idStrt001 9 2 4.5 idPer03 PSU0003 Rural Female 24 Married ## 4 idHH00031 idStrt001 9 2 4.5 idPer04 PSU0003 Rural Male 26 Married ## 5 idHH00031 idStrt001 9 2 4.5 idPer05 PSU0003 Rural Female 3 &lt;NA&gt; ## 6 idHH00041 idStrt001 9 2 4.5 idPer01 PSU0003 Rural Female 61 Widowed ## Income Expenditure Employment Poverty dki dk wk Region CatAge ## 1 409.87 346.34 Employed NotPoor 8 36 34.50371 Norte Más de 60 ## 2 409.87 346.34 Employed NotPoor 8 36 33.63761 Norte 46-60 ## 3 409.87 346.34 Employed NotPoor 8 36 33.63761 Norte 16-30 ## 4 409.87 346.34 Employed NotPoor 8 36 34.50371 Norte 16-30 ## 5 409.87 346.34 &lt;NA&gt; NotPoor 8 36 33.63761 Norte 0-5 ## 6 823.75 392.24 Employed NotPoor 8 36 33.63761 Norte Más de 60 Una vez cargada la base de datos de la muestra compleja en el ambiente de R, el siguiente paso es definir el diseño de muestreo del cual proviene dicha muestra. Para esto se utilizará el paquete srvyr el cual, como se definió anteriormente, surge como un complemento para el paquete survey. Estas librerías permiten definir objetos tipo survey.design a los que se aplican las funciones de estimación y análisis de encuestas cargadas en el paquete srvyr complementados con la programación de tubería (%&gt;%) del paquete tidyverse. Para ejemplificar los conceptos mencionados anteriormente, se definirá en R el diseño de muestreo del cual proviene la muestra contenida en el objeto encuesta: library(survey) library(srvyr) options(survey.lonely.psu = &quot;adjust&quot;) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = T ) En el código anterior se puede observar que, en primera instancia se debe definir la base de datos en la cual se encuentra la muestra seleccionada. Seguido de eso, se debe definir el tipo de objeto en R con el cual se trabajará, para nuestro caso, será un objeto tipo survey_design, el cual se define usando la función as_survey_design. Ahora bien, una vez establecido el tipo de objeto se procede a definir los parámetros del diseño complejo. Para este caso el diseño de muestreo fue estratificado y en varias etapas. Estos argumentos se definen dentro de la función as_survey_design como sigue: Para definir los estratos, se utiliza el argumento strata incluyendo la columna específica en donde están los estratos de muestreo en la base de datos. Para definir las UPM, se usa el argumento ids incluyendo la columna particular en donde se encuentran los conglomerados o unidades primarias de muestreo que fueron seleccionadas en la muestra de la primera etapa. Para definir los factores de expansión, se acude al argumento weights. Por último, con el argumento nest = T se hace explícito que las UPM están efectivamente anidadas dentro de los estratos de muestreo. Con la función summary es posible observar un resumen rápido del diseño de muestreo, en este caso conteninedo 119 estratos con 2 UPM por estrato, para un total de 238 UPM. summary(diseno) Además, como verificación adicional, es recomendable asegurarse que la suma de los factores de expansión en la muestra dé un resultado coherente. En este caso, esta suma es igual a 1.50266^{5}, que es el tamaño de la población de interés. sum(weights(diseno)) ## [1] 150266 Como se mostró en capítulos anteriores, el diseño de muestreo permite dividir la muestra en sub grupos. La primera subdvisión de interés es por zona (urbano y rural), la cual corresponde también a un estrato de muestreo. Es decir, tanto en la zona urbana, como en la zona rural hay independencia en el muestreo. Esto básicamente implica que la selección en estas zonas no está relacionada. De hecho, no existe ninguna intersección en las UPM que componen estos estratos puesto que, por definición, si una UPM está localizadas en la zona urbana, no puede estar en la zona rural, y viceversa. sub_Urbano &lt;- diseno %&gt;% filter(Zone == &quot;Urban&quot;) sub_Rural &lt;- diseno %&gt;% filter(Zone == &quot;Rural&quot;) La segunda subdivisión de interés es por sexo. Sin embargo, esta subdivisión no conforma estratos independientes. En particular, nótese que, en la gran mayoría de casos, las UPM seleccionadas que contienen a hombres, serán las mismas que contengan a las mujeres. sub_Mujer &lt;- diseno %&gt;% filter(Sex == &quot;Female&quot;) sub_Hombre &lt;- diseno %&gt;% filter(Sex == &quot;Male&quot;) Existen otro tipo de archivos propios de R como los .Rdata . Sin embargo existen algunas diferencias importantes entre ellos. Por ejemplo, mientras que los archivos .rds pueden contener cualquier número de objetos, los archivos .Rdata se limitan a un solo objeto.↩︎ "],["estimación-puntual.html", "4.2 Estimación puntual", " 4.2 Estimación puntual Después de realizar el análisis gráfico de las tendencias de las variables continuas de la encuesta, es necesario obtener las estimaciones puntuales de los parámetros que se midieron. Dichas estimaciones se pueden obtener de forma general, para toda la población, o desagregadas por dominios de interés, de acuerdo con las necesidades de la investigación. Entiéndase como estimaciones puntuales en el contexto de las encuestas de hogares a la estimación de totales, promedios, razones, medias, etc. Como lo mencionan Heeringa, West, y Berglund (2017), la estimación de los totales o promedios de un avariable de interés en la población junto con la estimación de su varianza ha jugado un papel muy importante en el desarrollo de la teoría del muestreo probabilístico, dado que permiten llegar a valores insesgados y precisos, dando una estimación muy acertada de lo que está pasando en los hogares estudiados y con ello tomar decisiones de política publica de manera informada. 4.2.1 Estimación de totales Una vez definido el diseño muestral, lo cual se hizo en la sección anterior, se procede a realizar los procesos de estimación de los parámetros de interés. Para la estimación de totales con diseños muestrales complejos que incluyen estratificación \\(\\left(h=1,2,...,H\\right)\\) y submuestreo en las UPM (que se suponen están dentro del estrato \\(h\\)) indexadas por \\(\\alpha=1,2,...,a_{h}\\), el estimador para el total se puede escribir como: \\[\\begin{eqnarray} \\hat{y}_{\\omega} &amp; = &amp; \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}y_{h\\alpha i}. \\end{eqnarray}\\] En donde \\(n_{h\\alpha}\\) es el tamaño de muestra de hogares o personas en la UPM \\(\\alpha\\) del estrato \\(h\\); \\(a_{h}\\) es el tamaño de muestra de UPM dentro del estrato \\(h\\); \\(H\\) es el total de los estratos en el diseño de muestreo. Finalmente, \\(y_{h\\alpha i}\\) y \\(\\omega_{h\\alpha i}\\) corresponden respectivamente con la observación de la variable de interés y el factor de expansión del elemento \\(i\\) asociado a la UPM \\(\\alpha\\) dentro del estrato \\(h\\). El estimador insesgado de la varianza para este estimador \\(\\hat{y}_{\\omega}\\) es: \\[\\begin{eqnarray} \\widehat{var}\\left(\\hat{y}_{\\omega}\\right) &amp; = &amp; \\sum_{h=1}^{H}\\frac{a_{h}}{\\left(a_{h}-1\\right)}\\left[\\sum_{\\alpha=1}^{a_{h}}\\left(\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}y_{h\\alpha i}\\right)^{2}-\\frac{\\left({ \\sum_{\\alpha=1}^{a_{h}}}\\omega_{h\\alpha i}y_{h\\alpha i}\\right)^{2}}{a_{h}}\\right] \\end{eqnarray}\\] Como se puede observar, calcular la estimación del total y su varianza estimada es complejo. Sin embargo, dichos cálculos se pueden hacer en R mediante la función svytotal. El intervalo de confianza está dado por la siguiente expresión: \\[\\begin{eqnarray} \\hat{y}_{\\omega} \\pm 1.96 * \\sqrt{\\widehat{var}\\left(\\hat{y}_{\\omega}\\right)} \\end{eqnarray}\\] El intervalos de confianza en R se calcula con la función confint. A continuación, se muestran los códigos pertinentes: total_Ingresos &lt;- svytotal( ~ Income, diseno, deff = T,) total_Ingresos ## total SE DEff ## Income 85793667 4778674 11 confint(total_Ingresos, level = 0.95) ## 2.5 % 97.5 % ## Income 76427637 95159697 Los argumentos que utiliza de la función svytotal son muy sencillos. Para el ejemplo, se introduce primero la variable en la cual está la información que se desea estimar (Income). Posterior a esto, se introduce el diseño muestral del cual proviene la muestra y, por último, se indica si desea que se reporte el efecto de diseño deff de la estimación o no. Por otro lado, para el cálculo del intervalo de confianza, lo único que se requiere es indicarle a la función confint el estimador y la confianza requerida. Para seguir ilustrando el uso de la función svytotal y de confint, estimaremos el total de gastos de los hogares, pero ahora el intervalo de confianza se calculará al 90% de confianza. Los siguientes códigos realizan las estimaciones: total_gastos &lt;- svytotal ( ~ Expenditure, diseno, deff = T) total_gastos ## total SE DEff ## Expenditure 55677504 2604138 10.222 confint(total_gastos, level = 0.9) ## 5 % 95 % ## Expenditure 51394077 59960931 Si el objetivo ahora es estimar el total de los ingresos pero discriminando por sexo, se utilizará la función cascadede la librería srvyr, la cual permite agregar la suma de las categorías al final la tabla. También se utilizará la función group_by la cual permite obtener resultados agrupados por los niveles de interés. diseno %&gt;% group_by(Sex) %&gt;% cascade(Total = survey_total(Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;)), .fill = &quot;Total ingreso&quot;) ## # A tibble: 3 × 5 ## Sex Total Total_se Total_low Total_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 44153820. 2324452. 39551172. 48756467. ## 2 Male 41639847. 2870194. 35956576. 47323118. ## 3 Total ingreso 85793667. 4778674. 76331414. 95255920. Como se pudo observar en lo códigos anteriores, otra forma de obtener las estimaciones del total, su desviación estándar y el intervalo de confianza es usando el argumento vartype e indicándole las opciones “se”, “ci” respectivamente. 4.2.2 Estimación de promedios La estimación del promedio o media poblacional es un parámetro muy importante en las encuestas de hogares. Según Gutiérrez (2016), un estimador de la media poblacional se puede escribir como una razón no lineal de dos totales de población finitas estimados como sigue: \\[\\begin{eqnarray} \\hat{\\bar{y}}_{\\omega} &amp; = &amp; \\frac{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}y_{h\\alpha i}}{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}} = \\frac{\\hat{y}_{\\omega}}{\\hat{N}_{\\omega}}. \\end{eqnarray}\\] Como observación, se debe tener en cuenta que, si \\(y\\) es una variable binaria, la media ponderada estima la proporción de la población. Por otro lado, como \\(\\hat{\\bar{y}}_{\\omega}\\) no es una estadística lineal, no existe una fórmula cerrada para la varianza de este estimador. Es por lo anterior que, se deben recurrir a usar métodos de remuestreo o series de Taylor. Para este caso en particular, usando series de Taylor, la varianza del estimador es como sigue: \\[\\begin{eqnarray} var\\left(\\hat{\\bar{y}}_{\\omega}\\right) &amp; \\dot{=} &amp; \\frac{var\\left(\\hat{y}_{\\omega}\\right)+\\hat{\\bar{y}}_{\\omega}^{2}\\times var\\left(\\hat{N}_{\\omega}\\right)-2\\times\\hat{\\bar{y}}_{\\omega}\\times cov\\left(\\hat{y}_{\\omega},\\hat{N}_{\\omega}\\right)}{\\hat{N}_{\\omega}^{2}} \\end{eqnarray}\\] Como se puede observar, el cálculo de la estimación de la varianza tiene componentes complejos de calcular de manera analítica, como la covarianza entre el total estimado y el tamaño poblacional estimado. Sin embargo, R incorpora estos cálculos de forma automática. A continuación, se presenta la sintaxis para estimar la media de los ingresos. Media_ingresos &lt;- svymean( ~ Income, diseno, deff = T) Media_ingresos ## mean SE DEff ## Income 570.945 28.478 8.8211 confint(Media_ingresos, level = 0.95) ## 2.5 % 97.5 % ## Income 515.1299 626.7607 Como se puede observar, los argumentos que utiliza la función svymean para realizar la estimación del promedio de los ingresos, junto con su error estándar estimado son similares a los utilizados con la función svytotal. Algo similar ocurre con los intervalos de confianza. Por otro lado, se estima la media de los gastos en los hogares como sigue a continuación: Media_gastos &lt;- svymean ( ~ Expenditure, diseno, deff = T) Media_gastos ## mean SE DEff ## Expenditure 370.526 13.294 6.0156 confint(Media_gastos) ## 2.5 % 97.5 % ## Expenditure 344.4697 396.5829 También se pueden realizar estimaciones de la media por subgrupos siguiendo el mismo esquema mostrado para la función svytotal. Particularmente, los gastos de los hogares discriminados por sexo es: diseno %&gt;% group_by(Sex) %&gt;% cascade(Media = survey_mean(Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;)), .fill = &quot;Gasto promedio&quot;) %&gt;% arrange(desc(Sex)) ## # A tibble: 3 × 5 ## Sex Media Media_se Media_low Media_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Male 374. 16.1 343. 406. ## 2 Gasto promedio 371. 13.3 344. 397. ## 3 Female 367. 12.3 343. 391. También se pueden realizar estimaciones del promedio en desagregaciones más complejas, por ejemplo las interacciones entre sexo y zona. El siguiente código permite obtenerlos: diseno %&gt;% group_by(Zone, Sex) %&gt;% cascade(Media = survey_mean(Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;)), .fill = &quot;Promedio&quot;) %&gt;% arrange(desc(Zone), desc(Sex)) %&gt;% data.frame() ## Zone Sex Media Media_se Media_low Media_upp ## 1 Urban Promedio 459.6162 22.20655 415.6450 503.5874 ## 2 Urban Male 469.8124 26.96068 416.4276 523.1973 ## 3 Urban Female 450.8151 20.11853 410.9784 490.6518 ## 4 Rural Promedio 273.9461 10.26141 253.6275 294.2647 ## 5 Rural Male 275.3018 10.24848 255.0088 295.5948 ## 6 Rural Female 272.6769 11.61470 249.6786 295.6751 ## 7 Promedio Promedio 370.5263 13.29444 344.2020 396.8506 4.2.3 Estimación de medidas de dispersión y localización En las encuestas de hogares siempre es necesario estimar medidas de dispersión de las variables estudiadas. Por ejemplo, para conocer qué tan disímiles son los ingresos en un país determinado y con esto poder tomar acciones de política pública. Por lo anterior, es importante estudiar este tipo de parámetros. A continuación, se presenta el estimador de la desviación estándar: \\[\\begin{eqnarray*} s_{\\omega}\\left(y\\right) &amp; = &amp; \\frac{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(y_{h\\alpha i}-\\hat{\\bar{y}}_{\\omega}\\right)^{2}}{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}-1} \\end{eqnarray*}\\] Para llevar a cabo la estimación de la desviación estándar en R, se utiliza la función survey_var, la cual se ejemplifica a continuación: diseno %&gt;% group_by(Zone) %&gt;% summarise(Sd = sqrt(survey_var( Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), ))) ## # A tibble: 2 × 5 ## Zone Sd Sd_se Sd_low Sd_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 310. 117. 263. 352. ## 2 Urban 582. 285. 422. 707. Como se pudo ver en el ejemplo anterior, se estimó la desviación estándar de los ingresos por zona reportando el error estándar en la estimación y un intervalo de confianza al 95%. Los argumentos que utiliza la función survey_var son similares a los usados en las funciones anteriores para estimar medias y totales. Si el interés ahora se centra en estimar la desviación estándar desagregando por sexo y zona, los códigos computacionales son los siguientes: diseno %&gt;% group_by(Zone, Sex) %&gt;% summarise(Sd = sqrt(survey_var( Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), ))) ## # A tibble: 4 × 6 ## # Groups: Zone [2] ## Zone Sex Sd Sd_se Sd_low Sd_upp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural Female 295. 112. 250. 334. ## 2 Rural Male 326. 125. 274. 370. ## 3 Urban Female 568. 286. 401. 697. ## 4 Urban Male 597. 289. 437. 722. Las medidas de posición no central (percentiles) se establecen con el fin de conocer otros puntos característicos de la distribución de los datos que no son los valores centrales. Entre las medidas de posición no central más importantes están la mediana, los cuartiles y los percentiles. En la mayoría de las encuestas de hogares no solo se estiman totales, medias y proporciones; para algunos indicadores es necesario estimar otros parámetros, por ejemplo, medianas y percentiles. La mediana es una medida de tendencia central la cual, a diferencia del promedio, no es fácilmente influenciada por datos atípicos y, por esto, se conoce como una medida robusta. La mediana es el valor que divide la población en dos partes iguales. Lo que implica que, la mitad de las observaciones de la característica de interés está por encima de la mediana y la otra mitad está por debajo. Por otro lado, la estimación de los percentiles de los ingresos en un país determinado puede definir el inicio de una política pública. por ejemplo, al establecer un impuesto a aquellas personas naturales que etán ubicadas en el 10% más alto de la distribución de los ingresos o por el contrario, generar subsidios de transporte a aquellas personas que están en el 15% inferior de la distribución de los ingresos. La estimación de cuantiles se basa en los resultados relacionados con el estimador ponderado para totales, empleando una estimación de la función de distribución (CDF, por sus siglas en inglés) acumulada de la población. Específicamente, la CDF para una variable y en una población finita dada de tamaño \\(N\\) se define de la siguiente manera: \\[\\begin{eqnarray*} F\\left(x\\right) &amp; = &amp; \\frac{{\\sum_{i=1}^{N}}I\\left(y_{i}\\leq x\\right)}{N} \\end{eqnarray*}\\] En donde, \\(I\\left(y_{i}\\leq x\\right)\\) es una variable indicadora que toma el valor de 1 si \\(y_{i}\\) es menor o igual a un valor específico \\(x\\), y cero en cualquier otro caso. Un estimador de la CDF en un diseño de muestreo complejo está dado por: \\[\\begin{eqnarray} \\hat{F}_{\\omega}\\left(x\\right) &amp; = &amp; \\frac{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}I\\left(y_{i}\\leq x\\right)}{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}} \\end{eqnarray}\\] Una vez estimada la CDF utilizando los pesos del diseño muestral, el cuantil \\(q\\)-ésimo de una variable \\(y\\) es el valor más pequeño de \\(y\\) tal que la CDF es mayor o igual que \\(q\\). Como es bien sabido, la mediana es aquel valor donde la CDF es mayor o igual a 0.5 y, por tanto, la media estimada es aquel valor donde la estimación de CDF es mayor o igual a 0.5. Siguiendo las recomendaciones de Heeringa, West, y Berglund (2017), para estimar cuantiles primero se consideran las estadísticas de orden que se denotan como \\(y_{(1)},\\ldots,y_{(n)}\\), y se encuentra el valor de \\(j\\) \\((j=1,\\ldots,n)\\) tal que: \\[\\begin{eqnarray*} \\hat{F}_{\\omega}\\left(y_{j}\\right)\\leq q\\leq\\hat{F}_{\\omega}\\left(y_{j+1}\\right) \\end{eqnarray*}\\] Ahora bien, la estimación del q-ésimo cuantil \\(y_{(q)}\\) en un diseño de muestreo complejo está dado por: \\[\\begin{eqnarray} \\hat{y}_{(q)} &amp; = &amp; y_{j}+\\frac{q-\\hat{F}_{\\omega}\\left(y_{j}\\right)}{\\hat{F}_{\\omega}\\left(y_{j+1}\\right)-\\hat{F}_{\\omega}\\left(y_{j}\\right)}\\left(y_{j+1}-y_{j}\\right) \\end{eqnarray}\\] Para la estimación de la varianza e intervalos de confianza de cuantiles, Kovar, Rao, y Wu (1988) muestran los resultados de un estudio de simulación en donde recomiendan el uso de la técnica BRR (Balanced Repeated Replication) para estimarla. Los estimadores y procedimientos antes mencionados para la estimación de percentiles y sus varianzas están implementados en R. Particularmente, la estimación de la mediana se realiza usando la función survey_median. A continuación, se muestra la sintaxis de cómo calcular la mediana de los gastos, la desviación estándar y el intervalo de confianza al 95% de los hogares en la población. diseno %&gt;% summarise(Mediana = survey_median( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), )) ## # A tibble: 1 × 4 ## Mediana Mediana_se Mediana_low Mediana_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 298. 8.83 282. 317. Como se puede observar, los argumentos de la función survey_median son similares a los del total y la media. Ahora bien, al igual que con los demás parámetros, si el objetivo ahora es estimar la mediana de los gastos de los hogares, pero esta vez discriminada por zona y también por sexo, el código computacional sería el siguiente: diseno %&gt;% group_by(Zone) %&gt;% summarise(Mediana = survey_median( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), )) ## # A tibble: 2 × 5 ## Zone Mediana Mediana_se Mediana_low Mediana_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 241. 11.0 214. 258. ## 2 Urban 381. 19.8 337. 416. diseno %&gt;% group_by(Sex) %&gt;% summarise(Mediana = survey_median( Expenditure, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), )) ## # A tibble: 2 × 5 ## Sex Mediana Mediana_se Mediana_low Mediana_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 300. 10.5 282. 324. ## 2 Male 297. 9.29 277. 314. Si el objetivo ahora es estimar cuantiles, por ejemplo, el cuantil 0.25 de los gastos de los hogares, se realizaría usando la función survey_quantile como sigue: diseno %&gt;% summarise(Q = survey_quantile( Expenditure, quantiles = 0.5, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), interval_type = &quot;score&quot; )) ## # A tibble: 1 × 4 ## Q_q50 Q_q50_se Q_q50_low Q_q50_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 298. 12.0 265. 312. Si ahora se desea estimar el cuantil 0.25 pero discriminando por sexo y por zona se realizaría como sigue: diseno %&gt;% group_by(Sex) %&gt;% summarise(Q = survey_quantile( Expenditure, quantiles = 0.25, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), interval_type = &quot;score&quot; )) ## # A tibble: 2 × 5 ## Sex Q_q25 Q_q25_se Q_q25_low Q_q25_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 210. 14.9 169. 228. ## 2 Male 193. 10.4 163. 205. diseno %&gt;% group_by(Zone) %&gt;% summarise(Q = survey_quantile( Expenditure, quantiles = 0.25, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;), interval_type = &quot;score&quot; )) ## # A tibble: 2 × 5 ## Zone Q_q25 Q_q25_se Q_q25_low Q_q25_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 160. 4.64 145. 163. ## 2 Urban 258. 9.05 256. 292. 4.2.4 Estimación del coeficiente de Gini La desigualdad en todos los aspectos es un problema común en todos los países del mundo. Particularmente, la desigualdad económica atañe a muchas instituciones internacionales. Dado lo anterior, es de particular interés poder medir la desigualdad económica de los hogares en los países y para esto, el indicador más utilizado es el coeficiente de Gini (\\(G\\)). El valor de este índice se encuentra entre 0 y 1. Un valor del coeficiente de Gini de \\(G = 0\\) indica perfecta igualdad en la distribución de la riqueza, con valores más grandes significa una desigualdad cada vez mayor en la distribución de la riqueza. Siguiendo la ecuación de estimación de David A. Binder y Kovacevic (1995), un estimador del coeficiente de Gini es: \\[\\begin{eqnarray} \\widehat{G}_{\\omega}\\left(y\\right) = \\frac{2 \\times \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}^{*}\\hat{F}_{\\omega}^{h\\alpha i}y^{h\\alpha i}-1}{\\hat{\\bar{y}}_{\\omega}} \\end{eqnarray}\\] En donde, \\(\\omega_{h\\alpha i}^{*}\\) es una ponderación normalizada, dada por la siguiente expresión: \\[ \\omega_{h\\alpha i}^{*}=\\frac{\\omega_{h\\alpha i}}{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}} \\] Mientras que \\(\\hat{F}_{h\\alpha i}{}_{\\omega}\\) es la estimación de la CDF para el individuo \\(i\\) en el conglomerado \\(\\alpha\\) del estrato \\(h\\); asimismo, \\(\\hat{\\bar{y}}_{\\omega}\\) es la estimación del promedio. Osier (2009) junto con Langel y Tillé (2013) proveen importantes detalles computacionales para la estimación de la varianza de este estimador complejo. Por consiguiente, para calcular el índice de Gini y su varianza estimada en una encuesta de hogares, R tiene cargados los procedimientos en la librería convey. A continuación, se muestra la sintaxis de cómo se realiza la estimación del índice de Gini para los hogares en la base de ejemplo de este capítulo. En primer lugar, se carga el diseño de muestreo con la función convey_prep. Luego, se estima el índice Gini con la función svygini. En los argumentos de esta última función se introducen la variable ingresos y el diseño muestral complejo. library(convey) diseno_gini &lt;- convey_prep(diseno) svygini(~ Income, design = diseno_gini) ## gini SE ## Income 0.41328 0.0187 Por otro lado, si el interés ahora es estimar la curva de Lorenz que, para una distribución dada de ingresos, traza el porcentaje acumulado de la población (desplegado desde el más pobre hasta el más rico) frente a su participación en el ingreso total Osier (2009). El área entre la curva de Lorenz y la línea de 45 grados se conoce como el área de Lorenz y el índice de Gini es igual al doble del área de Lorenz. Una población con la curva de Lorenz más cerca de la línea de 45 grados tiene una distribución de ingresos más equitativa. Si todos los ingresos son iguales, la curva de Lorenz degenera a la línea de 45 grados. Para realizar la curva de Lorenz en R se utiliza la función svylorenz. A continuación, se muestran los códigos computacionales para realizar la curva de Lorenz para los ingresos: library(convey) svylorenz( formula = ~ Income, design = diseno_gini, quantiles = seq(0, 1, .1), alpha = .01 ) ## $quantiles ## 0 0.1 0.2 0.3 0.4 0.5 0.6 ## Income 0 0.01759645 0.04922299 0.09258712 0.1469261 0.2158231 0.3027002 ## 0.7 0.8 0.9 1 ## Income 0.4096304 0.5398749 0.7042464 1 ## ## $CIs ## , , Income ## ## 0 0.1 0.2 0.3 0.4 0.5 0.6 ## (lower 0 0.01644571 0.04643347 0.08770299 0.1396359 0.2056539 0.2898807 ## upper) 0 0.01874718 0.05201251 0.09747124 0.1542163 0.2259923 0.3155197 ## 0.7 0.8 0.9 1 ## (lower 0.3943025 0.5227820 0.6835287 1 ## upper) 0.4249582 0.5569677 0.7249642 1 Los argumentos que requiere la función son, inicialmente, los ingresos de los hogares y el diseño muestral complejo. Adicionalmente, se definen una secuencia de probabilidades que define la suma de los cuantiles a calcular (quantiles) y por último, un número que especifica el nivel de confianza para el gráfico (alpha). References "],["relación-entre-varias-variables.html", "4.3 Relación entre varias variables", " 4.3 Relación entre varias variables En muchos análisis de variables relacionadas con encuestas de hogares no solo basta con analizar el comportamiento de variables de manera individual, por ejemplo, ingresos medios de hombres y mujeres en un país sino también, analizar la diferencia entre los ingresos de los hombres y las mujeres. Esto último con el fin de ir cerrando la brecha salarial que existe. En esta sección se darán las herramientas computacionales para estimar razones y se estudiarán las pruebas de hipótesis para diferencia de medias, junto con contrastes más complejos. 4.3.1 Estimación de razones Un caso particular de una función no lineal de totales es la razón poblacional. Esta se define como el cociente de dos totales poblacionales de características de interés de tipo continuo. En las encuestas de hogares, en ocasiones se requiere estimar este parámetro, por ejemplo, la razón entre los gastos y los ingresos, la cantidad de hombres por cada mujer o la cantidad de mascotas por cada hogar en un país determinado. Puesto que la razón es un cociente de totales, tanto en numerador como el denominador son cantidades desconocidas y por tanto requieren estimarse. El estimador puntual de una razón en muestreos complejos equivale al cociente de los estimadores de los totales como se define a continuación: \\[\\begin{eqnarray} \\hat{R}_{\\omega} = \\frac{\\hat{y}_{\\omega}}{\\hat{x}_{\\omega}} = \\frac{{ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i=1}^{nh\\alpha}}\\omega_{h\\alpha i}y_{h\\alpha i}}{{ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i=1}^{nh\\alpha}}\\omega_{h\\alpha i}x_{h\\alpha i}} \\end{eqnarray}\\] Sin embargo, dado que estimador de la razón es un cociente entre dos estimadores, es decir, un cociente de dos variables aleatorias, el cálculo de la estimación de la varianza no es sencillo de obtener. Para ellos, se debe aplicar linealización de Taylor como lo muestra Gutiérrez (2016). La función survey_ratio tiene implementado los procedimientos para estimar las razones y sus varianzas. Para un correcto cálculo de la estimación de la razón y su varianza estimada se debe introducir en la función el numerador de la razón (numerator) y el denominador (denominator). Adicional a esto, se debe indicar el nivel de confianza de los intervalos. A continuación, se muestran los códigos computacionales para estimar la razón entre el gasto y el ingreso. diseno %&gt;% summarise(Razon = survey_ratio( numerator = Expenditure, denominator = Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.649 0.0232 0.603 0.695 Como se puede observar, la razón entre el gasto y el ingreso se estima en 0.64. Lo que implica que por cada unidad 100 unidades monetarias que le ingrese al hogar, se gastan 64 unidades, consiguiendo un intervalo de confianza al 95% de 0.60 y 0.69. Si el objetivo es estimar la razón entre mujeres y hombres , se realiza de la siguiente manera: diseno %&gt;% summarise(Razon = survey_ratio( numerator = (Sex == &quot;Female&quot;), denominator = (Sex == &quot;Male&quot;), level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.11 0.0351 1.04 1.18 Como la variable sexo en la base de datos es una variable categórica, se tuvo la necesidad de generar las variables indicadores (dummies) para su cálculo; Sex == \"Female\" para el caso de las mujeres y Sex == \"Male\" para el caso de los hombres. Los resultados del ejercicio anterior muestran que en la base de datos hay más mujeres que hombres, generando una razón de 1.11. Esto significa que, por cada 100 hombres hay aproximadamente 111 mujeres con un intervalo que varía entre 1.04 y 1.18. Si se desea hacer la razón de mujeres y hombres pero en la zona rural, se haría de la siguiente manera: sub_Rural %&gt;% summarise(Razon = survey_ratio( numerator = (Sex == &quot;Female&quot;), denominator = (Sex == &quot;Male&quot;), level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.07 0.0352 0.997 1.14 Obteniendo nuevamente que hay más mujeres que hombres. Ahora bien, otro análisis de interés es estimar la razón de gastos pero solo en la población femenina. A continuación, se presentan los códigos computacionales. sub_Mujer %&gt;% summarise(Razon = survey_ratio( numerator = Expenditure, denominator = Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.658 0.0199 0.619 0.698 Dando como resultado que por cada 100 unidades monetarias que le ingresan a las mujeres se gastan 65 con un intervalo de confianza entre 0.61 y 0.69. Por último, análogamente para los hombres, la razón de gastos resulta muy similar que para las mujeres. sub_Hombre %&gt;% summarise(Razon = survey_ratio( numerator = Expenditure, denominator = Income, level = 0.95, vartype = c(&quot;se&quot;, &quot;ci&quot;) )) ## # A tibble: 1 × 4 ## Razon Razon_se Razon_low Razon_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.639 0.0288 0.582 0.696 4.3.2 Prueba de hipótesis para la diferencia de medias Se le llama prueba de hipótesis a un procedimiento estadístico utilizado para evaluar la evidencia en favor o en contra de una afirmación o suposición acerca de una población. En este proceso, se plantea una hipótesis nula (\\(H_0\\)), que representa la afirmación inicial que debe ser probada, y una hipótesis alternativa (\\(H_1\\)), que es la afirmación contraria a la hipótesis nula. Esta afirmación puede estar basada en alguna creencia o experiencia pasada que será contrastada con la evidencia que se obtengan a través de la información contenida en la muestra. Si se sospecha que el parámetro \\(\\theta\\) es igual a cierto valor particular \\(\\theta_{0}\\), las posible combinaciones de hipótesis que se pueden contrastar son: \\[\\begin{eqnarray*} \\begin{cases} H_{0}: &amp; \\theta=\\theta_{0}\\\\ H_{1}: &amp; \\theta\\neq\\theta_{0} \\end{cases}\\,\\,\\,\\,\\,\\,\\,\\,\\, \\begin{cases} H_{0}: &amp; \\theta=\\theta_{0}\\\\ H_{1}: &amp; \\theta&gt;\\theta_{0} \\end{cases}\\,\\,\\,\\,\\,\\,\\,\\,\\, \\begin{cases} H_{0}: &amp; \\theta=\\theta_{0}\\\\ H_{1}: &amp; \\theta&lt;\\theta_{0} \\end{cases} \\end{eqnarray*}\\] Se dirá que una de las dos hipótesis es cierta sólo si la evidencia estadística, la cual es obtenida de la muestra, la apoya. El proceso por medio del cual se escoge una de las dos hipótesis es llamado Prueba de Hipótesis. En términos generales, algunos parámetros importantes se pueden escribir como una combinación lineal de medidas de interés. Los casos más usuales son diferencias de medias, sumas ponderadas de medias utilizadas para construir índices económicos, etc. De esta forma, considere una función que es una combinación lineal de \\(j\\) estadísticas descriptivas como se muestra a continuación: \\[\\begin{eqnarray*} f\\left(\\theta_{1},\\theta_{2},...,\\theta_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}\\theta_{j} \\end{eqnarray*}\\] En \\(a_{j}\\) son constantes conocidas. Un estimador de esta función está dado por: \\[\\begin{eqnarray} \\hat{f}_{\\omega}\\left(\\hat{\\theta}_{1},\\hat{\\theta}_{2},...,\\hat{\\theta}_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}\\hat{\\theta}_{j} \\end{eqnarray}\\] Y su varianza se calcula como sigue: \\[\\begin{eqnarray} var\\left(\\sum_{j=1}^{J}a_{j}\\hat{\\theta}_{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}^{2}var\\left(\\hat{\\theta}_{j}\\right)+2\\times\\sum_{j=1}^{J-1}\\sum_{k&gt;j}^{J}a_{j}a_{k}\\,cov\\left(\\hat{\\theta}_{j},\\hat{\\theta}_{k}\\right) \\end{eqnarray}\\] Como se pudo observar en la ecuación de la varianza del estimador, esta incorpora las varianzas de las estimaciones de todos los componentes individuales, así como las covarianzas de los pares estimados. Es de nuestro particular interés analizar la diferencia de medias poblacionales, que se puede escribir como sigue: \\[ \\bar{y}^{1}-\\bar{y}^{2} \\] En donde, \\(\\bar{y}_{1}\\) es la media de la primera población, por ejemplo, ingresos medios en los hogares obtenido por los padres de familia y \\(\\bar{y}_{2}\\) es la media de la segunda población, que para seguir el ejemplo serían, los ingresos medios de las madres en un hogar. Este parámetro puede ser estimado insesgadamente por: \\[ \\hat{\\bar{y}}_{\\omega}^{1}-\\hat{\\bar{y}}_{\\omega}^{2} \\] En donde \\(\\hat{\\bar{y}}_{\\omega}^{i}\\) es el estimador de muestreo de \\(\\bar{y}^{i}\\) (\\(i = 1, 2.\\)). Considerando el parámetro de interés en esta sección, las hipótesis a estudiar serían las siguientes: \\[\\begin{eqnarray*} \\begin{cases} H_{0}:\\bar{y}_{1}-\\bar{y}_{2}=0\\\\ H_{1}:\\bar{y}_{1}-\\bar{y}_{2}\\neq0 \\end{cases} &amp; \\begin{cases} H_{0}:\\bar{y}_{1}-\\bar{y}_{2}=0\\\\ H_{1}:\\bar{y}_{1}-\\bar{y}_{2}&gt;0 \\end{cases} &amp; \\begin{cases} H_{0}:\\bar{y}_{1}-\\bar{y}_{2}=0\\\\ H_{1}:\\bar{y}_{1}-\\bar{y}_{2}&lt;0 \\end{cases} \\end{eqnarray*}\\] Para probar estas hipótesis se utiliza el siguiente estadístico de prueba, el cual tiene distribución t-student con \\(gl\\) grados de libertad, los cuales están dados por la diferencia entre el número de UPM y el número de estratos. \\[ t = \\frac{\\hat{\\bar{y}}_{\\omega}^{1}-\\hat{\\bar{y}}_{\\omega}^{2}}{se\\left(\\bar{y}_{1}-\\bar{y}_{2}\\right)} \\sim t_{gl} \\] donde, \\[ \\widehat{se}\\left(\\hat{\\bar{y}}_{\\omega}^{1}-\\hat{\\bar{y}}_{\\omega}^{2}\\right) = \\sqrt{\\widehat{var}\\left(\\hat{\\bar{y}}_{\\omega}^{1}\\right) + \\widehat{var}\\left(\\hat{\\bar{y}}_{\\omega}^{2}\\right) - 2 \\ \\widehat{cov}\\left(\\hat{\\bar{y}}_{\\omega}^{1},\\hat{\\bar{y}}_{\\omega}^{2}\\right)} \\] Si se desea construir un intervalo de confianza para la diferencia de media se realizaría de la siguiente manera: \\[ \\hat{\\bar{y}}_{\\omega}^{1}-\\hat{\\bar{y}}_{\\omega}^{2} \\pm t_{gl}\\ \\widehat{se}\\left(\\hat{\\bar{y}}_{\\omega}^{1}-\\hat{\\bar{y}}_{\\omega}^{2}\\right) \\] Para poder llevar a cabo la prueba de hipótesis para la diferencia de media de los ingresos en un hogar por sexo, tomemos la base de datos que tenemos como ejemplo. La función que se encarga de realizar la prueba es svyttest y solo requiere como argumentos la variable ingreso (o variable de interés), la variable sexo (variable discriminadora), el diseño muestral y el nivel de confianza. A continuación, se muestran los códigos computacionales que se requieren: svyttest(Income ~ Sex, design = diseno, level = 0.95) ## ## Design-based t-test ## ## data: Income ~ Sex ## t = 1.3625, df = 118, p-value = 0.1756 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -12.82205 69.38503 ## sample estimates: ## difference in mean ## 28.28149 En esta salida podemos observar que el p-valor de la prueba es 0.17. Si tomamos una significancia del 5% para la prueba se puede concluir que, con una confianza del 95% y basados en la muestra, no existe suficiente evidencia estadística para decir que los ingresos medios en los hogares son diferentes por sexo. Por otro lado, el intervalo de confianza al 95% para la diferencia de medias entre los ingresos de hombres y mujeres es \\(\\left(-12.8,\\,69.3\\right)\\). Si ahora el objetivo es realizar la prueba de diferencia de medias para los ingresos entre hombres y mujeres pero solo en la zona urbana, los códigos computacionales son los siguientes: svyttest(Income ~ Sex, design = sub_Urbano, level = 0.95) ## ## Design-based t-test ## ## data: Income ~ Sex ## t = 1.5667, df = 63, p-value = 0.1222 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -12.31754 101.74023 ## sample estimates: ## difference in mean ## 44.71134 En donde, al igual que el anterior, no se rechaza la hipótesis nula con una confianza del 95%. Por otro lado, la función svyttest permite usar filtros. Si se requiere probar la hipótesis de diferencia de medias de ingresos por sexo pero solo en aquellas personas del hogar mayores a 18 años, se utilizará dentro de la función svyttest la función filter como se muestra a continuación: svyttest(Income ~ Sex, design = diseno %&gt;% filter(Age &gt; 18), level = 0.95) ## ## Design-based t-test ## ## data: Income ~ Sex ## t = 1.5263, df = 118, p-value = 0.1296 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -10.72746 82.85253 ## sample estimates: ## difference in mean ## 36.06253 y con una confianza del 95% y basado en la muestra tampoco se rechaza la hipótesis hula. Es decir, no existe evidencia estadística para concluir que los ingresos medios entre hombres y mujeres mayores de 18 años son diferentes. 4.3.3 Contrastes En muchas ocasiones, se requiere comparar más de dos poblaciones al mismo tiempo; por ejemplo, comparar los ingresos medios de los hogares en tres regiones con el fin de identificar aquellas regiones donde hubo un mayor impacto en los hogares. En casos como estos la diferencia de medias que estudiamos en capítulos anteriores no es suficiente dado que permite solo comprar parejas de poblaciones y por ende que, hacer contraste resulta una muy buena alternativa para abordar este tipo de problemas. Recurriendo en las definiciones que se han trabajado en este capítulo, un contraste es una combinación lineal de parámetros de la forma: \\[\\begin{eqnarray*} f = A * \\theta = f\\left(\\theta^{1},\\theta^{2},...,\\theta^{j}\\right) &amp; = &amp; \\sum_{j=1}^{J}a_{j}\\theta^{j} \\end{eqnarray*}\\] En donde \\(A\\) es una matriz o vector de constantes y \\(\\theta\\) es una matriz o vector de parámetros. Los procedimientos metodológicos para implementar los contrastes en diseños de muestreo complejos están desarrolladas en la función svycontrast. A continuación, se muestra el uso de dicha función para el cálculo de contraste en la base de datos de ejemplo, comparando el promedio de ingresos por región. Como primer ejemplo, se realizará la comparación de dos poblaciones, las regiones Norte y Sur. En particular, es de interés la diferencia de ingresos (\\(f = \\bar{y}^{Norte} - \\bar{y}^{Sur}\\)). Puesto que la población tiene cinco regiones y solo se construirá el contraste para dos de ellas (la región Norte y la región Sur), este queda definido de la siguiente manera: \\[\\begin{eqnarray*} f = A * \\theta = 1\\times{\\bar{y}}^{Norte}+\\left(-1\\right)\\times{\\bar{y}}^{Sur}+0\\times{\\bar{y}}^{Centro}+0\\times{\\bar{y}}^{Occidente}+0\\times{\\bar{y}}^{Oriente} \\end{eqnarray*}\\] Como se puede observar, en este caso el vector de contraste es \\(A = \\left[1,\\,-1,\\,0,\\,0,\\,0\\right]\\). De forma matricial, el estimador de este contraste específico queda definido de la siguiente manera: \\[\\begin{eqnarray*} \\hat{f}_{\\omega} = A * \\hat{\\theta} = \\left[1,\\,-1,\\,0,\\,0,\\,0\\right]\\times\\left[\\begin{array}{c} \\hat{\\bar{y}}^{Norte}_{\\omega}\\\\ \\hat{\\bar{y}}^{Sur}_{\\omega}\\\\ \\hat{\\bar{y}}^{Centro}_{\\omega}\\\\ \\hat{\\bar{y}}^{Occidente}_{\\omega}\\\\ \\hat{\\bar{y}}^{Oriente}_{\\omega} \\end{array}\\right] \\end{eqnarray*}\\] Ahora bien, para realizar procesos de la construcción del estimador del contraste y su varianza estimada paso a paso se procede a calcular las medias estimadas por región con la función svyby como se muestra a continuación: prom_region &lt;- svyby( formula = ~ Income, by = ~ Region, design = diseno, FUN = svymean, na.rm = T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;) ) prom_region ## Region Income se ci_l ci_u ## Norte Norte 552.3637 55.35987 443.8603 660.8670 ## Sur Sur 625.7740 62.40574 503.4610 748.0870 ## Centro Centro 650.7820 61.46886 530.3053 771.2588 ## Occidente Occidente 517.0071 46.22077 426.4161 607.5982 ## Oriente Oriente 541.7543 71.66487 401.2938 682.2149 La función svyby permite aplicar una función, en este caso la media (svymean) por región (by) utilizando el diseño muestral complejo de la encuesta (design). Las demás componentes de la función ya se han utilizado previamente. Como resultado de aplicar esta función se obtienen las medias estimadas de los ingresos por región. Para continuar con el ejemplo, se tomarán solo los ingresos medios estimados de las regiones Norte y Sur y calcularemos su diferencia: # Paso 1: diferencia de estimaciones (Norte - Sur) 552.4 - 625.8 ## [1] -73.4 El paso siguiente es calcular la matriz de varianzas y covarianzas y de allí extraer las varianzas y covarianzas de las regiones Norte y Sur: # Paso 2: Matriz de varianzas y covarianzas vcov(prom_region) ## Norte Sur Centro Occidente Oriente ## Norte 3064.715 0.000 0.00 0.000 0.000 ## Sur 0.000 3894.476 0.00 0.000 0.000 ## Centro 0.000 0.000 3778.42 0.000 0.000 ## Occidente 0.000 0.000 0.00 2136.359 0.000 ## Oriente 0.000 0.000 0.00 0.000 5135.854 Como el muestreo es independiente en cada región, las covarianzas en la anterior matriz son nulas. Para calcular el error estándar de la diferencia (contraste) se usará las propiedades de la varianza, puesto que: \\[ se(\\hat{f}_{\\omega}) = se\\left(\\hat{\\bar{y}}^{Norte}_{\\omega}-\\hat{\\bar{y}}^{Sur}_{\\omega}\\right)=\\sqrt{var\\left(\\hat{\\bar{y}}^{Norte}_{\\omega}\\right)+var\\left(\\hat{\\bar{y}}^{Sur}_{\\omega}\\right)-2\\,cov\\left(\\hat{\\bar{y}}^{Norte}_{\\omega},\\hat{\\bar{y}}^{Sur}_{\\omega}\\right)} \\] Por tanto, tenemos que el error estándar estimado para este contraste es: sqrt(3065 + 3894 - 2 * 0) ## [1] 83.42062 Ahora bien, la función svycontrast devuelve el contraste estimado y su error estándar. Los argumentos de esta función son los promedios de los ingresos estimados (stat) y las constantes de contraste (contrasts). Se obtiene como resultado que la diferencia de los ingresos medios se estima en 73.4 unidades monetarias con un error estándar de 83.42 unidades. svycontrast(stat = prom_region, contrasts = list(diff_NS = c(1, -1, 0, 0, 0))) ## contrast SE ## diff_NS -73.41 83.422 Por otro lado, es posible ampliar la estructura de los contrastes. Por ejemplo, considere que se quieren estimar los siguientes contrastes: \\(\\bar{y}^{Norte} - \\bar{y}^{Centro}\\), \\(\\bar{y}^{Sur}-\\bar{y}^{Centro}\\), \\(\\bar{y}^{Occidente}-\\bar{y}^{Oriente}\\) En este caso, la matriz de contrastes escrita de forma matricial tendría la siguiente estructura: \\[ A = \\left[\\begin{array}{ccccc} 1 &amp; 0 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\end{array}\\right] \\] Ahora, aplicando la función svycontrast en R se obtiene: svycontrast( stat = prom_region, contrasts = list( Norte_sur = c(1, 0,-1, 0, 0), Sur_centro = c(0, 1,-1, 0, 0), Occidente_Oriente = c(0, 0, 0, 1,-1) ) ) ## contrast SE ## Norte_sur -98.418 82.723 ## Sur_centro -25.008 87.595 ## Occidente_Oriente -24.747 85.277 También es posible construir contrastes en dominios que estén correlacionadas. Por ejemplo, Ingreso y Sexo. Como se hizo en el ejemplo anterior, se inicia con el promedio estimado por sexo. prom_sexo &lt;- svyby( formula = ~ Income, by = ~ Sex, design = diseno, FUN = svymean, na.rm = T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;) ) prom_sexo ## Sex Income se ci_l ci_u ## Female Female 557.5681 25.82995 506.9423 608.1939 ## Male Male 585.8496 34.58759 518.0592 653.6400 Si el contraste de interés es \\(\\theta = \\bar{y}^{F} - \\bar{y}^{M}\\), entonces usando la función svycontrast se obtiene el contraste estimado, junto con su error estándar, obteniendo como resultado que, en promedio, los hombres obtienen 28.3 unidades monetarias más que las mujeres con una error estándar de 20.76. svycontrast(stat = prom_sexo, contrasts = list(diff_Sexo = c(1,-1))) ## contrast SE ## diff_Sexo -28.281 20.756 Más aún, otra posibilidad es poder obtener resultados agregados, por ejemplo, con sumas de totales en regiones: \\[ \\hat{f}_{\\omega}=\\hat{y}^{Norte}_{\\omega}+\\hat{y}^{Sur}_{\\omega} +\\hat{y}^{Centro}_{\\omega} \\] En este caso, se crea la función adecuada con sumas de totales por regiones sum_region &lt;- svyby( ~ Income, ~ Region, diseno, svytotal, na.rm = T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;) ) sum_region ## Region Income se ci_l ci_u ## Norte Norte 14277323 1507575 11322530 17232115 ## Sur Sur 16068151 1877989 12387359 19748942 ## Centro Centro 16483319 2383556 11811634 21155003 ## Occidente Occidente 16853540 1823807 13278944 20428135 ## Oriente Oriente 22111335 2833460 16557856 27664814 Para este caso en particular, la matriz de contraste queda como: \\[ \\left[\\begin{array}{cccccc} 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\end{array}\\right] \\] Una vez más, el procedimiento en R es como sigue: svycontrast(stat = sum_region, contrasts = list(Agregado_NCS = c(1, 1, 1, 0, 0))) ## contrast SE ## Agregado_NCS 46828792 3388357 Por último, si se desean obtener los promedios por categorías. Por ejemplo: \\[ \\hat{f} = \\hat{\\bar{y}}_{Edad} = \\frac{1}{J}\\sum_{j=1}^J\\hat{\\bar{y}}_{j} \\] donde \\(K\\) es el número de categorías de la variable. En R se hace de la siguiente manera: prom_edad &lt;- svyby( formula = ~ Income, by = ~ CatAge, design = diseno, FUN = svymean, na.rm = T, covmat = TRUE ) prom_edad ## CatAge Income se ## 0-5 0-5 463.7715 28.86795 ## 6-15 6-15 511.6179 34.88031 ## 16-30 16-30 607.2917 37.41561 ## 31-45 31-45 573.4167 26.94744 ## 46-60 46-60 763.0610 58.97170 ## Más de 60 Más de 60 466.6133 31.20795 Cuyo vector de contraste estaría dada por: \\[ A = \\left[\\begin{array}{cccccc} \\frac{1}{6} &amp; \\frac{1}{6} &amp; \\frac{1}{6} &amp; \\frac{1}{6} &amp; \\frac{1}{6} &amp; \\frac{1}{6} \\end{array}\\right] \\] El procedimiento en R es: svycontrast(stat = prom_edad, contrasts = list(agregado_edad = rep(1 / 6, times = 6))) ## contrast SE ## agregado_edad 564.3 25.404 También es posible realizar contrastes con parámetros más complejos, como por ejemplo razones. Considere la diferencia por sexo entre las razones de gastos contra ingresos. En primer lugar se debe crear la función que permita estimar las razones por sexo: razon_sexo &lt;- svyby( formula = ~ Income, by = ~ Sex, denominator = ~ Expenditure, design = diseno, FUN = svyratio, na.rm = T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;) ) razon_sexo ## Sex Income/Expenditure se.Income/Expenditure ci_l ci_u ## Female Female 1.519060 0.04582607 1.429243 1.608878 ## Male Male 1.564762 0.07044239 1.426698 1.702827 Luego, la estimación de la diferencia de razones, estaría dada por el siguiente contraste, en donde se puede concluir que la diferencia de las razones es estimada en 0.045 en favor de los hombres. svycontrast(stat = razon_sexo, contrasts = list(diff_sexo = c(1,-1))) ## contrast SE ## diff_sexo -0.045702 0.0416 References "],["análisis-de-variables-categóricas.html", "Capítulo 5 Análisis de variables categóricas", " Capítulo 5 Análisis de variables categóricas Una variable categórica es aquella que representa categorías, clases o grupos distintos. Estas categorías pueden o no tener un orden inherente entre ellas y se utilizan para organizar la información en grupos discretos. En general, las variables categóricas pueden ser nominales (no tienen un orden específico, como la raza, el estado civil, o la región geográfica) o pueden ser ordinales (en donde las categorías sí tienen un orden). Estas variables son fundamentales en el análisis de los datos, ya que permiten clasificar y organizar la información en subgrupos que pueden facilitar la creación de políticas públicas. En ocasiones, desde las mismas variables continuas es posible considerar grupos o particiones que dan como resultado la creación de variables categóricas al dividir el rango de valores de la variable en intervalos. Un ejemplo de esto es la variable edad, que en una encuesta de hogares se pregunta como variable cuantitativa, pero que se puede dividir, por ejemplo, en las siguientes categorías: primera infancia (de 0 a 5 años), niñez (de 6 a 11 años), adolescencia (de 12 a 18 años), juventud (de 19 a 26 años), adultez (de 27 a 59 años), adulto Mayor (60 años o más). "],["definición-del-diseño-de-muestreo-1.html", "5.1 Definición del diseño de muestreo", " 5.1 Definición del diseño de muestreo Se inicia este capítulo haciendo la definición del diseño de muestreo (como se mostró en capítulos anteriores) usando como ejemplo la misma base de datos del capítulo anterior. library(tidyverse) library(survey) library(srvyr) options(survey.lonely.psu = &quot;adjust&quot;) encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = TRUE ) A continuación, para efectos del ejemplo, se generan tres nuevas variables dicotómicas que indican si la persona encuestada está en estado de pobreza, o no; si está desempleada, o no; y si es mayor de 18 años, o no. Estas nuevas variables categórica nacen de variables propias de la encuesta, como lo son el ingreso percápita, el estado de ocupación y la edad en años diseno &lt;- diseno %&gt;% mutate( pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0), desempleo = ifelse(Employment == &quot;Unemployed&quot;, 1, 0), edad_18 = case_when(Age &lt; 18 ~ &quot;&lt; 18 anios&quot;, TRUE ~ &quot;&gt;= 18 anios&quot;) ) Como se pudo observar en el código anterior, se ha introducido la función case_when la cual es una extensión de la función ifelse que permite crear múltiples categorías a partir de una o varias condiciones. Asimismo, como se ha mostrado anteriormente, en ocasiones se desea realizar estimaciones por subpoblación; en este caso se extraen cuatro subgrupos de la encuesta y se definen a continuación: sub_Urbano &lt;- diseno %&gt;% filter(Zone == &quot;Urban&quot;) sub_Rural &lt;- diseno %&gt;% filter(Zone == &quot;Rural&quot;) sub_Mujer &lt;- diseno %&gt;% filter(Sex == &quot;Female&quot;) sub_Hombre &lt;- diseno %&gt;% filter(Sex == &quot;Male&quot;) "],["estimación-puntual-1.html", "5.2 Estimación puntual", " 5.2 Estimación puntual La estimación precisa de tamaños absolutos y proporciones en encuestas de hogares es fundamental para obtener datos representativos que reflejen la realidad demográfica y socioeconómica de una población. Estas cifras sirven como base para la toma de decisiones de política pública, para la asignación de recursos y para el diseño de programas sociales. La capacidad de entender la distribución de categorías específicas, como situación de pobreza, estado de ocupación, escolaridad, entre otras, aporta información valiosa para abordar desigualdades y promover el desarrollo equitativo. 5.2.1 Estimaciones de tamaños En esta sección se realizarán los procesos de estimación de variables categóricas. En primera instancia, uno de los parámetros más importantes es el tamaño de una población, que representa la cardinalidad de ese conjunto; es decir, el número total de integrantes que lo componen. En términos de notación, el tamaño de la población se estima de la siguiente manera: \\[\\begin{eqnarray} \\hat{N}_{\\omega} = \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i} \\end{eqnarray}\\] De la misma manera, la estimación del tamaño en una subpoblación está definida por una variable dicotómica \\(I(y_i = d)\\), que toma el valor uno si el individuo \\(i\\) pertenece a la categoría \\(d\\) en la variable discreta, está dada por la siguiente expresión: \\[\\begin{eqnarray} \\hat{N}^d_{\\omega} = \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}I(y_i = d) \\end{eqnarray}\\] A continuación, se presenta la forma apropiada para estimar los tamaños de la población finita y sus subpoblaciones. diseno %&gt;% group_by(Zone) %&gt;% cascade(n = unweighted(n()), Nd = survey_total(vartype = c(&quot;se&quot;, &quot;ci&quot;)), .fill = &quot;Poblacional&quot;) %&gt;% arrange(desc(Zone)) ## # A tibble: 3 × 6 ## Zone n Nd Nd_se Nd_low Nd_upp ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Urban 1308 78164. 2847. 72526. 83802. ## 2 Rural 1297 72102. 3062. 66039. 78165. ## 3 Poblacional 2605 150266. 4181. 141986. 158546. En la tabla anterior, n denota el número de observaciones en la muestra por Zona y Nd denota la estimación del tamaño (número de personas) en cada subpoblación. Adicionalmente, en el código anterior se introdujo la función unweighted, que calcula resúmenes no ponderados a partir de un conjunto de datos de encuestas. Para el ejemplo, el tamaño de muestra en la zona rural fue de 1297 personas y para la urbana fue de 1308. Con esta información se logró estimar una población de 72102 con un error estándar de 3062 en la zona rural; además, se estimó una población de 78164 en la zona urbana con un error estándar de 2847. Así mismo, con una confianza del 95% se construyeron los intervalos de confianza para el tamaño de las poblaciones que, en la zona rural está entre 66038 y 78165, mientras que para la urbana están entre 72526 y 83801. Ahora bien, empleando una sintaxis similar a la anterior, es posible estimar el número de personas en condición de pobreza extrema, pobreza relativa y personas no pobres como sigue: diseno %&gt;% group_by(Poverty) %&gt;% summarise(Nd = survey_total(vartype = c(&quot;se&quot;, &quot;ci&quot;))) ## # A tibble: 3 × 5 ## Poverty Nd Nd_se Nd_low Nd_upp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NotPoor 91398. 4395. 82696. 100101. ## 2 Extreme 21519. 4949. 11719. 31319. ## 3 Relative 37349. 3695. 30032. 44666. De la tabla anterior podemos concluir que, la cantidad estimada de personas que no se encuentran en pobreza es de 91398: mientras que 37348 personas se encuentran en pobreza y 21518 en pobreza extrema. Los demás parámetros estimados se interpretan de la misma manera que para la estimación desagregada por zona. En forma similar, es posible estimar el número total de personas que están por debajo de la línea de pobreza. diseno %&gt;% group_by(pobreza) %&gt;% summarise(Nd = survey_total(vartype = c(&quot;se&quot;, &quot;ci&quot;))) ## # A tibble: 2 × 5 ## pobreza Nd Nd_se Nd_low Nd_upp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 91398. 4395. 82696. 100101. ## 2 1 58868. 5731. 47519. 70216. Concluyendo que, 58867 personas están por debajo de la línea de pobreza con un error estándar de 5731 y un intervalo de confianza que va desde 47518 hasta 70216. Otra variable de interés en encuestas de hogares es el estado de ocupación de las personas. A continuación, se muestra el código computacional que estima el tamaño de cada una de sus categorías: diseno %&gt;% group_by(Employment) %&gt;% summarise(Nd = survey_total(vartype = c(&quot;se&quot;, &quot;ci&quot;))) ## # A tibble: 4 × 5 ## Employment Nd Nd_se Nd_low Nd_upp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Unemployed 4635. 761. 3129. 6141. ## 2 Inactive 41465. 2163. 37183. 45748. ## 3 Employed 61877. 2540. 56847. 66907. ## 4 &lt;NA&gt; 42289. 2780. 36784. 47794. De los resultados de la función, se puede estimar que 4634 personas están desempleadas con un intervalo de confianza entre 3128 y 6140. Además, se estima que 41465 personas están inactivas, con un intervalo de confianza entre 37182 y 45747. Por último, se estima que 61877 personas están ocupadas con un intervalo de confianza entre 36784 y 47793. 5.2.2 Estimación de proporciones La estimación de una proporción para una variable de respuesta binaria requiere una extensión directa del estimador de razón mostrado en el capítulo anterior. Como lo mencionan Heeringa, West, y Berglund (2017), al recodificar las categorías de respuesta originales en una sola variable indicadora \\(y_{i}\\) con valores posibles de 1 y 0 (por ejemplo, sí = 1, no = 0), se define el estimador de una proporción de la siguiente manera: \\[\\begin{eqnarray} \\hat{p}_{\\omega}^d = \\frac{\\hat{N}^d_{\\omega}}{\\hat{N}_{\\omega}} = \\frac{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\ I(y_i = d)}{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}} \\end{eqnarray}\\] Aplicando Linealización de Taylor al anterior estimador, se tiene que su varianza está dada por la siguiente expresión: \\[ var\\left(\\hat{p}_{\\omega}^d\\right) \\dot{=} \\frac{var\\left(\\hat{N}^{d}_{\\omega}\\right)+(\\hat{p}_{\\omega}^d)^{2}var\\left(\\hat{N}_{\\omega}\\right)-2\\,\\hat{p}_{\\omega}^d\\,cov\\left(\\hat{N}^{d}_{\\omega},\\hat{N}_{\\omega}\\right)}{(\\hat{N}_{\\omega})^{2}} \\] Es normal observar que muchos paquetes estadísticos opten por generar estimaciones de proporciones y errores estándar en la escala de porcentaje. R genera las estimaciones de proporciones dentro del intervalo [0,1]. A continuación, se presenta el código computacional para estimar la proporción de personas por zona: diseno %&gt;% group_by(Zone) %&gt;% summarise(prop = survey_mean(vartype = c(&quot;se&quot;, &quot;ci&quot;), proportion = TRUE)) ## # A tibble: 2 × 5 ## Zone prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 0.480 0.0140 0.452 0.508 ## 2 Urban 0.520 0.0140 0.492 0.548 Como se pudo observar, se usó la función survey_mean para la estimación. Sin embargo, con el parámetro proportion = TRUE, se le indica a R que lo que se desea estimar es una proporción. Para este ejemplo se puede estimar que el 47.9% de las personas viven en zona rural obteniendo un intervalo de confianza comprendido entre (45.2%, 50.7%); además el 52% de las personas viven en la zona urbana con un intervalo de confianza de (49.2%, 54.7%). La librería survey tiene implementado una función específica para estimar proporciones la cual es survey_prop que genera los mismos resultados mostrados anteriormente. Le queda al lector la decisión de usar la función con la que más cómodo se sienta. A continuación, se muestra un ejemplo del uso de la función survey_prop. diseno %&gt;% group_by(Zone) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) ## # A tibble: 2 × 5 ## Zone prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 0.480 0.0140 0.452 0.508 ## 2 Urban 0.520 0.0140 0.492 0.548 Como es bien sabido en la literatura especializada, cuando la proporción de interés estimada está cerca de cero o de uno, los límites del intervalo de confianza tradicional, basados en el diseño de muestreo, pueden salirse de los rangos permitidos para las proporciones. Lo anterior no tendría ninguna interpretación por la naturaleza del parámetro. Es por esto que, para solventar este problema, se pueden realizar estimaciones alternativas de los intervalos de confianza basados en el diseño de muestreo como lo proponen Rust, Hsu, y Westat (2007) y Dean y Pagano (2015). De esta manera, el intervalo de confianza utilizando la transformación \\(Logit\\left(p\\right)\\) está dado por: \\[ IC\\left[logit\\left(p^d\\right)\\right] = \\left\\{ ln\\left(\\frac{\\hat{p}_{\\omega}^d}{1-\\hat{p}_{\\omega}^d}\\right)\\pm\\frac{t_{1-\\alpha/2,\\,gl} \\times se\\left(\\hat{p}_{\\omega}^d\\right)}{\\hat{p}_{\\omega}^d\\left(1-\\hat{p}_{\\omega}^d\\right)}\\right\\} \\] Por tanto, el intervalo de confianza para \\(p^d\\) sería: \\[\\begin{eqnarray} IC\\left(p^d\\right) = \\left\\{ \\frac{exp\\left[ln\\left(\\frac{\\hat{p}_{\\omega}^d}{1-\\hat{p}_{\\omega}^d}\\right)\\pm\\frac{t_{1-\\alpha/2,\\,gl}\\times se\\left(\\hat{p}_{\\omega}^d\\right)}{\\hat{p}_{\\omega}^d\\left(1-\\hat{p}_{\\omega}^d\\right)}\\right]}{1+exp\\left[ln\\left(\\frac{\\hat{p}_{\\omega}^d}{1-\\hat{p}_{\\omega}^d}\\right)\\pm\\frac{t_{1-\\alpha/2,\\,gl}\\times se\\left(\\hat{p}_{\\omega}^d\\right)}{\\hat{p}_{\\omega}^d\\left(1-\\hat{p}_{\\omega}^d\\right)}\\right]}\\right\\} \\end{eqnarray}\\] A continuación, siguiendo con la base de ejemplo, se estima la proporción de hombres y mujeres en pobreza y no pobreza junto con su error estándar e intervalos de confianza. diseno %&gt;% group_by(pobreza, Sex) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) ## # A tibble: 4 × 6 ## # Groups: pobreza [2] ## pobreza Sex prop prop_se prop_low prop_upp ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 Female 0.529 0.0124 0.505 0.554 ## 2 0 Male 0.471 0.0124 0.446 0.495 ## 3 1 Female 0.524 0.0159 0.492 0.555 ## 4 1 Male 0.476 0.0159 0.445 0.508 Como se puede observar, se ha estimado que entre las personas en condición de pobreza, el 52.3% son mujeres y el 47.6% son hombres; generando intervalos de confianza al 95% de (49.2%, 55.5%) para las mujeres y (44.5%, 50.7%) para los hombres. En la librería survey existe una alternativa para estimar tablas de proporciones utilizando la función svyby. Los argumentos que requiere la función se definen a partir de la la variable que se desea estimar (formula), las categorías por la cual se desea estimar (by), el diseño muestral (desing) y el parámetro que se desea estimar (FUN). A continuación, se ejemplifica el uso de la función: tab_Sex_Pobr &lt;- svyby( formula = ~ Sex, by = ~ pobreza, design = diseno, FUN = svymean ) tab_Sex_Pobr ## pobreza SexFemale SexMale se.SexFemale se.SexMale ## 0 0 0.5291800 0.4708200 0.01242026 0.01242026 ## 1 1 0.5236123 0.4763877 0.01586237 0.01586237 Para la estimación de los intervalos de confianza (que coinciden con los generados anteriormente usando la funicón group_by.) se utiliza la función confint como sigue: confint(tab_Sex_Pobr) ## 2.5 % 97.5 % ## 0:SexFemale 0.5048367 0.5535232 ## 1:SexFemale 0.4925226 0.5547019 ## 0:SexMale 0.4464768 0.4951633 ## 1:SexMale 0.4452981 0.5074774 Otro análisis de interés relacionado con tablas de doble entrada en encuestas de hogares es estimar el porcentaje de desempleados por sexo. tab_Sex_Ocupa &lt;- svyby( formula = ~ Sex, by = ~ Employment, design = diseno, FUN = svymean ) tab_Sex_Ocupa ## Employment SexFemale SexMale se.SexFemale se.SexMale ## Unemployed Unemployed 0.2726730 0.7273270 0.05351318 0.05351318 ## Inactive Inactive 0.7703406 0.2296594 0.02340005 0.02340005 ## Employed Employed 0.4051575 0.5948425 0.01851986 0.01851986 De la anterior salida se puede estimar que, dentro de los desempleado, el 27.2% son mujeres y el 72.7% son. Por la naturaleza simétrica de las proporciones con dos únicos grupos, los errores estándares para estas estimaciones coinciden y se estiman en 5.3%. Los intervalos de confianza se muestran a continuación: confint(tab_Sex_Ocupa) ## 2.5 % 97.5 % ## Unemployed:SexFemale 0.1677891 0.3775570 ## Inactive:SexFemale 0.7244773 0.8162038 ## Employed:SexFemale 0.3688592 0.4414557 ## Unemployed:SexMale 0.6224430 0.8322109 ## Inactive:SexMale 0.1837962 0.2755227 ## Employed:SexMale 0.5585443 0.6311408 Si ahora el objetivo es estimar la pobreza, pero por las distintas regiones que se tienen en la base de datos, lo primero que se debe realizar es la conversión de la variable pobreza, la cual de de tipo numérica, en tipo factor; luego se realiza la estimación con la función svyby. svyby( formula = ~ as.factor(pobreza), by = ~ Region, design = diseno, FUN = svymean ) ## Region as.factor(pobreza)0 as.factor(pobreza)1 ## Norte Norte 0.6410318 0.3589682 ## Sur Sur 0.6561536 0.3438464 ## Centro Centro 0.6346152 0.3653848 ## Occidente Occidente 0.5991839 0.4008161 ## Oriente Oriente 0.5482079 0.4517921 ## se.as.factor(pobreza)0 se.as.factor(pobreza)1 ## Norte 0.05547660 0.05547660 ## Sur 0.04348901 0.04348901 ## Centro 0.07858599 0.07858599 ## Occidente 0.04670473 0.04670473 ## Oriente 0.08849644 0.08849644 De lo anterior se puede concluir que, en la región Norte, el 35% de las personas están en estado de pobreza mientras que en el sur es el 34%. La pobreza más alta se tiene en la región oriente con una estimación de 45%. Si el interés ahora se centra en estimar proporciones en subpoblaciones desagregadas, por zona, el código computacional apropiado es el siguiente: sub_Urbano %&gt;% group_by(Sex) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) ## # A tibble: 2 × 5 ## Sex prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 0.537 0.0130 0.511 0.563 ## 2 Male 0.463 0.0130 0.437 0.489 Arrojando como resultado una estimación enn donde el 53.6% de las mujeres y 46.4% de los hombres viven en la zona urbana con intervalos de confianza entre (51%, 56.2%) y (43.7%, 48.9%), respectivamente. Realizando el mismo ejercicio anterior, pero ahora en la zona rural se tiene: sub_Rural %&gt;% group_by(Sex) %&gt;% summarise(n = unweighted(n()), prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) ## # A tibble: 2 × 6 ## Sex n prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 679 0.516 0.00824 0.500 0.533 ## 2 Male 618 0.484 0.00824 0.467 0.500 De donde se estima que el 51.6% de las mujeres y el 48.4% de los hombres viven en la zona rural con intervalos de confianza de (49.9%, 53.2%) y (46.7%, 50.0%), respectivamente. Ahora bien, si nos centramos solo en la población de hombres en la base de datos y se desea estimar la proporción de hombres por zona, el código computacional es el siguiente: sub_Hombre %&gt;% group_by(Zone) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) ## # A tibble: 2 × 5 ## Zone prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 0.491 0.0178 0.455 0.526 ## 2 Urban 0.509 0.0178 0.474 0.545 En la anterior tabla se puede observar que el 49% de los hombres están en la zona rural y el 51% en la zona urbana. Si se realiza ahora el mismo ejercicio para la mujeres, el código computacional es el siguiente: sub_Mujer %&gt;% group_by(Zone) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) ## # A tibble: 2 × 5 ## Zone prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural 0.470 0.0140 0.443 0.498 ## 2 Urban 0.530 0.0140 0.502 0.557 De la tabla anterior se puede inferir que, el 47% de las mujeres están en la zona rural y el 52% en la zona urbana. Observando también intervalos de confianza al 95% de (44%, 49%) y (50%, 55%) para las zonas rural y urbana, respectivamente. Si, dentro de la base de datos filtrada por hombres, ahora se desea estimar por varios niveles de desagregación, se debería recurrir al uso de la función group_by, la cual hace posible combinar dos o más variables dentro de un filtro. Por ejemplo, si se desea estimar la proporción de hombres por zona y en estado de pobreza, se realiza de la siguiente manera: sub_Hombre %&gt;% group_by(Zone, Poverty) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) ## # A tibble: 6 × 6 ## # Groups: Zone [2] ## Zone Poverty prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural NotPoor 0.549 0.0626 0.424 0.668 ## 2 Rural Extreme 0.198 0.0675 0.0958 0.364 ## 3 Rural Relative 0.254 0.0372 0.187 0.334 ## 4 Urban NotPoor 0.660 0.0366 0.584 0.728 ## 5 Urban Extreme 0.113 0.0245 0.0726 0.171 ## 6 Urban Relative 0.227 0.0260 0.180 0.283 De la salida anterior se puede estimar que, en la ruralidad, el 19% de los hombres están en pobreza extrema, mientras que en la zona urbana el 11% lo está. Por otro lado, se estima que el 54% de los hombres que viven en la zona rural no están en pobreza mientras que, en la zona urbana el 65% no está en esta condición. Otro parámetro de interés es estimar en encuestas de hogares es la proporción de personas en condición de pobreza asociada a la edad; por ejemplo, personas menores y mayores de 18 años. A continuación, ejemplificamos la estimación de estos subgrupos cruzado por pobreza: diseno %&gt;% group_by(edad_18, pobreza) %&gt;% summarise(Prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) ## # A tibble: 4 × 6 ## # Groups: edad_18 [2] ## edad_18 pobreza Prop Prop_se Prop_low Prop_upp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &lt; 18 anios 0 0.498 0.0373 0.425 0.572 ## 2 &lt; 18 anios 1 0.502 0.0373 0.428 0.575 ## 3 &gt;= 18 anios 0 0.665 0.0298 0.603 0.721 ## 4 &gt;= 18 anios 1 0.335 0.0298 0.279 0.397 De la anterior salida se puede observar que, el 50% de los menores de edad y el 33% de los mayores de edad están en estado de pobreza. Al observar los intervalos de confianza para los menores de edad en estado de pobreza se puede observar que, dicha estimación puede llegar, con una confianza del 95% a 57% mientras que a los mayores de edad puede llegar a 39%. Como se mencionó al inicio del capítulo, es posible categorizar una variable de tipo cuantitativo como por ejemplo la edad y cruzarla con la variable que categoriza la empleabilidad. A continuación, se estiman las proporciones de mujeres por edad y condición de ocupación: sub_Mujer %&gt;% mutate(edad_rango = case_when(Age &gt;= 18 &amp; Age &lt;= 35 ~ &quot;18 - 35&quot;, TRUE ~ &quot;Otro&quot;)) %&gt;% group_by(edad_rango, Employment) %&gt;% summarise(Prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) ## # A tibble: 7 × 6 ## # Groups: edad_rango [2] ## edad_rango Employment Prop Prop_se Prop_low Prop_upp ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18 - 35 Unemployed 0.0289 0.00914 0.0154 0.0537 ## 2 18 - 35 Inactive 0.517 0.0379 0.442 0.591 ## 3 18 - 35 Employed 0.455 0.0357 0.385 0.526 ## 4 Otro Unemployed 0.0102 0.00403 0.00462 0.0222 ## 5 Otro Inactive 0.353 0.0207 0.313 0.395 ## 6 Otro Employed 0.255 0.0217 0.214 0.300 ## 7 Otro &lt;NA&gt; 0.382 0.0223 0.339 0.427 De la anterior tabla se puede observar, entre otros que, de las mujeres con edades entre 18 y 35 años el 2.8% están desempleadas, mientras que el 45% están empleadas. Análisis similares se pueden hacer para los demás rangos de edades. References "],["relación-entre-varias-variables-1.html", "5.3 Relación entre varias variables", " 5.3 Relación entre varias variables Las tablas de contingencia y las pruebas de independencia son herramientas esenciales en el análisis de las encuestas de hogares, puesto que permiten analizar relaciones entre variables categóricas. Estas tablas organizan las estimaciones poblacionales en función de dos o más características, revelando patrones y asociaciones. Las pruebas de independencia evalúan si las variables están relacionadas o son independientes. Este análisis es crucial en investigaciones y toma de decisiones, ya que proporciona información sobre la dependencia entre factores, influyendo en la formulación de estrategias basadas en estimaciones precisas y exactas. 5.3.1 Tablas En la literatura especializada las tablas también se denominan como tablas de contingencia o tablas cruzadas. En general, una tabla se asume como un arreglo bidimensional de \\(r=1,\\ldots,R\\) filas y \\(c=1,\\ldots,C\\) columnas. Estas son herramientas muy utilizadas en el análisis de encuestas de hogares puesto que, al estar conformada por al menos dos filas y dos columnas, representan información de variables categóricas en términos de conteos de frecuencia al mismo tiempo. Estas tablas tienen el objetivo de representar de manera resumida la relación entre diferentes variables categóricas. En la muestra no expandida, estas tablas se definen con frecuencias no ponderadas como se muestra a continuación: Variable 2 Variable 1 0 1 Marginal fila 0 \\(n^{00}\\) \\(n^{01}\\) \\(n^{0+}\\) 1 \\(n^{10}\\) \\(n^{11}\\) \\(n^{1+}\\) Marginal columna \\(n^{+0}\\) \\(n^{+1}\\) \\(n^{++}\\) Mientras que, en un análisis ponderado sobre la muestra expandida, la tabla de doble entrada se presenta con la estimación poblacional de las frecuencias, justo como sigue: Variable 2 Variable 1 0 1 Marginal fila 0 \\(\\hat{N}^{00}_{\\omega}\\) \\(\\hat{N}^{01}_{\\omega}\\) \\(\\hat{N}^{0+}_{\\omega}\\) 1 \\(\\hat{N}^{10}_{\\omega}\\) \\(\\hat{N}^{11}_{\\omega}\\) \\(\\hat{N}^{1+}_{\\omega}\\) Marginal columna \\(\\hat{N}^{+0}_{\\omega}\\) \\(\\hat{N}^{+1}_{\\omega}\\) \\(\\hat{N}_{\\omega}\\) De esta manera, teniendo en cuenta que el subíndice \\(i\\in\\left(r,c\\right)\\) representa a los individuos que están clasificados en la celda (\\(r, c\\)), entonces el estimador de la frecuencia en esta celda está dado por la siguiente expresión. \\[\\begin{eqnarray} \\hat{N}^{rc}_{\\omega}={ \\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i\\in\\left(r,c\\right)}^{n_{h\\alpha}}}\\omega_{h\\alpha i} \\end{eqnarray}\\] Los estimadores de las demás frecuencias en la tabla se definen de forma similar, inclusive las marginales por fila y por columna. Las proporciones estimadas a partir de estas frecuencias muestrales ponderadas, se obtienen de la siguiente manera \\[\\begin{eqnarray} \\hat{p}_{\\omega}^{rc}=\\frac{\\hat{N}^{rc}_{\\omega}}{\\hat{N}_{\\omega}} \\end{eqnarray}\\] Utilizando la función group_by es posible obtener resultados por más de un nivel de agregación. A continuación, se muestra la estimación ocupación desagregada por niveles de pobreza: diseno %&gt;% group_by(Employment, Poverty) %&gt;% cascade(Nd = survey_total(vartype = c(&quot;se&quot;, &quot;ci&quot;)), .fill = &quot;Total&quot;) ## # A tibble: 17 × 6 ## # Groups: Employment [5] ## Employment Poverty Nd Nd_se Nd_low Nd_upp ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Unemployed NotPoor 1768. 405. 966. 2571. ## 2 Unemployed Extreme 1169. 348. 480. 1859. ## 3 Unemployed Relative 1697. 458. 791. 2604. ## 4 Unemployed Total 4635. 761. 3129. 6141. ## 5 Inactive NotPoor 24346. 1736. 20908. 27784. ## 6 Inactive Extreme 6422. 1321. 3807. 9037. ## 7 Inactive Relative 10697. 1460. 7806. 13589. ## 8 Inactive Total 41465. 2163. 37183. 45748. ## 9 Employed NotPoor 44600. 2596. 39460. 49741. ## 10 Employed Extreme 5128. 1122. 2907. 7349. ## 11 Employed Relative 12149. 1347. 9483. 14816. ## 12 Employed Total 61877. 2540. 56847. 66907. ## 13 Total Total 150266. 4181. 141986. 158546. ## 14 &lt;NA&gt; NotPoor 20684. 1257. 18195. 23172. ## 15 &lt;NA&gt; Extreme 8800. 2980. 2900. 14701. ## 16 &lt;NA&gt; Relative 12805. 1551. 9734. 15876. ## 17 &lt;NA&gt; Total 42289. 2780. 36784. 47794. De las anteriores salidas se puede estimar, entre otros, que 44600 personas que trabajan no son pobres con un intervalo de confianza entre 39459 y 49741. Asimismo, se estima que 6421 personas se encuentran inactivas y al mismo tiempo están en situación de pobreza extrema con un intervalo de confianza entre 3806 y 9037. Para obtener un arreglo rectangular con las estimaciones anteriores, es posible utilizar la función svytable del paquete survey de la siguiente manera: svytable( ~ Poverty + Employment, diseno) ## Employment ## Poverty Unemployed Inactive Employed ## NotPoor 1768.375 24346.008 44600.347 ## Extreme 1169.201 6421.825 5127.531 ## Relative 1697.231 10697.414 12149.142 Por otro lado, también es posible tener tablas que reporten las estimaciones de frecuencias relativas, o porcentajes, en la población. Este análisis se hace, por supuesto, de manera ponderada sobre la muestra expandida. La tabla de doble entrada con la estimación poblacional de las proporciones se presenta a continuación: Variable 2 Variable 1 0 1 Marginal fila 0 \\(\\hat{p}^{00}_{\\omega}\\) \\(\\hat{p}^{01}_{\\omega}\\) \\(\\hat{p}^{0+}_{\\omega}\\) 1 \\(\\hat{p}^{10}_{\\omega}\\) \\(\\hat{p}^{11}_{\\omega}\\) \\(\\hat{p}^{1+}_{\\omega}\\) Marginal columna \\(\\hat{p}^{+0}_{\\omega}\\) \\(\\hat{p}^{+1}_{\\omega}\\) \\(\\hat{p}_{\\omega}\\) De la misma manera que para las frecuencias absolutas, teniendo en cuenta que el subíndice \\(i\\in\\left(r,c\\right)\\) representa a los individuos que están clasificados en la celda (\\(r, c\\)), entonces el estimador de la proporción asociada a esta celda está dado por la siguiente expresión. \\[ \\hat{p}^{rc}_{\\omega}=\\frac{\\hat{N}^{rc}_{\\omega}}{\\hat{N}_{\\omega}}= \\frac{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i\\in\\left(r,c\\right)}^{n_{h\\alpha}}\\omega_{h\\alpha i}}{\\sum_{h=1}^{H}\\sum_{\\alpha=1}^{\\alpha_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}} \\] Por ejemplo, si se desea estimar la proporción de personas por zona y en estado de pobreza, se realiza de la siguiente manera: svytable( ~ Poverty + Zone, diseno, Ntotal = 1) ## Zone ## Poverty Rural Urban ## NotPoor 0.26460893 0.34363467 ## Extreme 0.08547174 0.05773275 ## Relative 0.12974844 0.11880348 5.3.2 Pruebas de independencia Sobre las tablas estimadas, es posible realizar pruebas de independecia para corroborar si existe o no asociación entre dos variables de tipo categórico. Que dos variables sean independientes significa que el comportamiento estructural de una variable no depende de la otra, ni viceversa. Heeringa, West, y Berglund (2017) afirman que, bajo muestreo aleaotrio simple, dos variables categóricas son independientes si la proporción esperada en la fila \\(r\\) y la columna \\(c\\), denotada como \\(\\pi^{rc}\\), guarda la siguiente relación: \\[ \\pi^{rc} = \\frac{n^{r+}\\times n^{+c}}{(n^{++})^2} \\] De esta forma, una manera de corroborar si existe independencia entras las variables de interés es comparar directamente las proporciones estimadas \\(\\hat{p}^{rc}_{\\omega}\\) con las proporciones esperadas \\(\\pi^{rc}\\). Si hay una diferencia muy grande entre ellas, entonces la hipótesis de independencia no sería corroborada por los datos recolectados. Por ende, se define la siguiente estadística \\(\\chi^{2}_{RS}\\) (Rao y Scott 1984b), que sigue una distribución Ji cuadrado con \\((R-1) \\times (C-1)\\) grados de libertad. \\[\\begin{eqnarray} \\chi^{2}_{RS} = \\frac{\\chi^{2}_{Pearson}}{GDEFF} \\end{eqnarray}\\] En donde \\[ \\chi^{2}_{Pearson} = n^{++}\\ \\left(\\sum_r\\sum_c (\\hat{p}^{rc}_{\\omega} -\\pi^{rc} )^2/\\pi^{rc}\\right) \\] Además, \\(GDEFF\\) es una estimación del efecto de diseño generalizado de Rao–Scott, dado por \\[ GDEFF=\\frac{\\sum_{r}\\sum_{c}\\left(1-p_{rc}\\right)d^{2}\\left(p_{rc}\\right)-\\sum_{r}\\left(1-p_{r+}\\right)d^{2}\\left(p_{r+}\\right)-\\sum_{c}\\left(1-p_{+c}\\right)d^{2}\\left(p_{+c}\\right)}{\\left(R-1\\right)\\left(C-1\\right)} \\] Como lo mencionan Heeringa, West, y Berglund (2017), fueron Fay (1979), junto con Fellegi (1980) quienes empezaron a proponer la corrección del estadístico chi-cuadrado de Pearson basada en un efecto de diseño generalizado. Rao y Scott (1984a) y más tarde Thomas y Rao (1987) ampliaron la teoría de las correcciones del efecto de diseño generalizado para estas pruebas estadísticas. El método de Rao-Scott requiere el cálculo de efectos de diseño generalizados que son analíticamente más complicados que el enfoque de Fellegi. Las correcciones de Rao-Scott son ahora el estándar en los procedimientos para el análisis de datos de encuestas categóricas en sistemas de software como Stata y SAS. Adicional a lo anterior, la prueba de independencia F de Fisher permite analizar si dos variables dicotómicas están asociadas cuando la muestra que se observó es demasiado pequeña y no se cumplen las condiciones para aplicar la prueba \\(\\chi^{2}_{Pearson}\\). Para utilizar esta técnica, tengamos en cuenta las expresiones para la probabilidad estimada y la misma estadística \\(\\chi{2}\\) de Pearson. A partir de estas, la estadística de razón de verosimilitud se define como: \\[ G^{2}=2\\times n_{++}\\times\\sum_{r}\\sum_{c}p_{cr}\\times\\ln\\left(\\frac{p_{rc}}{\\hat{\\pi}_{rc}}\\right) \\] donde, \\(r\\) es el número de filas y \\(c\\) representa el número de columnas, la prueba tiene \\((R-1)\\times (C-1)\\) grados de libertad. Realizando una corrección por el efecto de diseño generalizado, la estadística basada en la razón de verosimilitud se calcula como: \\[ G^2_{(R-S)} = G^2\\big/GDEFF \\] Por tanto, la estadística F para independencia basada en la prueba chi-cuadrado de Pearson se calcula como sigue: \\[ F_{R-S,Pearson}=\\chi_{R-S}^{2}\\big/\\left[\\left(R-1\\right)\\left(C-1\\right)\\right]\\sim F_{\\left(R-1\\right)\\left(C-1\\right),\\left(R-1\\right)\\left(C-1\\right)df} \\] y, la estadística F para independencia basada en la razón de verosimilitudes se calcula como sigue: \\[ F_{R-S,LRT}=G_{R-S}^{2}\\big/\\left(C-1\\right)\\sim F_{\\left(C-1\\right),df} \\] donde \\(C\\) es el número de columnas de la tabla cruzada. Para realizar la prueba de independencia \\(\\chi^{2}_{RS}\\) en R, se utilizará la función svychisq del paquete survey. Esta función requiere que se definan las variables de interés (formula) y el diseño muestral (desing). Ahora, para ejemplificar el uso de esta función tomaremos la base de datos de ejemplo y se probará si la pobreza es independiente del sexo. A continuación, se presentan los códigos computacionales: svychisq(formula = ~ Sex + pobreza, design = diseno, statistic = &quot;F&quot;) ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: NextMethod() ## F = 0.056464, ndf = 1, ddf = 119, p-value = 0.8126 De la anterior salida, se puede concluir con una confianza del 95% y basado en las estimaciones sobre la muestra expandida que la pobreza no depende del sexo de las personas, puesto que que el valor p (0.8126) es mayor que el nivel de significación (0.05). En este mismo sentido, si se desea saber si el desempleo está relacionado con el sexo, se realiza la prueba de hipótesis como sigue: svychisq( formula = ~ Sex + Employment, design = diseno, statistic = &quot;F&quot; ) ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: NextMethod() ## F = 62.251, ndf = 1.6865, ddf = 200.6978, p-value &lt; 2.2e-16 De la anterior salida, se puede concluir con una confianza del 95% y basado en las estimaciones sobre la muestra expandida que la desocupación depende del sexo de las personas, puesto que que el valor p (2.2e-16) no es mayor que el nivel de significación (0.05). Es decir, estas dos variables no son independientes. Si en el análisis ahora se quisiera verificar si la pobreza de las personas es independiente de las regiones establecidas en la base de datos, se realiza de la siguiente manera: svychisq( formula = ~ Region + pobreza, design = diseno, statistic = &quot;F&quot; ) ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: NextMethod() ## F = 0.48794, ndf = 3.0082, ddf = 357.9731, p-value = 0.6914 Concluyendo que sí hay independencia entre la pobreza y la región. Lo anterior implica que, no existe relación entre las personas en estado de pobreza por región. 5.3.3 Diferencia de proporciones y contrastes Como lo mencionan Heeringa, West, y Berglund (2017), las estimaciones de las proporciones de las filas en las tablas de doble entrada son estimaciones de subpoblaciones en las que la subpoblación se define por los niveles de la variable categórica. En algunas ocasiones, puede ser de interés estimar diferencias de las proporciones de las categorías entre dos niveles o en dos subpoblaciones. Como ya se vio en el capítulo anterior, esto puede ser logrado utilizando contrastes. A manera de ejemplo, considere que se requiere estimar el contraste de proporciones de mujeres en estado de pobreza contra los hombres en esta misma condición (\\(\\hat{\\Delta}_{\\omega} = \\hat{p}^{F1}_{\\omega}-\\hat{p}^{M1}_{\\omega}\\)). Para ello, primero estimamos la proporción de hombres y mujeres en estado de pobreza como se ha mostrado en capítulos anteriores: ( tab_sex_pobreza &lt;- svyby( formula = ~ pobreza, by = ~ Sex, design = diseno , svymean, na.rm = T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;) ) ) ## Sex pobreza se ci_l ci_u ## Female Female 0.3892389 0.03159581 0.3273123 0.4511656 ## Male Male 0.3945612 0.03662762 0.3227724 0.4663501 Ahora bien, para calcular la estimación de la diferencia de proporciones junto con sus errores estándares, se realizarán los siguientes pasos: Paso 1: Calcular la diferencia de estimaciones 0.3892 - 0.3946 ## [1] -0.0054 Paso 2: Con la función vcov obtener la matriz de covarianzas: vcov(tab_sex_pobreza) ## Female Male ## Female 0.0009982953 0.0009182927 ## Male 0.0009182927 0.0013415823 Paso 3: Calcular el error estándar es: sqrt(0.0009983 + 0.0013416 - 2*0.0009183) ## [1] 0.02243435 Ahora bien, aplicando la función svycontrast se puede obtener la estimación de la diferencia de proporciones anterior de manera más expedita: svycontrast(tab_sex_pobreza, list(diff_Sex = c(1,-1))) ## contrast SE ## diff_Sex -0.0053223 0.0224 De lo que se concluye que, la diferencia entre las proporciones estimadas de mujeres y hombres en condición de pobreza es -0.005 (-0.5%) con una error estándar estimado de 0.022. Otro ejercicio de interés en un análisis de encuestas de hogares es verificar la si existen brechas en la condición de ocupación (por ejemplo en el desempleo) por sexo. Al igual que el ejemplo anterior, se inicia con la estimación del porcentaje de desempleados por sexo, omitiendo las personas menores de edad: tab_sex_desempleo &lt;- svyby( formula = ~ desempleo, by = ~ Sex, design = diseno %&gt;% filter(!is.na(desempleo)) , FUN = svymean, na.rm = T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;) ) tab_sex_desempleo ## Sex desempleo se ci_l ci_u ## Female Female 0.02168620 0.005580042 0.01074952 0.03262288 ## Male Male 0.06782601 0.012161141 0.04399062 0.09166141 Para calcular la estimación de la diferencia de proporciones junto con sus errores estándares, se realizarán los siguientes pasos: Paso 1: Diferencia de las estimaciones 0.02169 - 0.06783 ## [1] -0.04614 Paso 2: Con la función vcov obtener la matriz de covarianzas: vcov(tab_sex_desempleo) ## Female Male ## Female 3.113687e-05 2.081301e-05 ## Male 2.081301e-05 1.478933e-04 Paso 3: Estimación del error estándar. sqrt(0.00003114 + 0.00014789 - 2*0.00002081) ## [1] 0.0117222 Siguiendo el ejemplo anterior, utilizando la función svycontrast se tiene que: svycontrast(tab_sex_desempleo, list(diff_Sex = c(-1, 1))) ## contrast SE ## diff_Sex 0.04614 0.0117 De los resultados anteriores, se concluye que la estimación del contraste es 0.04 (4%) con un error estándar estimado de 0.011. Adentrándose un poco más en la complejidad de los contrates, otro ejercicio que se puede realizar en una encuesta de hogares es estimar la proporción de desempleados por región. Para la realización de este ejercicio, se seguirán los pasos de los dos ejemplos anteriores: tab_region_desempleo &lt;- svyby( formula = ~ desempleo, by = ~ Region, design = diseno %&gt;% filter(!is.na(desempleo)) , FUN = svymean, na.rm = T, covmat = TRUE, vartype = c(&quot;se&quot;, &quot;ci&quot;) ) tab_region_desempleo ## Region desempleo se ci_l ci_u ## Norte Norte 0.04877722 0.02002293 0.009532997 0.08802144 ## Sur Sur 0.06563877 0.02375124 0.019087202 0.11219034 ## Centro Centro 0.03873259 0.01240317 0.014422832 0.06304235 ## Occidente Occidente 0.03996523 0.01229650 0.015864529 0.06406592 ## Oriente Oriente 0.02950231 0.01256905 0.004867428 0.05413719 Ahora, asuma que el interés es realizar contrastes para la proporción de desempleados entre las regiones Norte y Sur, entre Sur y Centro y, finalmente, entre Occidente y Oriente. A continuación se tendrían las estimaciones puntuales: \\(\\hat{p}_{Norte} - \\hat{p}_{Centro} = 0.04877722 - 0.03873259 = -0.01004463\\), \\(\\hat{p}_{Sur} - \\hat{p}_{Centro} = 0.06563877 - 0.03873259 = 0.02690618\\) \\(\\hat{p}_{Occidente} - \\hat{p}_{Oriente} = 0.03996523 - 0.02950231 = 0.01046292\\) Asimismo, escrita de forma matricial, la matriz de contraste sería: \\[ A = \\left[\\begin{array}{ccccc} 1 &amp; 0 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\end{array}\\right] \\] La matriz de varianzas y covarianzas de las proporciones estimadas es: vcov(tab_region_desempleo) Por tanto, la varianza estimada para cada diferencia de proporciones está dada por: sqrt(0.0004009178 + 0.0001538386 - 2 * 0) ## [1] 0.02355327 sqrt(0.0005641213 + 0.0001538386 - 2 * 0) ## [1] 0.02679477 sqrt(0.0001512039 + 0.000157981 - 2 * 0) ## [1] 0.01758365 Usando la función svycontrast, la estimación de los contrastes sería: svycontrast(tab_region_desempleo, list( Norte_sur = c(1, 0, -1, 0, 0), Sur_centro = c(0, 1, -1, 0, 0), Occidente_Oriente = c(0, 0, 0, 1, -1) )) ## contrast SE ## Norte_sur 0.010045 0.0236 ## Sur_centro 0.026906 0.0268 ## Occidente_Oriente 0.010463 0.0176 5.3.4 Razones de odds Como lo menciona Monroy, Rivera, y Dávila (2018), la traducción más aproximada del término odds es “ventaja” que denota la posibilidad de que un evento ocurra con relación a que no ocurra; es decir, es un número que expresa cuánto más probable es que se produzca un evento frente a que no se produzca. También se puede utilizar para cuantificar la asociación entre los niveles de una variable y un factor categórico. Por ejemplo, considere la siguiente salida que relaciona el sexo con la pobreza: tab_Sex_Pobr &lt;- svymean( x = ~ interaction (Sex, pobreza), design = diseno, se = T, na.rm = T, ci = T, keep.vars = T ) tab_Sex_Pobr ## mean SE ## interaction(Sex, pobreza)Female.0 0.32187 0.0178 ## interaction(Sex, pobreza)Male.0 0.28637 0.0177 ## interaction(Sex, pobreza)Female.1 0.20513 0.0166 ## interaction(Sex, pobreza)Male.1 0.18663 0.0178 Las ODDS de ser mujer dado que es pobre son \\(ODDS(Sexo = Mujer|Pobre) = \\hat{p}^{1|Female}_{\\omega} / (1 - \\hat{p}^{1|Female}_{\\omega}) = \\hat{p}^{1|Female}_{\\omega} / \\hat{p}^{0|Female}_{\\omega} = 0.20/0.32=0.63\\). Por otro lado, las ODDS de ser hombre dado que es pobre son \\(ODDS(Sexo = Hombre|Pobre) = \\hat{p}^{1|Male}_{\\omega} / \\hat{p}^{0|Male}_{\\omega}=0.18/0.28=0.65\\). De esta forma, la razón de odds estaría dada por la siguiente expresión. \\[ \\widehat{OR}_{\\omega}^{Sexo-Pobreza} = \\frac{ODDS(Sexo = Mujer|Pobre)}{ODDS(Sex = Hombre|Pobre)} = \\frac{\\frac{P(Sex = Female \\mid pobreza = 1 )}{P(Sex = Female \\mid pobreza = 0 )}}{ \\frac{P(Sex = Male \\mid pobreza = 1 )}{P(Sex = Male \\mid pobreza = 0 )} } = \\frac{0.63}{0.65}=0.97 \\] El procedimiento para realizarlo en R, luego de haber estimado las respectivas proporciones de la tabla cruzada entre las variables sexo y pobreza, se centra en realizar el contraste dividiendo cada uno de los elementos de la expresión mostrada anteriormente: svycontrast(stat = tab_Sex_Pobr, contrasts = quote(( `interaction(Sex, pobreza)Female.1` / `interaction(Sex, pobreza)Female.0` ) / ( `interaction(Sex, pobreza)Male.1` / `interaction(Sex, pobreza)Male.0` ) )) ## nlcon SE ## contrast 0.97791 0.0919 Del anterior resultado se estima que el odds de las mujeres que están en condición de pobreza es 0.97 (muy cercano a uno) comparándolo con el odds de los hombres que están en condición de pobreza. En otras palabras, se estima que las probabilidades de que las mujeres no estén en estado de pobreza sin tener en cuenta ninguna otra variable de la encuesta es cerca de 3% mayor que las probabilidades de los hombres. References "],["análisis-gráfico.html", "Capítulo 6 Análisis gráfico", " Capítulo 6 Análisis gráfico En todo análisis de encuestas, el componente gráfico es fundamental para revisar tendencias en algunas variables de interés. La visualización gráfica es una herramienta que permite lograr una representación visual clara de la distribución de los datos; por ejemplo con gráficos de barras, histogramas o gráficos de dispersión, lo cual facilita la comprensión de la forma, la localización y la dispersión de la distribución de las variables. Además, permite identificar patrones, tendencias y datos atípicos que pueden no ser evidentes al examinar únicamente las estimaciones directas. Es posible, además, visualizar diferencias entre variables o comparar la distribución de una variable en diferentes subgrupos de la población finita, por ejemplo, para la identificación de brechas. Las imágenes son poderosas para comunicar resultados a audiencias diversas que no necesariamente están en conocimiento de los pormenores técnicos que conlleva la estimación puntual de las estadísticas directas. Los gráficos son más accesibles y comprensibles para un público general en comparación con tablas de datos complejas, lo que facilita la comunicación de los resultados del análisis. La visualización gráfica no solo mejora la comprensión de los datos, sino que también facilita la interpretación y comunicación de los resultados, haciendo que el análisis de encuestas de hogares sea más efectivo y accesible. Además, son muy necesarias las gráficas cuando el objetivo es corroborar algunos supuestos en el ajustes de modelos estadísticas, por ejemplo, varianzas constantes en los errores, normalidad, etc. Las librerías especializadas en el manejo de datos de encuestas también tienen algunas opciones para la realización de gráficas. Sin embargo, uno de los paquetes más usados para representar de forma visual los resultados de las encuestas en R es ggplot2 (Wickham 2016), el cual representa una opción potente y flexible para producir gráficos elegantes. Sin embargo, Como es de costumbre, se inicia este capítulo cargando las librerías y bases de datos. options(digits = 4) library(survey) library(srvyr) library(convey) library(TeachingSampling) library(printr) library(ggplot2) library(patchwork) data(BigCity, package = &quot;TeachingSampling&quot;) encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) A continuación, se define el diseño de muestreo de la encuesta y, para efectos de los ejemplos, se definen las siguientes variables: diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = T ) %&gt;% mutate( pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0), desempleo = ifelse(Employment == &quot;Unemployed&quot;, 1, 0), edad_18 = case_when(Age &lt; 18 ~ &quot;&lt; 18 anios&quot;, TRUE ~ &quot;&gt;= 18 anios&quot;) ) Como se mostró en capítulos anteriores, se divide la muestra en subgrupos para ejemplificar los conceptos que se mostrarán en este capítulo: sub_Urbano &lt;- diseno %&gt;% filter(Zone == &quot;Urban&quot;) sub_Rural &lt;- diseno %&gt;% filter(Zone == &quot;Rural&quot;) sub_Mujer &lt;- diseno %&gt;% filter(Sex == &quot;Female&quot;) sub_Hombre &lt;- diseno %&gt;% filter(Sex == &quot;Male&quot;) References "],["análisis-gráfico-con-la-librería-survey.html", "6.1 Análisis gráfico con la librería survey", " 6.1 Análisis gráfico con la librería survey Una vez cargada la base de datos que contiene la muestra en R y definido el diseño muestral del cual proviene, se pueden hacer los primeros análisis visuales. Como recomendación, se inicia con análisis gráficos que, gracias al principio de representatividad, reflejaran el comportamiento de las variables continuas, no en la muestra obtenida, sino en la población de estudio, a través de la muestra expandida con los pesos de muestreo. Como ejemplo, a continuación, se muestran los códigos computacionales con los cuales se pueden realizar histogramas en R para la variable ingresos teniendo en cuenta el diseño muestral y los factores de expansión haciendo uso la función svyhist de la librería survey. svyhist( ~ Income , diseno, main = &quot;Ingreso poblacional&quot;, col = &quot;grey80&quot;, xlab = &quot;Ingreso&quot;, probability = FALSE ) Como se puede observar en el código anterior, para generar este histograma, se usó la función svyhist. En primer lugar, se definió la variable que se quiere visualizar, que para nuestro caso es Income. Seguido, se define el diseño muestral utilizado en la encuesta. Luego, los argumentos relacionados con la estética del gráfico como lo son: el título principal (main), el color (col) y el título horizontal (xlab). Finalmente, se establece si el histograma es de frecuencias o probabilidades con el argumento probability. Para este ejemplo, se tomó la opción probability = False indicando que es un histograma de frecuencias. Por otro lado, uno de los análisis gráficos más comunes que se realizan en encuestas de hogares están relacionados con subgrupos geográficos como lo pueden ser las zonas (urbano - rural) o también realizar desagregaciones temáticas por sexo (hombre mujer). A continuación, se muestra la sintaxis en R de cómo se realizan histogramas para hombres y mujeres mayores de 18 años: sub_Mujer &lt;- diseno %&gt;% filter(Sex == &quot;Female&quot;) sub_Hombre &lt;- diseno %&gt;% filter(Sex == &quot;Male&quot;) par(mfrow = c(1, 2)) svyhist( ~ Income , design = subset(sub_Mujer, Age &gt;= 18), main = &quot;Mujer&quot;, breaks = 30, col = &quot;grey80&quot;, xlab = &quot;Ingreso&quot; ) svyhist( ~ Income , design = subset(sub_Hombre, Age &gt;= 18), main = &quot;Hombre&quot;, breaks = 30, col = &quot;grey80&quot;, xlab = &quot;Ingreso&quot; ) Como se puede observar, los argumentos utilizando para realizar los gráficos son los mismos que se utilizaron y ejemplificaron anteriormente. Cabe notar que la función subset permite hacer un subconjunto de la población, que para nuestro caso son aquellos hombres y mujeres con edad mayor o igual a 18 años. Si el objetivo ahora es realizar un análisis gráfico de localización y variabilidad, es posible plantear un diagrama de cajas (boxplot), teniendo en cuenta los factores de expansión. A continuación, se muestra las sintaxis de cómo realizarlo en R, para ambas zonas: urbana y rural. sub_Urbano &lt;- diseno %&gt;% filter(Zone == &quot;Urban&quot;) sub_Rural &lt;- diseno %&gt;% filter(Zone == &quot;Rural&quot;) par(mfrow = c(1, 2)) svyboxplot( Income ~ 1 , sub_Urbano, col = &quot;grey80&quot;, ylab = &quot;Ingreso&quot;, xlab = &quot;Urbano&quot; ) svyboxplot( Income ~ 1 , sub_Rural, col = &quot;grey80&quot;, ylab = &quot;Ingreso&quot;, xlab = &quot;Rural&quot; ) Los argumentos usados en la función svyboxplot para generar el gráfico son muy similares a los usados en la función svyhist. Algo que se debe recalcar en los argumentos de esta función es que la sintaxis Income ~ 1 hace referencia a que todas las personas pertenecen a un solo grupo que puede ser urbano o rural, dependiendo del caso, y por eso se requiere indicarle a R esa restricción; esto se hace con el símbolo ~ 1. "],["análisis-gráfico-con-la-librería-ggplot2.html", "6.2 Análisis gráfico con la librería ggplot2", " 6.2 Análisis gráfico con la librería ggplot2 La librería ggplot2 es una herramienta destacada para la visualización de datos en R. Es ampliament econocida y usada por los investigadores alrededor del mundo y es conocida por su sintaxis declarativa y flexibilidad. Al adoptar un enfoque basado en capas, permite a los usuarios crear visualizaciones complejas de manera intuitiva. 6.2.1 Definición del tema En el ámbito de encuestas de hogares, ggplot2 se convierte en una herramienta valiosa para representar visualmente tendencias en el tiempo, distribuciones de variables continuas y otros patrones relevantes. De hecho, su capacidad para trabajar con datos complejos facilita la exploración de este tipo de bases de datos, proporcionando una visión clara y efectiva de la realidad social que reflejan las encuestas de hogares. Para crear las gráficas de este documento es posible se utilizará por defecto un tema específico, que define la apariencia visual de los gráficos. El tema incluye aspectos como colores, fuentes, márgenes y otros elementos estéticos que permiten personalizar la presentación de un gráfico de acuerdo con las preferencias del usuario. La librería ggplot2 proporciona una variedad de temas predefinidos que pueden aplicarse a los gráficos para cambiar su apariencia de manera rápida y sencilla. El siguiente código define el tema que se usará en los gráficos del docuemnto. theme_cepal &lt;- function(...) { theme_light(10) + theme( axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), legend.position = &quot;bottom&quot;, legend.justification = &quot;left&quot;, legend.direction = &quot;horizontal&quot;, plot.title = element_text(size = 10, hjust = 0.5), ... ) } Basado en lo anterior, los argumentos dentro de la función theme() en ggplot2 se utilizan para personalizar la apariencia de diferentes elementos en un gráfico. en particular: axis.text.x y axis.ticks.x controlan la apariencia de las etiquetas y marcas en el eje x. En este caso, se establece element_blank() para ocultar tanto las etiquetas como las marcas en el eje x. axis.text.y y axis.ticks.y controlan la apariencia de las etiquetas y marcas en el eje y. Aquí también se establece element_blank() para ocultar ambas. legend.position define la posición de la leyenda en el gráfico. En este caso, se establece en bottom, lo que significa que la leyenda se situará en la parte inferior del gráfico. legend.justification y legend.direction define la alineación y dirección de la leyenda. En este caso, la leyenda se justifica a la izquierda (“left”) y se presenta en dirección horizontal. plot.title controla la apariencia del título del gráfico. Se establece element_text(size = 20, hjust = 0.5) para especificar que el tamaño del texto del título sea 20 y que esté centrado horizontalmente (hjust = 0.5). 6.2.2 Histogramas Un histograma es una representación gráfica de los datos de una variable empleando rectángulos (barras) cuya altura es proporcional a la frecuencia de los valores representados y su ancho proporcional a la amplitud de los intervalos de la clase. A continuación, se presenta cómo realizar un histograma para la variable ingresos utilizando los factores de expansión de la encuesta. EN primera instancia se define la fuente de información (data), luego se definen la variable que se desea graficar (x) y los pesos de muestreo (weight). Una vez definidos los parámetros generales, se define el tipo de gráfico, que para el caso de los histogramas es geom_histogram. Además, se definen los títulos que se quiere que tenga el histograma y por último, se aplica el tema de la CEPAL. HistInc &lt;- ggplot(data = encuesta, aes(x = Income, weight = wk)) + geom_histogram(aes(y = ..density..)) + ylab(&quot;&quot;) + ggtitle(&quot;Histograma ponderado del ingreso&quot;) + theme_cepal() HistInc Por otro lado, repetimos ahora la secuencia de gráficos pero en este caso para la variable Expenditure: ggplot(data = encuesta, aes(x = Expenditure, weight = wk)) + geom_histogram(aes(y = ..density..)) + ylab(&quot;&quot;) + ggtitle(&quot;Histograma ponderado del gasto&quot;) + theme_cepal() Cuando el interés es realizar comparaciones entre dos o más agrupaciones, es posible hacer uso del parámetro fill, el cual rellena las barras del histograma con diferentes colores según el subgrupo poblacional. El siguiente ejemplo proporciona el código para realizar la comparación de la distribución de los ingresos poblacionales por zonas: ggplot(encuesta, aes(x = Income, weight = wk)) + geom_histogram(aes(y = ..density.., fill = Zone), alpha = 0.5, position = &quot;identity&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Histogramas ponderados del ingreso por zona&quot;) + theme_cepal() Como se pudo observar en la generación del histograma, se utilizó el parámetro position, el cual permite que las barras del gráfico sean distinguibles. Ahora, repetimos la secuencia de gráficos para la variable Expenditure. ggplot(encuesta, aes(x = Expenditure, weight = wk)) + geom_histogram(aes(y = ..density.., fill = Zone), alpha = 0.5, position = &quot;identity&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Histogramas ponderados del gasto por zona&quot;) + theme_cepal() Si quisieramos hacer una desagregación de la distribución del ingreso y del gasto por sexo, se debería usar el siguiente código que hace uso de la librería patchworks para posiconarlos de forma paralela. HistIncSex &lt;- ggplot(encuesta, aes(x = Income, weight = wk)) + geom_histogram(aes(y = ..density.., fill = Sex), alpha = 0.5, position = &quot;identity&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Histogramas del ingreso por sexo&quot;) + theme_cepal() HistExpSex &lt;- ggplot(encuesta, aes(x = Expenditure, weight = wk)) + geom_histogram(aes(y = ..density.., fill = Sex), alpha = 0.5, position = &quot;identity&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Histogramas del gasto por sexo&quot;) + theme_cepal() HistIncSex | HistExpSex 6.2.3 Densidades Dadas las cualidades de la librería ggplot2, se pueden agregar nuevas capas a los gráficos; particularmente, a los histogramas antes realizados. La densidad se agrega con el argumento geom_density y se incorpora el parámetro alpha que regula la transparencia del relleno. Esta capa de densidad es útil cuando se trabaja con variables continuas, ya que proporciona una estimación suavizada de la función de densidad de probabilidad de los datos. Esta función utiliza un kernel de suavizamiento para construir una curva suave que se ajusta a los datos, ayudando a obtener una representación visual más clara de la forma general de la distribución. A continuación, se muestra cómo se agregan las densidades: HistInc + geom_density(fill = &quot;yellow&quot;, alpha = 0.8) Ahora bien, al aplicar el argumento aes(fill = Zone) permite que la densidad sea agregada para cada una de las agrupaciones como se muestra a continuación. HistIncSex + geom_density(aes(fill = Sex)) 6.2.4 Diagramas de caja Los diagramas de caja (boxplot) son gráficos de resumen ampliamente utilizados en la práctica estadística. Este tipo de diagramas permite visualizar de forma general un conjunto de datos empleando la estimación directa de cinco cantidades: el mínimo, el máximo y los cuartiles Q1, Q2 y Q3. La forma generada por este gráfico compuesto por un rectángulo y dos brazos suministra información sobre la relación entre estas cantidades y permite verificar la existencia de valores atípicos. Para realizar este gráfico en ggplot2 se utiliza la función geom_boxplot. A continuación, se presentan los Boxplot para las variables ingresos y gastos respectivamente: BoxInc &lt;- ggplot(encuesta, aes(x = Income, weight = wk)) + geom_boxplot() + ggtitle(&quot;Boxplot ponderado de los ingresos&quot;) + coord_flip() + theme_cepal() BoxExp &lt;- ggplot(encuesta, aes(x = Expenditure, weight = wk)) + geom_boxplot() + ggtitle(&quot;Boxplot ponderado de los gastos&quot;) + coord_flip() + theme_cepal() BoxInc | BoxExp En los gráficos anteriores se puede observar que la variable ingresos tiene más variabilidad que la variable gastos. En ambos gráficos se observan datos atípicos. Ahora bien, esta clase de diagramas también permiten la comparación entre dos o más niveles de agrupamiento, por ejemplo, por zonas para las variables ingresos y gastos como se muestra a continuación. BoxIncZone &lt;- ggplot(encuesta, aes(x = Income, weight = wk)) + geom_boxplot(aes(fill = Zone)) + ggtitle(&quot;Boxplot de los ingresos por zona&quot;) + coord_flip() + theme_cepal() BoxExpZone &lt;- ggplot(encuesta, aes(x = Expenditure, weight = wk)) + geom_boxplot(aes(fill = Zone)) + ggtitle(&quot;Boxplot de los gastos por zona&quot;) + coord_flip() + theme_cepal() BoxIncZone | BoxExpZone Observándose, entre otros que, para la variable gasto en la zona rural es donde más datos atípicos hay. Ahora, si se desea personalizar los colores del relleno, es posible hacer uso de la función scale_fill_manualcomo se muestra a continuación: colorZona &lt;- c(Urban = &quot;#48C9B0&quot;, Rural = &quot;#117864&quot;) BoxIncZone + scale_fill_manual(values = colorZona) | BoxExpZone + scale_fill_manual(values = colorZona) Realizando la comparación para más de dos categorías, por ejemplo región, se procede como: BoxIncReg &lt;- ggplot( data = encuesta, aes(x = Income, weight = wk)) + geom_boxplot(aes(fill = Region)) + ggtitle(&quot;Boxplot de los ingresos por región&quot;) + coord_flip() + theme_cepal() BoxExpReg &lt;- ggplot( data = encuesta, aes(x = Expenditure, weight = wk)) + geom_boxplot(aes(fill = Region)) + ggtitle(&quot;Boxplot de los gastos por región&quot;) + coord_flip() + theme_cepal() BoxIncReg | BoxExpReg Una ventaja de este tipo de visualizaciones es que se pueden extender las comparaciones a variables que tienen más de dos categorías, creándose así una sinergia de posibles conclusiones sobre la distribución de las variables. ggplot(data = encuesta, aes(x = Income, y = Sex, weight = wk)) + geom_boxplot(aes(fill = Region)) + ggtitle(&quot;Boxplot de los ingresos por región y sexo&quot;) + coord_flip() 6.2.5 Diagramas de dispersión Un diagrama de dispersión (scaterplot) representa cada observación como un punto, posicionado según el valor de dos variables continuas. Además de una posición horizontal y vertical, cada punto también puede tener un tamaño, un color y una forma. Estos atributos se denominan estética y son las propiedades que se pueden percibir en el gráfico. Cada estética puede asignarse a una variable o establecerse en un valor constante. Para visualizar estas relaciones en encuestas que provienen de un diseño de muestreo complejo que podría asignar diferentes factores de expansión a cada hogar, es posible realizar un diagrama de dispersión ponderado. Este es una representación gráfica de los datos que incorpora los pesos de muestreo finales para resaltar la importancia relativa de cada unidad observada en la población. En un diagrama de dispersión convencional, cada punto tiene el mismo peso en la visualización, lo que significa que todos los puntos se tratan de manera igualitaria. En cambio, uno ponderado asigna pesos específicos a cada punto, reflejando su relevancia o contribución a la representación global. La ponderación generalmente se logra mediante el uso de un tercer conjunto de datos que proporciona los pesos correspondientes a cada observación. Para realizar este tipo de gráfico se usará la función geom_point, como se muestra a continuación: ggplot(data = encuesta, aes(y = Income, x = Expenditure, weight = wk)) + geom_point() + ggtitle(&quot;Scatterplot ponderado entre los ingresos y los gastos&quot;) + theme_cepal() Note que, en este caso, el parámetro weight no está aportando información visual al gráfico. Luego, el parámetro alpha se puede usar para controlar el tamaño de los puntos, para tener un mejor panorama del comportamiento de la muestra y su expansión. ggplot(data = encuesta, aes(y = Income, x = Expenditure)) + geom_point(aes(size = wk), alpha = 0.1) + ggtitle(&quot;Scatterplot ponderado entre los ingresos y los gastos&quot;) + theme_cepal() Otra forma de usar la variable wk, es asignar la intensidad del color según el valor de los factores de expansión. ggplot(data = encuesta, aes(y = Income, x = Expenditure)) + geom_point(aes(col = wk), alpha = 0.3) + ggtitle(&quot;Scatterplot ponderado entre los ingresos y los gastos&quot;) + theme_cepal() Se puede extender las bondades de los gráficos de ggplot2 para obtener mayor información de las muestra. Por ejemplo, agrupar los datos por Zona. Para lograr esto se introduce el parámetro shape. ggplot( data = encuesta, aes(y = Income, x = Expenditure, shape = Zone)) + geom_point(aes(size = wk, color = Zone), alpha = 0.3) + labs(size = &quot;Peso&quot;) + ggtitle(&quot;Scatterplot por zona entre los ingresos y los gastos&quot;) + theme_cepal() 6.2.6 Diagrama de barras Un diagrama de barras es una representación gráfica que utiliza barras rectangulares para mostrar la relación entre distintas categorías. Cada barra representa la frecuencia, proporción o cantidad asociada a una categoría específica, y la longitud de la barra es proporcional al valor que está representando. Estos diagramas son efectivos para visualizar datos discretos y comparar cantidades entre diferentes categorías de manera clara y sencilla. Sin embargo, en encuestas de hogares, la altura de las barras siempre va a representar una estimación que, a su vez, está sujeta al error de muestreo. Incorporar esta incertidumbre en los gráficos es una manera rápida y correcta de plantear hipótesis en la comparación de subgrupos poblacionales. Para realizar estos gráficos, en primer lugar, se deben realizar las estimaciones puntuales de los valores que se van a graficar. En el siguiente ejemplo, se estima la cantidad de personas en la zona urbana y la rural, junto con sus respectivos errores estándar. tamano_zona &lt;- diseno %&gt;% group_by(Zone) %&gt;% summarise( Nd = survey_total(vartype = c(&quot;se&quot;, &quot;ci&quot;))) tamano_zona Zone Nd Nd_se Nd_low Nd_upp Rural 72102 3062 66039 78165 Urban 78164 2847 72526 83802 Para realizar este tipo de gráficos se requerirá de dos funciones, la primera geom_bar, que se utiliza para crear la capa de las barras que representarán la estimación del tamaño de cada zona; la altura de la barra es proporcional al valor que representa. La segunda función es geom_errorbar que se utiliza para agregar barras de error a un gráfico, que es útil porque precisamente queremos representar la variabilidad asociada con los valores estimados en la barra. El siguiente código permite reproducir este tipo de gráficos. Note que, los valores ymax y ymin se utilizan para definir los extremos superior e inferior de las barras de error, los cuales corresponden a los límites superior e inferior de los intervalos de confianza de la tabal anterior. ggplot(data = tamano_zona, aes( x = Zone, y = Nd, ymax = Nd_upp, ymin = Nd_low, fill = Zone )) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar(position = position_dodge(width = 0.9), width = 0.3) + theme_bw() Como se ha visto en los gráficos anteriores, este tipo de gráficos se pueden extender también a variables con más de dos categorías. Primero se realiza la estimación puntual junto con sus errores estándar asociados. tamano_pobreza &lt;- diseno %&gt;% group_by(Poverty) %&gt;% summarise(Nd = survey_total(vartype = c(&quot;se&quot;, &quot;ci&quot;))) tamano_pobreza Poverty Nd Nd_se Nd_low Nd_upp NotPoor 91398 4395 82696 100101 Extreme 21519 4949 11719 31319 Relative 37349 3695 30032 44666 El gráfico asociado a las anteriores estimaciones se obtiene con una sintaxis homologa a la anterior. ggplot(data = tamano_pobreza, aes( x = Poverty, y = Nd, ymax = Nd_upp, ymin = Nd_low, fill = Poverty )) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar(position = position_dodge(width = 0.9), width = 0.3) + theme_bw() De forma similar a los gráficos Boxplot, es posible realizar comparaciones entre más de dos variables. A continuación, se obtienen las estimaciones puntuales y los errores estándar para los tamaños absolutos de los cruces entre las dos categorías de desempleo y las tres categorías de pobreza. tamano_ocupacion_pobreza &lt;- diseno %&gt;% filter(!is.na(desempleo)) %&gt;% group_by(desempleo, Poverty) %&gt;% summarise(Nd = survey_total(vartype = c(&quot;se&quot;, &quot;ci&quot;))) %&gt;% as.data.frame() tamano_ocupacion_pobreza desempleo Poverty Nd Nd_se Nd_low Nd_upp 0 NotPoor 68946 3676.3 61666.8 76226 0 Extreme 11549 2208.8 7175.8 15923 0 Relative 22847 2558.5 17780.5 27913 1 NotPoor 1768 405.4 965.7 2571 1 Extreme 1169 348.1 479.9 1859 1 Relative 1697 457.8 790.7 2604 El gráfico para la tabla anterior queda de ejemplificado de la siguiente manera. ggplot(data = tamano_ocupacion_pobreza, aes( x = Poverty, y = Nd, ymax = Nd_upp, ymin = Nd_low, fill = as.factor(desempleo) )) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar(position = position_dodge(width = 0.9), width = 0.3) + theme_bw() En estos gráficos también se pueden presentar proporciones estimadas, o frecuencias relativas, como se muestra a continuación para la estimación de la proporción de personas en las tres categorías de pobreza cruzada con la zona. prop_ZonaH_Pobreza &lt;- sub_Hombre %&gt;% group_by(Zone, Poverty) %&gt;% summarise(prop = survey_prop(vartype = c(&quot;se&quot;, &quot;ci&quot;))) prop_ZonaH_Pobreza ## # A tibble: 6 × 6 ## # Groups: Zone [2] ## Zone Poverty prop prop_se prop_low prop_upp ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rural NotPoor 0.549 0.0626 0.424 0.668 ## 2 Rural Extreme 0.198 0.0675 0.0958 0.364 ## 3 Rural Relative 0.254 0.0372 0.187 0.334 ## 4 Urban NotPoor 0.660 0.0366 0.584 0.728 ## 5 Urban Extreme 0.113 0.0245 0.0726 0.171 ## 6 Urban Relative 0.227 0.0260 0.180 0.283 Después de obtener la tabla con los valores que se quieren presentar en el gráfico, los códigos computacionales para realizar un diagrama de barras es el siguiente: ggplot(data = prop_ZonaH_Pobreza, aes( x = Poverty, y = prop, ymax = prop_upp, ymin = prop_low, fill = Zone )) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar(position = position_dodge(width = 0.9), width = 0.3) + scale_fill_manual(values = colorZona) + theme_bw() "],["mapas.html", "6.3 Mapas", " 6.3 Mapas Los mapas son una herramienta gráfica poderosa para la visualización de datos. Particularmente, para indicadores socio-demográficos, estos proporcionan una referencia visual para desagregaciones de interés como región, departamento, provincia, distrito, municipio, comuna, etc. R posee un sin fin de métodos de programación para representar mapas. En una primera instacia, para realizar mapas es necesario contar con un tipo especial de información geoespacial; esto es, datos que contienen las coordenadas o delimitaciones geográficas de determinado país o región. Sitios web como http://www.diva-gis.org/gdata ofrecen de manera gratuita bases de datos geoespaciales que contienen los vectores asociados a las geografías correspondientes. El formato de este tipo de datos se conoce como shapefile y continene observaciones sobre la longitud y latitud, que permiten localizar un conjunto de puntos cuya unión en el gráfico formarán las formas de los polígonos que definen las áreas geográficas. Entre las distintas librería para realizar mapas en R están tmap y ggplot2. A continuación, se ilustra cómo se generan mapas, inicalmente con la librería tmap. Inicialmente, para realizar el mapa hay que contar con el archivo de shapefile el cual se carga de la siguiente manera. library(sf) library(tmap) shapeBigCity &lt;- read_sf(&quot;Data/shapeBigCity/BigCity.shp&quot;) Una vez cargado el shapefile, el mapa se genera usando las funciones tm_shape y la información que se desea graficar en el mapa se incluye con la función tm_polygons. Para este ejemplo inicial, solo se visualizarán las regiones en el mapa: tm_shape(shapeBigCity) + tm_polygons(col = &quot;Region&quot;) A modo de otro ejemplo, suponga que se desea visualizar la estimación de los ingresos medios a nivel de región. En primer lugar se debe obtener la tabla de estimaciones. prom_region &lt;- svyby(~Income, ~Region, diseno, svymean, na.rm = T, covmat = TRUE, vartype = c(&quot;cv&quot;)) prom_region Region Income cv Norte Norte 552.4 0.1002 Sur Sur 625.8 0.0997 Centro Centro 650.8 0.0945 Occidente Occidente 517.0 0.0894 Oriente Oriente 541.8 0.1323 El siguiente código representa áreas geográficas coloreadas según la estimación del promedio de la variable Income. El comando brks crea un vector que especifica los puntos de corte para clasificar los valores de la estimación en rangos. En este caso, hay tres rangos: [0, 550), [550, 600), [600, 1000]. Estos rangos se utilizarán para asignar colores a las áreas geográficas en el mapa. Luego, se crea un objeto tm_shape que contiene la información geométrica y de atributos de las áreas geográficas. También se realiza una operación de left_join para unir la información adicional de contenida en el dataframe prom_region al dataframe shapeBigCity. brks &lt;- c(0, 550, 600, 1000) shape_temp &lt;- tm_shape(shapeBigCity %&gt;% left_join(prom_region, by = &quot;Region&quot;)) shape_temp + tm_polygons(&quot;Income&quot;, breaks = brks, title = &quot;Income&quot;,) + tm_layout(asp = 0) Ahora, es posible realizar el mismo ejercicio anterior pero con la estimacion de la proporción de personas en condición de pobreza por zona y sexo. El código computacional apropiado se muestra a continuación. pob_region_Sex &lt;- diseno %&gt;% group_by(Region, Zone, Sex, pobreza) %&gt;% summarise(prop = survey_mean(vartype = &quot;cv&quot;)) %&gt;% filter(pobreza == 1, Zone == &quot;Rural&quot;, Sex == &quot;Female&quot;) shape_temp &lt;- tm_shape(shapeBigCity %&gt;% left_join(pob_region_Sex, by = &quot;Region&quot;)) shape_temp + tm_polygons(&quot;prop&quot;, title = &quot;Pobreza&quot;,) + tm_layout(asp = 0) "],["modelos-de-regresión.html", "Capítulo 7 Modelos de regresión", " Capítulo 7 Modelos de regresión Un modelo de regresión es una herramienta estadística utilizada para analizar la relación entre una variable dependiente y una o más variables independientes. Su objetivo es explicar el valor de la variable dependiente en función de las variables independientes; es decir, busca encontrar la mejor relación matemática que describe la asociación entre estas variables. En este documento, nuestro interés se centra en los modelos de regresión lineal, donde se asume que existe una relación lineal entre la variable dependiente y las independientes. Estos modelos son ampliamente utilizados en diversas disciplinas para entender y predecir fenómenos, tomar decisiones informadas e identificar patrones en los datos. Durante el proceso de construcción del modelo, se estiman los parámetros para minimizar la diferencia entre las estimaciones del modelo y los valores reales observados. Sin embargo, el hecho de que los modelos se ajusten con conjuntos de datos que fueron recolectados siguiendo un diseño de muestreo complejo, hace que la metodología para estimar los parámetros de la regresión deba ajustarse a las particularidades del diseño para así asegurar el insesgamiento de la inferencia. "],["definiciones-básicas.html", "7.1 Definiciones básicas", " 7.1 Definiciones básicas A modo de contexto histórico, Heeringa, West, y Berglund (2017) afirman que los primeros autores en discutir, de manera empírica, el impacto que surten los diseños muestrales complejos en las inferencias relacionadas con modelos de regresión fueron Kish y Frankel (1974); posteriormente Fuller (1975) desarrolló un estimador de varianza para parámetros de modelos de regresión tomando como insumos teóricos la linealización de Taylor con ponderación desigual de las observaciones bajo diseños de muestreo estratificado y de dos etapas. Ahora bien, como es bien sabido, para el uso de la teoría de modelos de regresión se requieren que se cumplan algunos supuestos estadísticos que en ocasiones son difíciles de verificar en la práctica. En este sentido, Shah, Holt, y Folsom (1977) discuten algunos aspectos relacionados con las violaciones de dichos supuestos y dan algunos métodos apropiados para hacer inferencias sobre los parámetros estimados de los modelos de regresión lineal usando datos de encuestas. Asimismo, David A. Binder (1983) obtuvo las distribuciones muestrales de los estimadores para parámetros de regresión en poblaciones finitas y estimadores de varianza relacionados en el contexto de muestras complejas. Skinner, Holt, y Smith (1989) estudiaron las propiedades de los estimadores de las varianzas para los coeficientes de regresión bajo diseños de muestras complejos. Más adelante, Fuller (2002) generó un resumen de los métodos de estimación para modelos de regresión que contienen información relacionada con muestras complejas. Por último, Pfeffermann (2011) realizó una discusión sobre los distintos enfoques basados en el ajuste de modelos de regresión lineal a datos de encuestas de muestras complejas, presentando apoyo empírico para el uso del método “q-weighted”, que será el recomendado en este documento. Un modelo de regresión lineal simple se define como \\(y=\\beta_{0}+\\beta_{1}x+\\varepsilon\\); en donde \\(y\\) se presenta como la variable dependiente, \\(x\\) es la variable independiente y \\(\\beta_{0}\\) y \\(\\beta_{1}\\) son los parámetros del modelo. La variable \\(\\varepsilon\\) se conoce como el error aleatorio del modelo y se define como \\(\\varepsilon=y-\\hat{y}=y-\\beta_{0}+\\beta_{1}x\\). Generalizando el modelo anterior, se definen los modelos de regresión lineal múltiples, al permitir la interacción de la variable dependiente con más de dos variables, justo como se presenta a continuación: \\[ y = \\boldsymbol{x}\\boldsymbol{\\beta}+\\varepsilon = \\sum_{j=0}^{p}\\beta_{j}x_{j}+\\varepsilon = \\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{p}x_{p}+\\varepsilon \\] Otra manera de escribir el modelo de regresión múltiple es: \\[ y_{i} = x_{i}\\boldsymbol{\\beta}+\\varepsilon_{i} \\] En donde, \\(x_{i}=\\left[1\\,x_{1i}\\,\\ldots\\,x_{pi}\\right]\\) y \\(\\boldsymbol{\\beta}^{T}=\\left[\\beta_{0}\\,\\,\\beta_{1}\\,\\,\\ldots\\,\\,\\beta_{p}\\right]\\). El subíndice \\(i\\) hace referencia al elemento muestral o respondiente en el conjunto de datos. Heeringa, West, y Berglund (2017) presenta algunas consideraciones para los modelos de regresión, las cuales son descritas a continuación: \\(E\\left(\\varepsilon_{i}\\mid x_{i}\\right)=0\\), lo que significa que el valor esperado de los residuos condicionado al grupo de covariables es igual a cero. \\(Var\\left(\\varepsilon_{i}\\mid x_{i}\\right)=\\sigma_{y,x}^{2}\\) (homogenidad de varianza) lo que significa que la varianza de los residuos condicionado al grupo de covariables es igual y constante. \\(\\varepsilon_{i}\\mid x_{i}\\sim N\\left(0,\\,\\sigma_{y,x}^{2}\\right)\\) (normalidad en los errores) lo que significa que, los residuos condicionados al grupo de covariables siguen una distribución normal. Esta propiedad también se extiende a la variable respuesta \\(y_{i}\\). \\(cov\\left(\\varepsilon_{i},\\,\\varepsilon_{j}\\mid x_{i},x_{j}\\right)\\) (independencia en los residuales) los residuales en diferentes unidades observadas no están correlacionados con los valores dados por sus variables predictoras. Una vez definido el modelo de regresión lineal y sus supuestos, se puede deducir que la mejor estimación lineal insesgada se define como el valor esperado de la variable dependiente condicionado a las variables independientes \\(x\\) como, \\(E\\left(y\\mid x\\right)=\\hat{\\beta}_{0}+\\hat{\\beta_{1}}x_{1}+\\hat{\\beta}_{2}x_{2}+\\cdots+\\hat{\\beta}_{p}x_{p}\\). \\[ \\hat{y} = E\\left(y\\mid x\\right) = E\\left(\\boldsymbol{x}\\boldsymbol{\\beta}\\right)+E\\left(\\varepsilon\\right) = \\boldsymbol{x}\\boldsymbol{\\beta}+0 = \\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{p}x_{p} \\] y adicionalmente, \\[ var\\left(y_{i}\\mid x_{i}\\right) = \\sigma_{y,x}^{2} \\] Así mismo, se tiene que: \\[ cov\\left(y_{i},y_{j}\\mid x_{i},x_{j}\\right) = 0 \\] Luego, la variable respuesta tiene la siguiente distribución: \\[ y_{i} \\sim N\\left(x_{i}\\boldsymbol{\\beta},\\sigma_{y,x}^{2}\\right) \\] References "],["estimación-de-los-parámetros-en-un-modelo-de-regresión-con-muestras-complejas..html", "7.2 Estimación de los parámetros en un modelo de regresión con muestras complejas.", " 7.2 Estimación de los parámetros en un modelo de regresión con muestras complejas. Una vez se establecen los supuestos del modelo y las características distribucionales de los errores, el paso siguientes es el proceso de estimación de los parámetros. A modo ilustrativo e introductorio, si en lugar de observar una muestra de tamaño \\(n\\) de los \\(N\\) elementos de población se hubiera realizado un censo completo, el parámetro de regresión de población finita \\(\\beta_{1}\\) podría calcularse como sigue: \\[ \\beta_{1} = \\frac{{ \\sum_{i=1}^{N}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)}}{\\sum_{i=1}^{N}\\left(X_{i}-\\bar{X}\\right)^{2}} \\] Ahora bien, cuando se desea estimar los parámetros de un modelo de regresión lineal, pero considerando que la información observada proviene de encuestas con muestras complejas, se altera el enfoque estándar que se le da a la estimación de coeficientes de regresión y sus errores estándar. La principal razón por la que los métodos de estimación de parámetros de los coeficientes de regresión cambian es que la información recolectada por medio de una encuesta compleja generalmente no tiene una idéntica distribución, y tampoco se puede sostener el supuesto de independencia, dado que el diseño muestral así es planeado (dado que los diseños complejos en su mayoría contienen estratificación, conglomerados, probabilidades de selección desiguales, etc.). En este contexto, al ajustar modelos de regresión con este tipo de conjuntos de datos, el uso de estimadores convencionales que se pueden derivar por los m{etodos tradicionales (como máxima verosimilitud, por ejemplo) induciran sesgo puesto que, con estas metodología siempre se está asumiendo que los datos son independientes e idénticamente distribuidos y que provienen de alguna distribución de probabilidad (binomial, Poisson, exponencial, normal, etc.). En su lugar, según Wolter (2007), se emplean métodos no paramétricos robustos basados en linealización de Taylor o métodos de estimación de la varianza usando replicación (Jackknife, bootstrapping, etc) para eliminar el sesgo al incluir el diseño de muestreo en los análisis. Con fines ilustrativos, se mostrará la estimación del parámetro \\(\\beta_{1}\\) y su varianza para una regresión lineal simple. La extensión a la estimación de los parámetros de un modelo de regresión múltiple, algebraicamente es compleja y se sale del contexto de este libro. A continuación, se presenta la estimación de la pendiente y su varianza en un modelo de regresión lineal simple: \\[ \\hat{\\beta_{1}} = \\frac{{\\sum_{h}^{H}\\sum_{\\alpha}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(y_{h\\alpha i}-\\hat{\\bar{y}}_{\\omega}\\right)\\left(x_{h\\alpha i}-\\hat{\\bar{x}}_{\\omega}\\right)}}{{ \\sum_{h}^{H}\\sum_{\\alpha}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(x_{h\\alpha i}-\\hat{\\bar{x}}_{\\omega}\\right)^{2}}} \\] Como se puede observar en la ecuación anterior, el estimador del parámetro es un cociente de totales, por ende, su varianza está dada por: \\[ var\\left(\\hat{\\beta_{1}}\\right) = \\frac{var\\left(\\hat{t}_{xy}\\right)+\\hat{\\beta}_{1}^{2}var\\left(\\hat{t}_{x^{2}}\\right)-2\\hat{\\beta}_{1}cov\\left(\\hat{t}_{xy},\\hat{t}_{x^{2}}\\right)}{\\left(\\hat{t}_{x^{2}}\\right)^{2}} \\] A modo de generalización, según Kish y Frankel (1974), la estimación de la varianza de los coeficientes en un modelo de regresión lineal múltiple, los métodos de aproximación requieren totales ponderados para los cuadrados y productos cruzados de todas las combinaciones \\(y\\) y \\(x = {1 x_{1} … x_{p}}\\). A continuación, se presenta la estimación de estas varianzas: \\[\\begin{eqnarray*} var\\left(\\hat{\\beta}\\right)=\\hat{\\Sigma}\\left(\\hat{\\beta}\\right) &amp; = &amp; \\left[\\begin{array}{cccc} var\\left(\\hat{\\beta}_{0}\\right) &amp; cov\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{1}\\right) &amp; \\cdots &amp; cov\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{p}\\right)\\\\ cov\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{1}\\right) &amp; var\\left(\\hat{\\beta}_{1}\\right) &amp; \\cdots &amp; cov\\left(\\hat{\\beta}_{1},\\hat{\\beta}_{p}\\right)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ cov\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{p}\\right) &amp; cov\\left(\\hat{\\beta}_{1},\\hat{\\beta}_{p}\\right) &amp; \\cdots &amp; var\\left(\\hat{\\beta}_{p}\\right) \\end{array}\\right] \\end{eqnarray*}\\] Para ejemplificar los conceptos trabajados hasta este momento, se tomará la misma base de ejemplo y se inicia con el cargue de las librerías, la base de datos y la definición del diseño de muestreo: knitr::opts_chunk$set(warning = FALSE, message = FALSE, error = FALSE) options(digits = 4) options(tinytex.verbose = TRUE) library (survey) library(srvyr) library(convey) library(TeachingSampling) library(printr) library(stargazer) library(jtools) library(broom) library(tidyverse) library(ggpmisc) data(BigCity, package = &quot;TeachingSampling&quot;) encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) %&gt;% mutate(Age2 = I(Age ^ 2)) library(srvyr) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = T ) Para efectos de los ejemplos y como se ha hecho en anteriores ocasiones, se divide la muestra en sub-grupos de la encuesta como sigue: sub_Urbano &lt;- diseno %&gt;% filter(Zone == &quot;Urban&quot;) sub_Rural &lt;- diseno %&gt;% filter(Zone == &quot;Rural&quot;) sub_Mujer &lt;- diseno %&gt;% filter(Sex == &quot;Female&quot;) sub_Hombre &lt;- diseno %&gt;% filter(Sex == &quot;Male&quot;) En este capítulo se ajustarán los modelos de regresión usando la base de datos de ejemplo que se ha venido trabajando en capítulos anteriores. Puesto que, en modelos de regresión, se utiliza muy frecuente el recurso gráfico. A continuación, se define un tema estándar para generar gráficos con el mismo estilo unificado. Para observar que existe una correlación entre el ingreso y el gasto, las cuales son las variables que se utilizarán para el ajuste de los modelos, se construye un diagrame de puntos usando la librería ggplot. Una vez revisada la información poblacional, se utilizará la información obtenida de la muestra para estimar los parámetros y con ello analizar qué tan buenas son las estimaciones. A continuación, se presenta la sintaxis que permite construir el scatterplot para los datos de la muestra. plot_sin &lt;- ggplot(data = encuesta, aes(x = Expenditure, y = Income)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ x) + theme_cepal() plot_sin + stat_poly_eq(formula = y~x, aes(label = paste(..eq.label.., ..rr.label.., sep = &quot;~~~&quot;), size = 5), parse = TRUE) Como se puede observar, los datos de la muestra tienen una tendencia lineal aunque un poco dispersa a medida que crecen los gastos en las familias. Una vez hecho el análisis gráfico, se procede a ajustar los modelos de regresión lineal. Para comparar el efecto que tiene hacer un correcto uso de los factores de expansión del diseño, primero se ajustará un modelo sin tener encuesta dichos factores como se muestra a continuación: fit_sinP &lt;- lm(Income ~ Expenditure, data = encuesta) summary(fit_sinP) ## ## Call: ## lm(formula = Income ~ Expenditure, data = encuesta) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2318.3 -189.2 -55.7 130.0 1993.2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 121.5159 11.4080 10.7 &lt;2e-16 *** ## Expenditure 1.2201 0.0245 49.7 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 345 on 2603 degrees of freedom ## Multiple R-squared: 0.487, Adjusted R-squared: 0.487 ## F-statistic: 2.47e+03 on 1 and 2603 DF, p-value: &lt;2e-16 Para el modelo ajustado sin factores de expansión, el \\(\\hat{\\beta}_{0}\\) es 121.52 y el \\(\\hat{\\beta}_{1}\\) asociado a la variable gastos es 1.22. Se recalca que la anterior salida produce valores estimados sesgados y su ajuste es simplemente para fines ilustrativos. Por otro lado, hacer un diagrama de dispersión con los datos de la encuesta utilizando los factores de expansión del diseño, es posible usando el argumento mapping = aes(weight = wk) en la función geom_smoothcomo sigue: plot_Ponde &lt;- ggplot(data = encuesta, aes(x = Expenditure, y = Income)) + geom_point(aes(size = wk)) + geom_smooth( method = &quot;lm&quot;, se = FALSE, formula = y ~ x, mapping = aes(weight = wk) ) + theme_cepal() plot_Ponde + stat_poly_eq( formula = y ~ x, aes( weight = wk, label = paste(..eq.label.., ..rr.label.., sep = &quot;~~~&quot;) ), parse = TRUE, size = 5 ) Cuando los datos provienen de levantamientos complejos como los son las encuestas de hogares, ajustar modelos de regresión incluyendo el diseño de muestreo complejo (factores de expansión, estratos y unidades primarias de muestreo) es crucial al analizar las relaciones entre la variable dependiente y las covariables. Sin este tipo de ajustes, los resultados pueden sesgarse y no representar de manera adecuada la realidad, puesto que al ajustar un modelo de regresión ingenuamente, es probable que se produzca un sesgo en los resultados debido a la estructura de muestreo. Los factores de expansión ayudan a corregir este sesgo, garantizando que los resultados sean generalizables a la población de interés. Por otro lado, al ajustar modelos de regresión incluyendo el diseño de muestreo, se mejora la precisión de las estimaciones, lo cual es importante cuando se trabaja con subpoblaciones específicas o grupos minoritarios, donde las muestras pueden ser más pequeñas y la variabilidad puede ser mayor. De esta forma, la estimación de varianzas y errores estándar se torna más precisa, lo que se traduce en intervalos de confianza más exactos y pruebas de hipótesis más robustas. En resumen, ajustar modelos de regresión utilizando factores de expansión en el análisis de datos provenientes de encuestas complejas es esencial para garantizar la validez, representatividad y precisión de los resultados, cumpliendo con estándares estadísticos y éticos. En este sentido, para ajustar modelos teniendo en cuenta los factores de expansión y el diseño de muestreo complejo, se debe recurrir a la función svyglm de la librería survey. En el siguiente ejemplo se ilustra el ajuste de un modelo de regresión con datos de encuestas complejas en donde la variable dependiente es el ingreso en función del gasto. fit_svy &lt;- svyglm(Income ~ Expenditure, design = diseno) fit_svy ## Stratified 1 - level Cluster Sampling design (with replacement) ## With (238) clusters. ## Called via srvyr ## Sampling variables: ## - ids: PSU ## - strata: Stratum ## - weights: wk ## ## Call: svyglm(formula = Income ~ Expenditure, design = diseno) ## ## Coefficients: ## (Intercept) Expenditure ## 103.14 1.26 ## ## Degrees of Freedom: 2604 Total (i.e. Null); 118 Residual ## Null Deviance: 6.35e+08 ## Residual Deviance: 3.11e+08 AIC: 38300 Obteniendo estimaciones para el intercepto de \\(\\hat{\\beta}_{0} = 103.14\\) y para la pendiente de \\(\\hat{\\beta}_{1} = 1.26\\). References "],["la-ponderación-de-pfeffermann.html", "7.3 La ponderación de Pfeffermann", " 7.3 La ponderación de Pfeffermann Heeringa, West, y Berglund (2017) aborda el problema de cómo ponderar correctamente los modelos de regresión y aborda la cuestión de si se deben utilizar los factores de expansión para estimar los coeficientes de regresión al trabajar con datos de encuestas complejas. En este sentido, se debe saber que en la literatura especializada existen dos paradigmas esenciales: El enfoque basado en el diseño de muestreo, el cual se ilustra en este documento, busca hacer inferencias sobre toda la población finita, y el uso de los factores de expansión garantiza que las estimaciones de los parámetros de regresión sean insesgadas. Sin embargo, el uso de los pesos de muestreo no protege contra la mala especificación del modelo; si el investigador ajusta un modelo mal especificado usando los factores de expansión, se estará calculando estimaciones insesgadas de los parámetros de regresión en un modelo que no describe bien las relaciones en la población finita. El enfoque basado en modelos de población, que argumenta que el uso de los factores de expansión en la estimación no debería ser necesario si el modelo está correctamente especificado. Bajo este enfoque la inclusión de los pesos de muestreo sólo sirve para aumentar la varianza de los estimadores, induciendo errores estándar más grande de lo que deberían ser. La elección entre estos dos enfoques debería depender de la sensibilidad de las inferencias a diferentes métodos de estimación. Es posible recomendar que se utilice software estadístico para ajustar modelos de regresión con y sin pesos de muestreo para evaluar la sensibilidad de los resultados. Si el uso de pesos produce estimaciones y conclusiones sustancialmente diferentes, se sugiere que el modelo podría estar mal especificado y se debería optar por las estimaciones ponderadas. Sin embargo, si el uso de pesos no altera significativamente las estimaciones de los parámetros de la regresión y solo aumenta considerablemente los errores estándar, podría se un indicio de que el modelo está bien especificado y, por tanto, el uso de los pesos puede no ser necesario. Una solución intermedia a estos dos enfoques está dada por Pfeffermann (2011), quien propuso una variante (llamada q-weighted approach) haciendo una especificación ligeramente diferente de los factores de expansión, la cual se detalla a continuación: Ajustar un modelo de regresión a los pesos finales de la encuesta utilizando las variables predictoras en el modelo de regresión de interés. Obtener las predicciones de los pesos de la encuesta para cada caso como una función de las variables predictoras en el conjunto de datos. Dividir los factores de expansión de la encuesta por los valores predichos en el paso anterior. Usar los nuevos pesos obtenidos para el ajuste de los modelos de regresión. A continuación se ejemplificará la forma de calcular estas nuevas ponderaciones, asumiendo que el ingreso está relacionado con el gasto, la zona, el sexo, y el cuadrado de la edad. De esta forma, el siguiente código computacional puede ser usado: modwk &lt;- lm(wk ~ Expenditure + Zone + Sex + Age2, data = encuesta) wkpred &lt;- predict(modwk) encuesta %&lt;&gt;% mutate(qw = wk / wkpred) diseno_qwgt &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = qw, nest = T ) A continuación se muestra el diagrama de dispersión entre los pesos q-weighted y los factores de expansión originales: plot(encuesta$wk, encuesta$qw) Por otro lado, se puede observar que el comportamiento estructural de ambos conjuntos de pesos es similar. par(mfrow=c(2, 2)) hist(encuesta$wk) hist(encuesta$qw) boxplot(encuesta$wk) boxplot(encuesta$qw) Por ende, utilizando las nuevas ponderaciones, el modelo queda especificado en R de la siguiente forma fit_svy &lt;- svyglm(Income ~ Expenditure + Zone + Sex + Age2, design = diseno_qwgt) summary(fit_svy) ## ## Call: ## svyglm(formula = Income ~ Expenditure + Zone + Sex + Age2, design = diseno_qwgt) ## ## Survey design: ## Called via srvyr ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 72.69927 68.14862 1.07 0.288 ## Expenditure 1.18859 0.21578 5.51 2.2e-07 *** ## ZoneUrban 70.97453 42.13154 1.68 0.095 . ## SexMale 20.93437 15.99093 1.31 0.193 ## Age2 0.00824 0.00564 1.46 0.147 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 122148) ## ## Number of Fisher Scoring iterations: 2 References "],["diagnóstico-del-modelo.html", "7.4 Diagnóstico del modelo", " 7.4 Diagnóstico del modelo En el análisis de las encuestas de hogares cuando se ajusten modelos estadísticos es importante realizar verificaciones de calidad y con esto tener certezas de las conclusiones que se obtienen. La mayoría de textos académicos dan un panorama bastante detallado de los supuestos y consideraciones que se deben tener en cuenta para tener un modelo correctamente definido. A continuación, se enlistan algunas de ellas: Determinar si el modelo proporciona un adecuado ajuste a los datos. Examinar si los errores están normalmente distribuidos. Examinar si los errores tienen varianza constante. Verificar si los errores se pueden asumir no correlacionados. Determinar si alguno de los datos tiene valores con un efecto inusualmente grande sobre el modelo de regresión estimado, estos se conocen como datos influyentes. Determinar si algún punto no sigue la tendencia de la mayoría de los datos cuando se toma en cuenta el modelo, estos puntos se conocen como outliers. En este capítulo se abordarán alguno de los supuestos que se deben tener en cuenta al momento de ajustar un modelo de regresión lineal. 7.4.1 Coeficientes de determinación Una medida de la bondad del ajuste en un modelo de regresión es el coeficiente de determinación o coeficiente de correlación múltiple (\\(R^{2})\\)). Dicho parámetro estima la proporción de la varianza de la población explicada por la regresión y oscila entre cero y uno. Entre más cercano esté de uno significa que mayor variabilidad explica y lo contrario ocurrirá si está cerca de cero. Lo anterior, en ocasiones es muy ambiguo puesto que, por ejemplo, en algunas disciplinas es posible obtener \\(R^{2}\\) con valores muy altos, mientras que en otras se obtienen \\(R^{2}\\) bajos. El cálculo de este parámetro a nivel poblacional se lleva a cabo de la siguiente manera: \\[ R^{2} = 1-\\frac{SSE}{SST} \\] Donde, \\(SST= \\sum_{i=1}^N (y_i - \\bar{y})^2\\) es la suma de cuadrados totales, que representa la variabilidad total en la variable dependiente, y \\(SSE= \\sum_{i=1}^N (y_i - x_i \\beta)^2\\) es la suma de cuadrados del error, que representa la variabilidad no explicada por el modelo de regresión. El estimador de este parámetro usando muestras complejas está dado por: \\[ \\widehat{R}_{\\omega}^{2} = 1-\\frac{\\widehat{SSE}_{\\omega}}{\\widehat{SST}_{\\omega}} \\] En donde \\(\\widehat{SSE}_{\\omega}\\) es la suma de cuadrados del error estimada, dada por: \\[ \\widehat{SSE}_{\\omega} = \\sum_{h}^{H}\\sum_{\\alpha}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(y_{h\\alpha i}-x_{h\\alpha i}\\hat{\\beta}\\right)^{2} \\] Para continuar con los modelos ajustados en la sección anterior, se procede a estimar los \\(R^{2}\\) utilizando R. Inicialmente, se procede a estimar los parámetros del modelo utilizando la función svyglm de survey como se mostró anteriormente y también, se ajusta un modelo solo con el intercepto para obtener la estimación de la SST: modNul &lt;- svyglm(Income ~ 1, design = diseno_qwgt) s1 &lt;- summary(fit_svy) s0 &lt;- summary(modNul) wSST &lt;- s0$dispersion wSSE &lt;- s1$dispersion Por tanto, la estimación del \\(R^{2}\\) es: R2 = 1- wSSE/wSST R2 ## variance SE ## [1,] 0.513 20579 7.4.2 Residuales estandarizados En el diagnóstico de los modelos, es crucial el análisis de los residuales. Estos análisis proporcionan, bajo el supuesto que el modelo ajustado es adecuado, una estimación de los errores. Por tanto, un estudio cuidadoso de los residuales deberá ayudar al investigador a concluir si el procedimiento de ajuste no ha violado los supuestos o si, por el contrario, uno o varios de los supuestos no se verifican y hay necesidad de revisar el procedimiento de ajuste. Para realizar el análisis de los residuales, en primera instancia, se definen los residuales de Pearson (Heeringa, West, y Berglund 2017) como sigue: \\[ r_{p_{i}} = \\left(y_{i}-\\mu_{i}\\left(\\hat{\\beta}_{\\omega}\\right)\\right)\\sqrt{\\frac{\\omega_{i}}{V\\left(\\hat{\\mu}_{i}\\right)}} \\] Donde, \\(\\mu_{i}\\) es el valor esperado de \\(y_{i}\\), \\(w_{i}\\) es la ponderación de la encuesta para el i-ésimo individuo del diseño muestral complejo, Por último, \\(V(\\mu_{i})\\) es la función de varianza del resultado. Sobre estos residuales se realizan los análisis de normalidad y varianza constante. Si el supuesto de varianza constante no se cumple, los estimadores siguen siendo insesgados y consistentes, pero dejan de ser eficientes, es decir, dejan de ser los mejores en cuanto a que ya no tienen la menor varianza entre todos los estimadores insesgados. Una de las formas de analizar el supuesto de varianzas constantes en los errores es hacerlo de manera gráfica. Para ello, se grafica los residuos del modelo contra \\(\\hat{y}\\) o los residuos del modelo contra \\(X_{i}\\). Si al realizar estos gráficos se logra evidenciar cualquier patrón que no sea una nube de puntos constante, se puede concluir que la varianza de los errores no es constante. A manera de ejemplificar los conceptos vistos, se van a utilizar los modelos previamente ajustados. En primero instancia, el análisis del modelo se centrará en los supuestos de normalidad y varianza constante en los errores. La librería svydiags está pensada en ayudar en el diagnostico de modelos de regresión lineal, siendo una extensión más para complementar el paquete survey. Con las librerías svydiags se extraen los residuales estandarizados como sigue: library(svydiags) stdresids = as.numeric(svystdres(fit_svy)$stdresids) diseno_qwgt$variables %&lt;&gt;% mutate(stdresids = stdresids) Una forma muy común para hacer la evaluación de la normalidad de los residuales es realizar un gráfico cuantil-cuantil normal o QQplot. El QQplot es una gráfica de cuantiles para los residuos observados frente a los calculados a partir de una distribución normal teórica que tiene la misma media y varianza que la distribución de los residuos observados. Por lo tanto, una línea recta de 45° en este gráfico sugeriría que la normalidad es una suposición razonable para los errores aleatorios en el modelo. qqnorm(stdresids) qqline(stdresids, col = 2) También podemos hacer el análisis de normalidad por medio del histograma de los residuales estandarizados con el siguiente código, en cuya salida se puede observar que el histograma de los residuales estandarizados (barras y línea azul) no necesariamente sigue el comportamiento de una distribución normal (línea roja). ggplot(data = diseno_qwgt$variables, aes(x = stdresids)) + geom_histogram( aes(y = ..density..), colour = &quot;black&quot;, fill = &quot;blue&quot;, alpha = 0.3 ) + geom_density(size = 2, colour = &quot;blue&quot;) + geom_function(fun = dnorm, colour = &quot;red&quot;, size = 2) + theme_cepal() + labs(y = &quot;&quot;) Como se puedo observar en el QQplot, hay evidencia gráfica de que los errores no se distribuyen según una distribución normal. Por otro lado, un análisis que se realiza de manera gráfica es la verificación de que las varianzas de los residuales sean constantes, condicionadas a los valores de las covariables. Esto se puede observar a través de diagramas de dispersión de los residuales estandarizados contra las covariables usadas en el modelo. Primero, se agregan las predicciones a la base de datos para poder realizar las gráficas. library(patchwork) diseno_qwgt$variables %&lt;&gt;% mutate(pred = predict(fit_svy)) g2 &lt;- ggplot(data = diseno_qwgt$variables, aes(x = Expenditure, y = stdresids)) + geom_point() + geom_hline(yintercept = 0) + theme_cepal() g3 &lt;- ggplot(data = diseno_qwgt$variables, aes(x = Age2, y = stdresids)) + geom_point() + geom_hline(yintercept = 0) + theme_cepal() g4 &lt;- ggplot(data = diseno_qwgt$variables, aes(x = Zone, y = stdresids)) + geom_point() + geom_hline(yintercept = 0) + theme_cepal() g5 &lt;- ggplot(data = diseno_qwgt$variables, aes(x = Sex, y = stdresids)) + geom_point() + geom_hline(yintercept = 0) + theme_cepal() (g2 | g3) / (g4 | g5) Como se puede observar en las gráficas de gastos y edad al cuadrado, ambas muestran que las varianzas no son constantes, puesto que la dispersión crece o decrece a medida que cambia el valor de estas covariables. 7.4.3 Observaciones influyentes Otras técnicas utilizadas también para el análisis de los modelos consisten en el análisis de observaciones influyentes. Una observación se denomina influyente si al removerla de la nube de puntos esta causa un cambio grande en el ajuste del modelo. Es importante resaltar que un punto influyente podría o no ser un dato atípico. Para detectar observaciones influyentes es necesario tener claro qué tipo de influencia se quiere detectar. Lo anterior puesto que, por ejemplo, una observación puede ser influyente sobre la estimación de los parámetros, pero no para la estimación de la varianza del error. A continuación, se presentan las distintas técnicas estadísticas para la detección de datos influyentes: Distancia de Cook: diagnostica si la i-ésima observación es influyente en la estimación del modelo, por estar lejos del centro de masa de los datos. En general, distintos autores consideran que las observaciones son influyentes cuando esta cantidad es mayor que 2 o 3. Estadístico \\(D_fBeta_{(i)}\\): este estadístico mide el cambio en la estimación del vector de coeficientes de regresión cuando la observación es eliminada. Se determina que la i-ésima observación es influyente para \\(B_j\\) si \\(\\mid D_{f}Betas_{\\left(i\\right)j}\\mid\\geq\\frac{z}{\\sqrt{n}}\\) con \\(z = 2\\). Como alternativa, se puede usar \\(t_{0.025,n-p}/\\sqrt(n)\\) donde \\(t_{0.025,n-p}\\) es el percentil \\(97.5\\) Estadístico \\(D_{f}Fits_{\\left(i\\right)}\\)*: este estadístico mide el cambio en el ajuste del modelo cuando se elimina una observación particular. En esta instancia, la \\(i\\)-ésima observación se considera influyente en el ajuste del modelo si \\(\\mid DfFits\\left(i\\right)\\mid\\geq z\\sqrt{\\frac{p}{n}}\\) con \\(z = 2\\). Para ejemplificar los conceptos definidos, se seguirá con los modelos ajustados en la sección anterior. Una vez ajustados estos modelos y verificados los supuestos, se procede a hacer el cálculo de la distancia de Cook usando la función svyCooksD del paquete svydiags como sigue: library(svydiags) d_cook = data.frame(cook = svyCooksD(fit_svy), id = 1:length(svyCooksD(fit_svy))) ggplot(d_cook, aes(y = cook, x = id)) + geom_point() + theme_bw(20) Como se puede observar, ninguna de las distancias de Cook’s es mayor a 3; por ende podemos afirmar que no existen observaciones influyentes. Ahora bien, si se desea corroborar que no hay observaciones influyentes utilizando el estadístico \\(D_{f}Betas_{\\left(i\\right)j}\\), esto se puede realizar con la función svydfbetas como se muestra a continuación: d_dfbetas &lt;- data.frame(t(svydfbetas(fit_svy)$Dfbetas)) colnames(d_dfbetas) &lt;- paste0(&quot;Beta_&quot;, 0:4) d_dfbetas %&gt;% slice(1:10) Beta_0 Beta_1 Beta_2 Beta_3 Beta_4 0.0005 -2e-04 0.0020 -0.0045 -0.0076 -0.0005 -1e-04 0.0013 0.0026 -0.0031 -0.0008 -1e-04 0.0008 0.0022 0.0008 -0.0004 -1e-04 0.0011 -0.0031 0.0007 -0.0008 0e+00 0.0008 0.0021 0.0014 0.0009 5e-04 -0.0037 -0.0066 0.0103 0.0027 4e-04 -0.0032 -0.0080 -0.0029 0.0011 3e-04 -0.0029 0.0081 -0.0045 0.0030 3e-04 -0.0031 -0.0082 -0.0054 -0.0003 4e-04 0.0012 -0.0039 -0.0042 Una vez calculado los \\(D_{f}Betas_{\\left(i\\right)j}\\) se procede a reacomodar la salida para verificar cuáles observaciones son influyentes. Para esto, de calcula el umbral (cutoff) para definir si es o no influyente la observación. Ese umbral es tomado de las salidas de la función svydfbetas. Por último, se genera una variable dicotómica que indique si la observación es o no influyente como se muestra a continuación: d_dfbetas$id &lt;- 1:nrow(d_dfbetas) d_dfbetas &lt;- reshape2::melt(d_dfbetas, id.vars = &quot;id&quot;) cutoff &lt;- svydfbetas(fit_svy)$cutoff d_dfbetas %&lt;&gt;% mutate(Criterio = ifelse(abs(value) &gt; cutoff, &quot;Si&quot;, &quot;No&quot;)) tex_label &lt;- d_dfbetas %&gt;% filter(Criterio == &quot;Si&quot;) %&gt;% arrange(desc(abs(value))) %&gt;% slice(1:10L) tex_label id variable value Criterio 889 Beta_0 0.3284 Si 890 Beta_1 -0.3043 Si 891 Beta_0 0.3037 Si 889 Beta_1 -0.2973 Si 891 Beta_1 -0.2934 Si 890 Beta_0 0.2903 Si 889 Beta_4 -0.2526 Si 890 Beta_4 -0.2270 Si 2311 Beta_4 0.2122 Si 890 Beta_3 0.2029 Si Como se pudo observar en la salida anterior hay varias observaciones que resultan influyentes dado el criterio del \\(D_{f}Betas_{\\left(i\\right)j}\\). A continuación, y de manera ilustrativa, se grafican los \\(D_{f}Betas_{\\left(i\\right)j}\\) y el umbral con el fin de ver de manera gráfica aquellas observaciones influyentes, teniendo en cuenta que, aquellos puntos rojos en la gráfica representan observaciones influyentes. ggplot(d_dfbetas, aes(y = abs(value), x = id)) + geom_point(aes(col = Criterio)) + geom_text(data = tex_label, angle = 45, vjust = -1, aes(label = id)) + geom_hline(aes(yintercept = cutoff)) + facet_wrap(. ~ variable, nrow = 2) + scale_color_manual(values = c(&quot;Si&quot; = &quot;red&quot;, &quot;No&quot; = &quot;black&quot;)) + theme_cepal() Si el objetivo es detectar observaciones influyentes pero considerando ahora la estadística \\(D_{f}Fits_{\\left(i\\right)}\\), se utiliza la función svydffits y se siguen los mismos pasos mostrados para el estadístico \\(D_{f}Betas_{\\left(i\\right)j}\\).A continuación se muestra el código computacional apropiado para realizarlo: d_dffits &lt;- data.frame(dffits = svydffits(fit_svy)$Dffits, id = 1:length(svydffits(fit_svy)$Dffits)) cutoff &lt;- svydffits(fit_svy)$cutoff d_dffits %&lt;&gt;% mutate(C_cutoff = ifelse(abs(dffits) &gt; cutoff, &quot;Si&quot;, &quot;No&quot;)) ggplot(d_dffits, aes(y = abs(dffits), x = id)) + geom_point(aes(col = C_cutoff)) + geom_hline(yintercept = cutoff) + scale_color_manual(values = c(&quot;Si&quot; = &quot;red&quot;, &quot;No&quot; = &quot;black&quot;)) + theme_cepal() Como se puede observar en el gráfico anterior, también hay observaciones influyentes utilizando \\(D_{f}Fits_{\\left(i\\right)}\\), las cuales se muestran en rojo en el gráfico. References "],["inferencia-sobre-los-parámetros-del-modelo.html", "7.5 Inferencia sobre los parámetros del Modelo", " 7.5 Inferencia sobre los parámetros del Modelo Una vez evaluado el correcto ajuste del modelo utilizando las metodologías vistas anteriormente y corroborando las propiedades distribucionales de los errores y, por ende, de la variable respuesta \\(y\\), el paso siguiente es verificar si los parámetros estimados son significativos y si las covariables utilizadas para ajustar el modelo aportan valor en la explicación y/o predicción de la variable de estudio y el fenómeno de interés. Dadas las propiedades distribucionales de los estimadores de los coeficientes de regresión, un estadístico de prueba natural para evaluar la significación de dicho parámetro se basa en la distribución t-student y se describe a continuación: \\[ t = \\frac{\\hat{\\beta}_{k}-\\beta_{k}}{se\\left(\\hat{\\beta}_{k}\\right)}\\sim t_{n-p} \\] Donde \\(p\\) es el número de parámetros del modelo y \\(n\\) el tamaño de la muestra de la encuesta. En este sentido, el estadístico de prueba anterior evalúa las hipótesis \\(H_{0}:\\beta_{k}=0\\) versus la alternativa \\(H_{1}:\\beta_{k}\\neq0\\). Asimismo, se puede construir un intervalo de confianza al \\((1-\\alpha)\\times100\\%\\) para \\(\\beta_{k}\\), el cual está dado por: \\[ \\hat{\\beta}_{k}\\pm t_{1-\\frac{\\alpha}{2},\\,df}\\,se\\left(\\hat{\\beta}_{k}\\right) \\] Donde, los grados de libertad (\\(df\\)) para el intervalo en una encuesta de hogares (muestras complejas) está dado por el número de conglomerados finales de la primera etapa menos el número de estratos de la etapa primaria \\(\\left(df=\\sum_{h}a_{h}-H\\right)\\). Para la aplicación de las temáticas vistas (juzgamiento de la prueba de hipótesis de significación y construcción de los intervalos de confianza para los parámetros) utilizaremos el modelo que se ha venido trabajando como ejemplo y aplicaremos las funciones summary.svyglm para las pruebas t y confint.svyglm para los intervalos de confianza como sigue: survey:::summary.svyglm(fit_svy) ## ## Call: ## svyglm(formula = Income ~ Expenditure + Zone + Sex + Age2, design = diseno_qwgt) ## ## Survey design: ## Called via srvyr ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 72.69927 68.14862 1.07 0.288 ## Expenditure 1.18859 0.21578 5.51 2.2e-07 *** ## ZoneUrban 70.97453 42.13154 1.68 0.095 . ## SexMale 20.93437 15.99093 1.31 0.193 ## Age2 0.00824 0.00564 1.46 0.147 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 122148) ## ## Number of Fisher Scoring iterations: 2 survey:::confint.svyglm(fit_svy) 2.5 % 97.5 % (Intercept) -62.2900 207.6886 Expenditure 0.7612 1.6160 ZoneUrban -12.4799 154.4290 SexMale -10.7406 52.6093 Age2 -0.0029 0.0194 De lo anterior se puede observar que, con una confianza del 95%, la variable Expenditure resulta ser significativa para la explicación del fenómeno de interés y ese mismo resultado lo reflejan los intervalos de confianza, puesto que no contienen al cero. "],["estimación-y-predicción.html", "7.6 Estimación y predicción", " 7.6 Estimación y predicción Según Neter, Wasserman, y Kutner (1996), los modelos de regresión lineales son utilizado esencialmente con dos fines. Uno es tratar de explicar la variable de interés en términos de covariables que pueden encontrarse en la encuesta o en registros administrativos, censos, etc. Adicionalmente, también son usados para predecir valores de la variable en estudio ya sea dentro del intervalo de valores recogidos en la muestra o por fuera de dicho intervalo. Lo primero se ha abordado a lo largo de todo el capítulo y lo segundo se obtiene de la siguiente manera: \\[ \\hat{E}(y_{i}\\mid\\boldsymbol{x}_{obs,i})=\\boldsymbol{x}_{obs,i}\\hat{\\boldsymbol{\\beta}} \\] De manera explícita, en el modelo que ejemplifica este capítulo, la expresión para las predicciones sería la siguiente: \\[ \\hat{E}(y_{i}\\mid\\boldsymbol{x}_{obs,i})=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x_{1i} \\] La varianza de la estimación se calcula de la siguiente manera: \\[ var\\left(\\hat{E}\\left(y_{i}\\mid x_{obs,i}\\right)\\right) = x&#39;_{obs,i}cov\\left(\\hat{\\beta}\\right)x{}_{obs,i} \\] A continuación, se presenta cómo se realiza la estimación del valor esperado utilizando R. Por ende, primero se estiman los parámetros del modelo: term estimate std.error statistic p.value (Intercept) 72.6993 68.1486 1.067 0.2883 Expenditure 1.1886 0.2158 5.508 0.0000 ZoneUrban 70.9745 42.1315 1.685 0.0948 SexMale 20.9344 15.9909 1.309 0.1931 Age2 0.0082 0.0056 1.460 0.1469 Por lo anterior, la estimación del valor esperado o predicción queda: \\[ \\hat{E}(y_{i}\\mid\\boldsymbol{x}_{obs,i})=103.136+1.263x_{1i} \\] Para calcular la varianza de la estimación, primero se deben obtener las varianzas de la estimación de los parámetros: vcov(fit_svy) (Intercept) Expenditure ZoneUrban SexMale Age2 (Intercept) 4644.2350 -13.5405 579.8621 310.2784 -0.2086 Expenditure -13.5405 0.0466 -4.5712 -0.4802 0.0006 ZoneUrban 579.8621 -4.5712 1775.0665 -148.0881 -0.0647 SexMale 310.2784 -0.4802 -148.0881 255.7098 -0.0034 Age2 -0.2086 0.0006 -0.0647 -0.0034 0.0000 Ahora bien, se procede a realizar los cálculos como lo indica la expresión mostrada anteriormente: xobs &lt;- model.matrix(fit_svy) %&gt;% data.frame() %&gt;% slice(1) %&gt;% as.matrix() cov_beta &lt;- vcov(fit_svy) %&gt;% as.matrix() as.numeric(xobs %*% cov_beta %*% t(xobs)) ## [1] 1999 Si el objetivo ahora es calcular el intervalo de confianza para la predicción se utiliza la siguiente ecuación: \\[ \\boldsymbol{x}_{obs,i}\\hat{\\beta}\\pm t_{\\left(1-\\frac{\\alpha}{2},n-p\\right)}\\sqrt{var\\left(\\hat{E}\\left(y_{i}\\mid\\boldsymbol{x}_{obs,i}\\right)\\right)} \\] Para realizar los cálculos en R, se utiliza la función confint y predict como sigue: pred &lt;- data.frame(predict(fit_svy, type = &quot;link&quot;)) pred_IC &lt;- data.frame(confint(predict(fit_svy, type = &quot;link&quot;))) colnames(pred_IC) &lt;- c(&quot;Lim_Inf&quot;, &quot;Lim_Sup&quot;) pred_IC Ahora, de manera gráfica las estimaciones y los intervalos de confianza para algunas observaciones se verían de la siguiente forma: pred &lt;- bind_cols(pred, pred_IC) pred$Expenditure &lt;- encuesta$Expenditure pred %&gt;% slice(1:6L) pd &lt;- position_dodge(width = 0.2) ggplot(pred %&gt;% slice(1:100L), aes(x = Expenditure , y = link)) + geom_errorbar(aes(ymin = Lim_Inf, ymax = Lim_Sup), width = .1, linetype = 1) + geom_point(size = 2, position = pd) + theme_bw() Por último, si el interés es hacer una predicción fuera del rango de valores que fue capturado en la muestra, se debe tener en cuenta que la expresión de la varianza cambia ligeramente. En ese caso, la varianza para la predicción se hace siguiendo la siguiente ecuación: \\[ var\\left(\\hat{E}\\left(y_{i}\\mid\\boldsymbol{x}_{obs,i}\\right)\\right)=\\boldsymbol{x}_{obs,i}^{t}cov\\left(\\boldsymbol{\\beta}\\right)\\boldsymbol{x}_{obs,i} + \\hat{\\sigma}^2_{yx} \\] Para ejemplificarlos en R, supongamos que se desea predecir el valor del ingreso para un gasto de 2600 (valor que no fue observado en la muestra). El primer paso es crear un conjunto de datos nuevos, así: datos_nuevos &lt;- data.frame( Expenditure = 1600, Age2 = 40 ^ 2, Sex = &quot;Male&quot;, Zone = &quot;Urban&quot; ) Luego, se construye la matriz de observaciones y se calcula la varianza como sigue: x_noObs = matrix(c(1, 1600, 1, 1, 40 ^ 2), nrow = 1) as.numeric(sqrt(x_noObs %*% cov_beta %*% t(x_noObs))) ## [1] 264.7 Por último, el intervalo de confianza sigue la siguiente ecuación: \\[ \\boldsymbol{x}_{obs,i}\\hat{\\beta}\\pm t_{\\left(1-\\frac{\\alpha}{2},n-p\\right)}\\sqrt{var\\left(\\hat{E}\\left(y_{i}\\mid\\boldsymbol{x}_{obs,i}\\right)\\right)+\\hat{\\sigma}_{yx}^{2}} \\] En R la predicción se realiza usando la función predict sobre el objeto fit_svy de la siguiente manera. Esta salida provee la predicción puntual y su correspondiente error estándar, el cual es idéntico al calculado anteriormente. predict(fit_svy, newdata = datos_nuevos, type = &quot;link&quot;) ## link SE ## 1 2080 265 Finalmente, el intervalo de confianza se calcula usando la función confint, así: confint(predict(fit_svy,newdata = datos_nuevos)) 2.5 % 97.5 % 1561 2598 References "],["modelos-lineales-generalizados.html", "Capítulo 8 Modelos lineales generalizados", " Capítulo 8 Modelos lineales generalizados Los Modelos Lineales Generalizados (MLG) constituyen una extensión natural a los modelos lineales tradicionales al incorporar flexibilidad en la elección de la distribución de la variable de interés. Mientras que los modelos lineales clásicos asumen normalidad como un imperativo en el ajuste e inferencia, los modelos lineales generalizados permiten abordar una variedad más amplia de situaciones, incluyendo respuestas no gausianas y variables de respuesta no lineales. El nombre que reciben se debe a que estos son una genralización de los modelos lineales basados en el supuesto de distribución normal para la variable respuesta. Al igual que los modelos lineales clásicos, tratados en capítulos anteriores, los MLG tienen aplicación en todas las disciplinas del saber. Por ejemplo, si la variable de interés es dicotómica, no tendría ningún sentido ajustar un modelo de regresión normal, y se debería usar un modelo de regresión logística para modelar la probabilidad de éxito en términos de una combinación lineal de las variables predictoras. Otro ejemplo es la regresión de Poisson, que emplea la función de enlace log para modelar tasas de eventos en situaciones donde la variable respuesta es el recuento de ocurrencias, como en el análisis de datos de frecuencia de eventos. En resumen, estos modelos lineales generalizados proporcionan una aproximación unificada a la mayoría de los procedimientos usados en estadística aplicada. Nelder y Wedderburn (1972) presentaron por primera vez el término en un artículo que, sin lugar a dudas, es uno de los más importantes publicados en el área de estadística, por su gran impacto y aplicación en diferentes disciplinas. En esta publicación se demostró que muchos de los métodos estadísticos ampliamente usados en la época, aparentemente desligados unos de otros, tales como la regresión lineal múltiple, el análisis probit, el análisis de datos provenientes de ensayos controlados, los modelos logit para proporciones, los modelos log-lineales para conteos, los modelos de regresión para datos de supervivencia, entre otros, se podían tratar con un marco teórico unificado y que las estimaciones de máxima verosimilitud para los parámetros de esos modelos podían obtenerse por el mismo algoritmo iterativo. Los desarrollos teóricos en modelos lineales clásicos parten del supuesto que la variable respuesta tiene distribución normal, cuando un fenómeno en estudio genera datos para los cuales no es razonable la suposición de normalidad, como por ejemplo cuando la respuesta es categórica, una proporción o un conteo, obviamente la respuesta no es normal y no es recomendable analizar los datos suponiendo normalidad. Otro supuesto de los modelos lineales clásicos es el de homogeneidad de la varianza, situación que no se verifica cuando la respuesta es, por ejemplo, una variable aleatoria de poisson, distribución donde la media y la varianza son iguales; es decir, en este modelo un cambio en la media necesariamente implica cambio en la varianza. Los modelos lineales generalizados son apropiados para modelar datos en condiciones de no normalidad y varianza no constante. Específicamente, en las encuestas de hogares existen variables que meritan su análisis usando modelos lineales generalizados. Es por esto que, este capítulo es de relevancia en este texto. Para ejemplificar los conceptos, inicialmente se cargan las librerías y la base de datos como sigue: options(digits = 4) options(tinytex.verbose = TRUE) library (survey) library(srvyr) library(convey) library(TeachingSampling) library(printr) library(stargazer) library(broom) library(jtools) library(modelsummary) library(patchwork) Cargue de las bases de datos, encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) %&gt;% mutate(Age2 = I(Age ^ 2)) Por último, se define el diseño muestral utilizando la ponderación de Pfefferman, tal como se vio en el capítulo anterior. modwk &lt;- lm(wk ~ Expenditure + Zone + Sex + Age2, data = encuesta) wkpred &lt;- predict(modwk) encuesta %&lt;&gt;% mutate(qw = wk / wkpred) diseno_qwgt &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = qw, nest = T ) A continuación, se generan nuevas variables en el diseño para ser utilizadas en los ejemplos. En particular, se crea una variable dicotómica que indica si la persona está o no clasificada como pobre. Además, se filtra la base excluyendo a los menores de edad y se crea otra variable que indica si la persona está o no empleada. diseno &lt;- diseno_qwgt %&gt;% filter(!is.na(Employment)) %&gt;% mutate( Pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0), Desempleo = ifelse(Employment == &quot;Unemployed&quot;, 1, 0) ) References "],["modelo-de-regresión-logistica-para-proporciones.html", "8.1 Modelo de regresión logistica para proporciones", " 8.1 Modelo de regresión logistica para proporciones La regresión logística es un método de regresión que permite estimar la probabilidad de éxito de una variable cualitativa binaria en función de otras covariables continuas o discretas. La variable de interés es de tipo binario o dicotómico, es decir, tomará el valor de uno (1) si cumple con la condición que se está observando, y cero (0) en caso contrario. De este modo, las observaciones son separadas en los grupos formados según el valor que tome la variable empleada como predictor. Si una variable categórica con dos posibles niveles se codifica mediante unos (1) y ceros (0), es posible matemáticamente ajustar un modelo de regresión lineal \\(\\beta_0 + \\beta_1 x\\) usando alguna técnica de estimación como mínimos cuadrados. Pero surge un inconveniente en esta aproximación, y es que, al tratarse de una recta, es perfectamente posible, obtener valores estimados que sean menores que cero, o incluso mayores que uno, lo cual, claramente va en contravía con la teoría, que requiere que las probabilidades siempre se encuentren dentro del rango [0 ,1]. El objetivo de la regresión logística es modelar el logaritmo de la probabilidad de pertenecer a cada grupo; por lo que finalmente la asignación se realiza en función de las probabilidades obtenidas. De esta manera, la regresión logística es ideal para modelar la probabilidad de ocurrencia de un evento en función de diversos factores. Por ende, este tipo de modelos son útiles para correlacionar la probabilidad de ocurrencia de un fenómeno con otras covariables. Por ende la probabilidad aproximada del suceso se denotará mediante una función logística del tipo: \\[ \\pi(\\textbf{x})= Pr(y = 1 | \\textbf{x}) = \\frac{\\exp\\{\\textbf{x}&#39;\\boldsymbol{\\beta}\\}}{1+\\exp\\{\\textbf{x}&#39;\\boldsymbol{\\beta}\\}} \\] Se debe recalcar que no se debe usar una regresión lineal cuando se tiene una variable de tipo binario como variable dependiente, ya que no es posible estimar la probabilidad del evento estudiado de manera directa, por esta razón se emplea una regresión logística, en la que para obtener las estimaciones de la probabilidad del evento estudiado se debe realizar una transformación (logit). Ahora bien, al aplicar la función logit a ambos lados, la expresión se torna similar al cálculo de una regresión lineal: \\[ g(\\textbf{x})=logit(\\pi(\\textbf{x}))=ln \\left\\{ \\frac{\\pi(\\textbf{x})}{1-\\pi(\\textbf{x})} \\right \\}= \\textbf{x}&#39;\\boldsymbol{\\beta} \\] De esta forma se asume que existe una relación de tipo lineal entre cada una de las variables explicativas y el logit de la variable respuesta. Obsérvese que existen al menos tres grandes diferencias entre la regresión logística y la regresión lineal. En primer lugar, en la regresión logística no se requiere una relación lineal entre las variables explicativas y la variable de interés; tampoco se requiere que los residuos del modelo guarden una distribución de tipo normal; y, finalmente, no es necesario que los residuos del modelo presenten una varianza constante, es decir que sean homoscedásticos. Usando técnicas apropiadas que incluyan el diseño de muestreo complejo en la inferencia, la probabilidad estimada de que la variable de interés tome el valor uno, que a su vez es también la esperanza de la variable de interés, en un modelo de regresión logística es la siguiente: \\[ \\hat{\\pi}(\\textbf{x})= \\frac{\\exp\\{\\textbf{x}&#39;\\hat{\\boldsymbol{\\beta}}\\}}{1+\\exp\\{\\textbf{x}&#39;\\hat{\\boldsymbol{\\beta}\\}}} \\] La varianza de los parámetros estimados se calcula a partir de la siguiente expresión: \\[ var\\left(\\boldsymbol{\\hat{B}}\\right)=\\boldsymbol{J}^{-1}var\\left(S\\left(\\hat{\\boldsymbol{B}}\\right)\\right)\\boldsymbol{J}^{-1} \\] En donde \\[ S\\left(B\\right)=\\sum_{h}\\sum_{a}\\sum_{i}w_{hai}\\boldsymbol{D}_{hai}^{t}\\left[\\left(\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)\\left(1-\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)\\right]^{-1}\\left(y_{hai}-\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)=0 \\] y, \\[ D_{hai} = \\frac{\\delta\\left(\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)}{\\delta B_{j}} \\] Donde \\(j=0,\\dots,p\\). Dado que el modelo tiene enlace logaritmo, para construir los intervalos de confianza se debe aplicar el función exponencial a cada parámetro, \\[ \\hat{\\psi}=\\exp\\left(\\hat{B}_{1}\\right) \\] Por ende, el intervalo de confianza estará dado por la siguiente expresión: \\[ CI\\left(\\psi\\right)=\\exp\\left(\\hat{B}_{j}\\pm t_{df,1-\\frac{\\alpha}{2}}se\\left(\\hat{B}_{j}\\right)\\right) \\] Es importante anotar que la interpretación de los coeficientes de la regresión logística debido a lo no-linealidad que la caracteriza por momentos puede ser complicada, razón por la cual como primera medida se expondrá las similitudes con una regresión lineal clásica y también sus diferencias principales, todo esto, con el fin de llegar a una correcta interpretación de los modelos. Una de las similitudes entre los modelos lineales y los modelos logísticos, es que es posible interpretar los signos de la ecuación estimada de la misma manera en los dos casos, lo que quiere decir que el signo de la pendiente indica la relación de la variable frente a la probabilidad de ocurrencia del evento que explique la variable dependiente, así que, un signo positivo acompañando la covariable indica un aumento de la probabilidad de ocurrencia del evento al cumplir con las características de la covariable; y en caso contrario, un signo negativo indica la disminución de la probabilidad de ocurrencia del evento observado al cumplir con las características de la covariable. Por otro lado, al igual que con la regresión lineal, el intercepto solo se puede interpretar asumiendo valores cero para los otros predictores. Ahora, la interpretación de los coeficientes de regresión entre un modelo logístico y un modelo lineal es bastante diferente, lo que significa que, aunque es posible interpretar los signos de los coeficientes no se interpreta la magnitud de manera directa, ya que la estimación de los coeficientes en la regresión logística corresponde a un logaritmo de probabilidades por lo que es necesaria la transformación previamente señalada. Según Gelman y Hill (2019), los coeficientes de regresión logística exponencidos se pueden interpretar como razones de Odds. Luego, si dos resultados presentan las probabilidades de \\((\\pi,1-\\pi)\\), entonces \\(\\pi/(1-\\pi)\\) es llamado el Odds. De este modo, un Odds de 1, es equivalente a una probabilidad de 0.5, es decir, resultados igualmente probables. Ahora bien, Odds de 0.5 representa probabilidades de (1/3, 2/3). Dividiendo dos Odds, \\(\\pi_1/(1-\\pi_1 ) /\\pi_2/(1-\\pi_2 )\\) se obtiene una razón de probabilidades. Por ejemplo, una razón de Odds igual a 2 correspondería a un cambio de \\(\\pi=0.33\\) a \\(\\pi=0.5\\) o un cambio de \\(\\pi=0.5\\) a \\(\\pi=0.67\\). Una ventaja de trabajar con razones de Odds en lugar de probabilidades, es que es posible escalar de manera constante las razones de probabilidades indefinidamente sin llegar a los puntos límite de 0 y 1. Por ejemplo, pasar de un Odds de 2 a un Odds de 4 aumenta la probabilidad de 2/3 a 4/5; duplicar los Odss, nuevamente aumenta la probabilidad a 8/9, y así sucesivamente. A continuación, se muestra el ajuste de un modelo logístico para la pobreza, que se considera el fenómeno de interés en este ejemplo. Las covariables que interesa relacionar son el sexo la zona y región de ubicación de la vivienda, la edad, la edad al cuadrado, el gasto y el estaod de desempleo. Para llevar a cabo la implementación de este modelo en R, se utiliza la función svyglm que tiene en cuenta el diseño muestral complejo: mod_logistic &lt;- svyglm( formula = Pobreza ~ Sex + Zone + Region + Age + Age2 + Expenditure + Desempleo, family = binomial, design = diseno ) tidy(mod_logistic) term estimate std.error statistic p.value (Intercept) 1.6780 0.4193 4.0016 0.0001 SexMale -0.2088 0.1222 -1.7090 0.0903 ZoneUrban 0.2983 0.2677 1.1144 0.2676 RegionSur 0.0782 0.3407 0.2296 0.8189 RegionCentro 0.6580 0.4513 1.4579 0.1477 RegionOccidente 0.1846 0.3399 0.5432 0.5881 RegionOriente 0.2880 0.4748 0.6065 0.5454 Age -0.0248 0.0133 -1.8717 0.0639 Age2 0.0001 0.0002 0.8587 0.3924 Expenditure -0.0063 0.0009 -7.1081 0.0000 Desempleo 1.2159 0.3379 3.5983 0.0005 La función tidy muestra que algunas de las covariables son significativas al 5%. En particular, el intercepto, el gasto y el estado de desempleo. A continuación, se presentan los intervalos de confianza en los cuales se pueden concluir sobre la significancia de los parámetros al revisar si el cero se encuentra dentro del intervalo: confint(mod_logistic, level = 0.95) 2.5 % 97.5 % (Intercept) 0.8469 2.5090 SexMale -0.4510 0.0334 ZoneUrban -0.2323 0.8289 RegionSur -0.5971 0.7536 RegionCentro -0.2365 1.5525 RegionOccidente -0.4891 0.8584 RegionOriente -0.6531 1.2291 Age -0.0511 0.0015 Age2 -0.0002 0.0005 Expenditure -0.0080 -0.0045 Desempleo 0.5461 1.8856 Para verificar de manera gráfica la distribución de los parámetros del modelo, se realizará un gráfico de estos usando la función plot_summs como se muestra a continuación, library(ggstance) plot_summs(mod_logistic, scale = TRUE, plot.distributions = TRUE) Se puede observar en el gráfico que el número cero se encuentra dentro del intervalo de confianza de algunos parámetros, lo que confirma la no significancia de estos. Para conocer si una variable es significativa en el modelo es común utilizar el estadístico de Wald que se basa en la razón de verosimilitudes. En este caso se contrastan el modelo con todos los parámetros (completo) con el modelo reducido, es decir, el modelo con menos parámetros (reducido). La estadística de prueba es la siguiente: \\[ G=-2\\ln\\left[\\frac{L\\left(\\hat{\\boldsymbol{\\beta}}\\right)_{reducido}}{L\\left(\\hat{\\boldsymbol{\\beta}}\\right)_{completo}}\\right] \\] El estadístico de Wald para cada una de las variables del modelo del ejemplo se calcula a continuación con la función regTermTest para las variables del modelo: regTermTest(model = mod_logistic, ~ 1) ## Wald test for ## in svyglm(formula = Pobreza ~ Sex + Zone + Region + Age + Age2 + ## Expenditure + Desempleo, design = diseno, family = binomial) ## F = 16.01 on 1 and 109 df: p= 0.00011 regTermTest(model = mod_logistic, ~ Sex) ## Wald test for Sex ## in svyglm(formula = Pobreza ~ Sex + Zone + Region + Age + Age2 + ## Expenditure + Desempleo, design = diseno, family = binomial) ## F = 2.921 on 1 and 109 df: p= 0.09 regTermTest(model = mod_logistic, ~ Zone) ## Wald test for Zone ## in svyglm(formula = Pobreza ~ Sex + Zone + Region + Age + Age2 + ## Expenditure + Desempleo, design = diseno, family = binomial) ## F = 1.242 on 1 and 109 df: p= 0.27 regTermTest(model = mod_logistic, ~ Region) ## Wald test for Region ## in svyglm(formula = Pobreza ~ Sex + Zone + Region + Age + Age2 + ## Expenditure + Desempleo, design = diseno, family = binomial) ## F = 0.5838 on 4 and 109 df: p= 0.68 regTermTest(model = mod_logistic, ~ Age) ## Wald test for Age ## in svyglm(formula = Pobreza ~ Sex + Zone + Region + Age + Age2 + ## Expenditure + Desempleo, design = diseno, family = binomial) ## F = 3.503 on 1 and 109 df: p= 0.064 regTermTest(model = mod_logistic, ~ Age2) ## Wald test for Age2 ## in svyglm(formula = Pobreza ~ Sex + Zone + Region + Age + Age2 + ## Expenditure + Desempleo, design = diseno, family = binomial) ## F = 0.7373 on 1 and 109 df: p= 0.39 regTermTest(model = mod_logistic, ~ Expenditure) ## Wald test for Expenditure ## in svyglm(formula = Pobreza ~ Sex + Zone + Region + Age + Age2 + ## Expenditure + Desempleo, design = diseno, family = binomial) ## F = 50.52 on 1 and 109 df: p= 1.3e-10 regTermTest(model = mod_logistic, ~ Desempleo) ## Wald test for Desempleo ## in svyglm(formula = Pobreza ~ Sex + Zone + Region + Age + Age2 + ## Expenditure + Desempleo, design = diseno, family = binomial) ## F = 12.95 on 1 and 109 df: p= 0.00048 Concluyendo una vez más que las únicas variables significativas son el intercepto, el gasto y estado de desempleo, justo como se había mencionado anteriormente. Como es tradicional en el ajuste de modelos de regresión ya sea, clásico o generalizado, se pueden realizar ajustes con interacciones. A continuación, se presenta un ejemplo de cómo se ajustan modelos logísticos con la interacción entre sexo y zona y, sexo y región: mod_logistic_int &lt;- svyglm( formula = Pobreza ~ Sex + Zone + Region + Age + Age2 + Expenditure + Desempleo + Sex:Zone + Sex:Region, family = binomial, design = diseno ) tidy(mod_logistic_int) %&gt;% arrange(p.value) term estimate std.error statistic p.value Expenditure -0.0063 0.0009 -7.1574 0.0000 (Intercept) 1.6756 0.3886 4.3123 0.0000 Desempleo 1.1854 0.3356 3.5326 0.0006 Age -0.0253 0.0136 -1.8664 0.0648 RegionCentro 0.6942 0.4376 1.5863 0.1157 ZoneUrban 0.2981 0.2518 1.1838 0.2392 SexMale:RegionSur 0.3697 0.3531 1.0469 0.2976 Age2 0.0001 0.0002 0.8601 0.3917 RegionOriente 0.3285 0.3936 0.8347 0.4058 RegionOccidente 0.2810 0.3597 0.7811 0.4365 SexMale -0.1875 0.2412 -0.7771 0.4389 SexMale:RegionOccidente -0.2372 0.3194 -0.7427 0.4593 RegionSur -0.1013 0.3273 -0.3096 0.7575 SexMale:RegionCentro -0.0915 0.3148 -0.2907 0.7718 SexMale:RegionOriente -0.0885 0.4137 -0.2139 0.8311 SexMale:ZoneUrban 0.0289 0.2350 0.1230 0.9024 Observando que ninguna de las interacciones tampoco es significativa en el modelo. El gráfico de la distribución de los parámetros del modelo con y sin interacciones se presenta a continuación: plot_summs(mod_logistic_int, mod_logistic, scale = TRUE, plot.distributions = TRUE) El estadístico de Wald sobre los parámetros de interacción del modelo se rpesentan a continuación: regTermTest(model = mod_logistic_int, ~ Sex:Zone) ## Wald test for Sex:Zone ## in svyglm(formula = Pobreza ~ Sex + Zone + Region + Age + Age2 + ## Expenditure + Desempleo + Sex:Zone + Sex:Region, design = diseno, ## family = binomial) ## F = 0.01512 on 1 and 104 df: p= 0.9 regTermTest(model = mod_logistic_int, ~ Sex:Region) ## Wald test for Sex:Region ## in svyglm(formula = Pobreza ~ Sex + Zone + Region + Age + Age2 + ## Expenditure + Desempleo + Sex:Zone + Sex:Region, design = diseno, ## family = binomial) ## F = 0.9353 on 4 and 104 df: p= 0.45 References "],["modelo-log-lineal-para-tablas-de-contingencia.html", "8.2 Modelo log-lineal para tablas de contingencia", " 8.2 Modelo log-lineal para tablas de contingencia Cuando se quieren analizar las relaciones de las variables que generan totales estimados en en una tabla de contingencia (arreglo rectangular que organiza los datos en función de dos o más variables categóricas, mostrando la frecuencia o proporción de casos que caen en cada combinación de categorías) es posible usar un modelo log-lineal. Este tipo de modelos se utiliza para estudiar la asociación entre variables categóricas, controlando o teniendo en cuenta posibles efectos de otras covariables. Además, permite ajustar las asociaciones observadas en la tabla de contingencia y evaluar si estas asociaciones son estadísticamente significativas. Los modelos loglineales en tablas de contingencia se pueden usar para determinar si hay una asociación significativa entre las variables categóricas; ajustar la asociación entre variables de interés por otras variables que podrían influir en la relación; evaluar cómo cambia la probabilidad de una categoría en una variable categórica dado un cambio en otra variable categórica; estimar la probabilidad de que un caso caiga en una categoría específica de una variable categórica dada la información de otras variables. El término log-lineal básicamente describe el papel de la función de enlace que se utiliza en los modelos lineales generalizados. En el caso más simple, con dos variables categóricas, que inducen datos provenientes de estimaciones de conteos o proporciones en tablas de contingencia, es posible plantear el siguiente modelo estadístico: \\[ \\ln(\\pi_{ijk}) = \\mu + \\lambda_i^X + \\lambda_j^Y + \\lambda_{ij}^{XY} , \\] En donde \\(\\pi_{ijk}\\) es la probabilidad condicional de ocurrencia de la combinación de categorías i, j en las variables categóricas \\(X\\) e \\(Y\\), respectivamente; \\(\\mu\\) es el intercepto que representa el logaritmo de la probabilidad de referencia; \\(\\lambda_i^X\\) y \\(\\lambda_j^Y\\) son los efectos principales asociados con las categorías i y j en las variables \\(X\\) e \\(Y\\), respectivamente; finalmente, \\(\\lambda_{ij}^{XY}\\) es el efecto de interacción entre las categorías i y j en las variables categóricas. La función logaritmo natural se utiliza comúnmente en estos modelos para transformar las probabilidades condicionales y permitir la interpretación en términos de log-odds. En resumen, el modelo describe cómo las probabilidades condicionales de eventos categóricos están relacionadas entre sí y cómo estas relaciones pueden ser influenciadas por efectos principales y de interacción en las variables categóricas \\(X\\) e \\(Y\\). Como se ha definido en secciones y capítulos anteriores, con variables dicotómicas se pueden generar tablas de frecuencias teniendo en cuenta los factores de expansión del diseño. En R se hace usando la función svyby de la siguiente manera. Primero, se define la variable a la que se le requiere hacer la tabla (formula), luego se le indica cuál es la variable clasificadora (by). En este caso se quiere hacer una tabla de pobreza claificada por sexo. En tercer lugar se define la función que se quiere aplicar (FUN), en este caso, se quieren calcular totales por celda, por último, se define el diseño de muestreo (design). N_pobreza_sexo &lt;- svyby( formula = ~ factor(Pobreza), by = ~ Sex, FUN = svytotal, design = diseno, se = F, na.rm = T, ci = T, keep.var = TRUE ) N_pobreza_sexo Sex factor(Pobreza)0 factor(Pobreza)1 se.factor(Pobreza)0 se.factor(Pobreza)1 Female Female 650.8 356.1 33.63 36.39 Male Male 576.9 284.0 39.90 34.72 Al hacer uso de la función svyby pero usando en el argumento FUN= svymean es posible estimar proporciones como se muestra a continuación: p_pobreza_sexo &lt;- svyby( formula = ~ factor(Pobreza), by = ~ Sex, FUN = svymean, design = diseno, se = F, na.rm = T, ci = T, keep.var = TRUE ) p_pobreza_sexo Sex factor(Pobreza)0 factor(Pobreza)1 se.factor(Pobreza)0 se.factor(Pobreza)1 Female Female 0.6463 0.3537 0.0311 0.0311 Male Male 0.6701 0.3299 0.0377 0.0377 El modelo log-lineal en R se ajusta utilizando la función svyloglin para ajustar un modelo loglineal a datos de encuestas complejas utilizando el diseño de muestreo especificado en diseno. Esta función pertenece al paquete survey en R y sus argumentos son formula en donde se especifica la expresión del modelo. En este caso, el modelo incluye la variable Pobreza, la variable Sex, y la interacción entre ambas (Pobreza:Sex). La interacción indica que el efecto de Pobreza puede depender de la variable Sex y viceversa. La función svyloglin tiene en cuenta el diseño de muestreo complejo para proporcionar estimaciones más precisas y adecuadas a la estructura de la encuesta. mod1 &lt;- svyloglin(formula = ~ Pobreza + Sex + Pobreza:Sex, design = diseno) summary(mod1) ## Loglinear model: svyloglin(formula = ~Pobreza + Sex + Pobreza:Sex, design = diseno) ## coef se p ## Pobreza1 0.32790 0.07207 5.372e-06 ## Sex1 0.08664 0.02065 2.729e-05 ## Pobreza1:Sex1 -0.02640 0.02753 3.375e-01 En la salida anterior se puede observar que el estado de pobreza es independiente del sexo, como se ha mostrado en las secciones anteriores. Ahora bien, puesto que en la salida anterior se pudo observar que la interacción es no significativa, entonces, se ajusta ahora el modelo sin interacción: mod2 &lt;- svyloglin(formula = ~ Pobreza + Sex, design = diseno) summary(mod2) ## Loglinear model: svyloglin(formula = ~Pobreza + Sex, design = diseno) ## coef se p ## Pobreza1 0.32562 0.07122 4.834e-06 ## Sex1 0.07828 0.01890 3.460e-05 Por último, mediante un análisis de varianza es posible comparar los dos modelos como sigue: anova(mod2, mod1) ## Analysis of Deviance Table ## Model 1: y ~ Pobreza + Sex ## Model 2: y ~ Pobreza + Sex + Pobreza:Sex ## Deviance= 1.171 p= 0.3391 ## Score= 1.169 p= 0.3394 De la anterior salida se puede concluir que la interacción no es significativa en el modelo log-lineal ajustado y que no hay evidencia para preferir un modelo sobre otro. Dado que los valores p son mayores que el nivel de significancia común de 0.05 (p=0.361 y p=0.3613), no hay evidencia suficiente para rechazar la hipótesis nula de que no hay diferencia significativa entre los dos modelos en términos de devianza. Esto sugiere que la inclusión de la interacción Pobreza:Sex en el segundo modelo no mejora significativamente el ajuste en comparación con el primer modelo. Este estadístico se aplica cuando ya se ha elegido un modelo estadístico ( regresión lineal simple, regresión logística, entre otros).El estadístico de prueba de Wald \\(\\chi^{2}\\) para la hipótesis nula de independencia de filas y columnas en una tabla de doble entrada se define de la siguiente manera: \\[ Q_{wald}=\\hat{\\boldsymbol{Y}^{t}}\\left(\\boldsymbol{H}\\hat{\\boldsymbol{V}}\\left(\\hat{\\boldsymbol{N}}\\right)\\boldsymbol{H}^{t}\\right)^{-1}\\hat{\\boldsymbol{Y}} \\] donde, \\[ \\hat{\\boldsymbol{Y}}=\\left(\\hat{N}-E\\right) \\] es un vector de \\(R\\times C\\) de diferencias entre los recuentos de celdas observadas y esperadas, esto es, \\(\\hat{N}_{rc}-E_{rc}\\). La matriz \\(\\boldsymbol{H}\\hat{\\boldsymbol{V}}\\left(\\hat{\\boldsymbol{N}}\\right)\\boldsymbol{H}^{t}\\), representa la matriz de varianza y covarianza estimada para el vector de diferencias. En el caso de un diseño de muestra complejo, la matriz de varianza-covarianza de los conteos de frecuencia ponderada, \\(\\hat{V}\\left(\\hat{N}\\right)\\), se estima utilizando métodos de remuestreo o aproximación de Taylor. La matriz \\(\\boldsymbol{H}\\) es la inversa de la matriz \\(\\boldsymbol{J}\\) dada por: \\[ \\boldsymbol{J}=-\\left[\\frac{\\delta^{2}\\ln PL\\left(\\boldsymbol{B}\\right)}{\\delta^{2}\\boldsymbol{B}}\\right] \\mid \\boldsymbol{B}=\\hat{\\boldsymbol{B}} \\] Bajo la hipótesis nula de independencia, el estadístico de wald se distribuye chi cuadrado con \\(\\left(R-1\\right)\\times\\left(C-1\\right)\\) grados de libertad, \\[ Q_{wald}\\sim\\chi_{\\left(R-1\\right)\\times\\left(C-1\\right)}^{2} \\] La transformación F del estadístico de Wald es: \\[ F_{wald}=Q_{wald}\\times\\frac{df-\\left(R-1\\right)\\left(C-1\\right)+1}{\\left(R-1\\right)\\left(C-1\\right)df}\\sim F_{\\left(R-1\\right)\\left(C-1\\right),df-\\left(R-1\\right)\\left(C-1\\right)+1} \\] En R, para calcular el estadístico de Wald se hace similarmente al cálculo de los estadísticos anteriores usando la función summary como sigue: summary(mod1, statistic = &quot;Wald&quot;) ## Loglinear model: svyloglin(formula = ~Pobreza + Sex + Pobreza:Sex, design = diseno) ## coef se p ## Pobreza1 0.32790 0.07207 5.372e-06 ## Sex1 0.08664 0.02065 2.729e-05 ## Pobreza1:Sex1 -0.02640 0.02753 3.375e-01 Se puede concluir que no hay relación entre el estado de pobreza y el sexo. El coeficiente de la interacción es -0.02640, con un error estándar de 0.02753. El valor p asociado es 0.3375, que es mayor que el nivel de significancia común de 0.05. Esto sugiere que no hay evidencia significativa de una interacción entre la pobreza y el sexo. En este mismo sentido, el estadístico de Wald ajustado en R se se calcula similarmente al anterior y los resultados fueron similares: summary(mod1, statistic = &quot;adjWald&quot;) ## Loglinear model: svyloglin(formula = ~Pobreza + Sex + Pobreza:Sex, design = diseno) ## coef se p ## Pobreza1 0.32790 0.07207 5.372e-06 ## Sex1 0.08664 0.02065 2.729e-05 ## Pobreza1:Sex1 -0.02640 0.02753 3.375e-01 "],["modelo-multinomial-para-variables-con-múltiples-categorías.html", "8.3 Modelo multinomial para variables con múltiples categorías", " 8.3 Modelo multinomial para variables con múltiples categorías En las secciones anteriores se explicó cómo hacer un análisis de variables aleatorias provenientes de encuestas complejas de tipo binario usando modelos de regresión logística. Sin embargo, es muy común que en estas encuestas existan variables de tipo multinomial; es decir, que tengan más de dos niveles de respuesta. Para el análisis de este tipo de variables se utiliza un modelo multinomial, el cual se desarrollará en esta sección. El modelo de regresión logístico multinomial es la extensión natural del modelo de regresión logístico binomial y se utiliza para analizar variables con respuestas que tienen tres o más categorías distintas. Esta técnica es más apropiada para variables de encuestas con categorías de respuesta nominales. Para el ajuste de un modelo multinomial se debe tener en cuenta que la variable dependiente debe medirse en el nivel nominal; se debe tener una o más variables independientes que pueden ser continuas o categóricas; la variable dependiente debe tener categorías mutuamente excluyentes y exhaustivas; no debe haber dos o más variables independientes que están altamente correlacionadas entre sí; ademas, debe haber una relación lineal entre cualquier variable independiente continua y la transformación logit de la variable dependiente. El modelo múltinomial está dado por la siguiente expresión: \\[ Pr\\left(Y_{ik}\\right)=Pr\\left(y_{i}=k\\mid\\boldsymbol{X}_i\\right)=\\frac{\\exp\\left(\\beta_{0k}+\\boldsymbol{X}_{i}&#39;\\boldsymbol{\\beta}_{k}\\right)}{\\sum_{j=1}^{m}\\exp\\left(\\beta_{0j}+\\boldsymbol{\\boldsymbol{X}_{i}&#39;\\beta}_{j}\\right)} \\] En donde \\(Pr\\left(y_{i}=k\\mid\\boldsymbol{X}_i\\right)\\) representa la probabilidad condicional de que la observación \\(i\\) pertenezca a la categoría \\(k\\) de la variable dependiente \\(Y\\), dado el conjunto de covariables \\(\\boldsymbol{X}_{i}\\); \\(\\boldsymbol{\\beta}_k\\) es el vector de coeficientes de regresión para las variables \\(\\boldsymbol{X}\\) en la \\(k\\)-ésima categoría. Para la estimación de los parámetros del modelo logístico es posible usar técnicas ajustadas de máxima pseudoverosimilitud, lo que implica maximizar la siguiente versión multinomial de la función de pseudoverosimilitud (Heeringa, West, y Berglund 2017): \\[ PL_{Mult}\\left(\\hat{\\beta}\\mid\\boldsymbol{X}\\right) = \\prod_{i=1}^{n}\\left\\{ \\prod_{i=1}^{k}\\hat{\\pi}_{k}\\left(x_{i}\\right)^{y_{i\\left(k\\right)}}\\right\\} ^{w_{i}} \\] En la anterior expresión, se tiene que \\(y_{i\\left(k\\right)}=1\\) si \\(y = k\\) para la unidad muestreada \\(i\\), 0 en caso contrario. \\(\\hat{\\pi}_{k}\\left(x_{i}\\right)\\) es la probabilidad estimada de que \\(y_{i}=k\\mid x_{i}\\). \\(w_{i}\\) es el peso de la encuesta para la unidad muestreada \\(i\\). David A. Binder (1983) afirma que la maximización implica la aplicación del algoritmo de Newton-Raphson para resolver el siguiente conjunto de ecuaciones de estimación \\(\\left(K-1\\right)\\times\\left(p+1\\right)\\), suponiendo un diseño de muestra complejo con estratos indexados por \\(h\\) y conglomerados dentro de estratos indexados por \\(\\alpha\\): \\[ S\\left(\\boldsymbol{\\beta}\\right)_{Mult} = \\sum_{h}\\sum_{\\alpha}\\sum_{i}\\omega_{h\\alpha i}\\left(y_{h\\alpha i}^{\\left(k\\right)}-\\pi_{k}\\left(\\boldsymbol{\\beta}\\right)\\right)x&#39;_{h\\alpha i}=0 \\] Donde, \\[ \\pi_{k}\\left(\\boldsymbol{\\beta}\\right)=\\frac{exp\\left(\\boldsymbol{x}&#39;\\boldsymbol{\\beta_{k}}\\right)}{1+{ \\sum_{k=2}^{K}exp\\left(\\boldsymbol{x}&#39;\\boldsymbol{\\beta_{k}}\\right)}} \\] y \\[ \\pi_{1\\left(referencia\\right)}\\left(\\boldsymbol{\\beta}\\right)=1-\\sum_{k=2}^{K}\\pi_{k}\\left(\\boldsymbol{\\beta}\\right) \\] Por último, la matriz de varianzas de los parámetros estimados basada en la aplicación de David A. Binder (1983) de la linealización de Taylor a las estimaciones derivadas utilizando la estimación de pseudomáxima verosimilitud es: \\[ \\hat{Var}\\left(\\hat{\\boldsymbol{\\beta}}\\right) = \\left(J^{-1}\\right)var\\left[S\\left(\\hat{\\boldsymbol{\\beta}}\\right)\\right]\\left(J^{-1}\\right) \\] Para ejemplificar el uso de los modelos multinomiales en encuestas de hogares utilizando R se utilizan los siguientes códigos computacionales. Inicialemente, para efectos de comparación, estimemos el porcentaje de personas por desempleo: diseno %&gt;% group_by(Employment) %&gt;% summarise(Prop = survey_mean(vartype = c(&quot;se&quot;, &quot;ci&quot;))) Employment Prop Prop_se Prop_low Prop_upp Unemployed 0.0427 0.0073 0.0283 0.0571 Inactive 0.3841 0.0153 0.3538 0.4143 Employed 0.5732 0.0140 0.5454 0.6010 Ahora bien, la estimación del modelo multinomial se realiza con la función svy_vglm del paquete svyVGAM como se muestra a continuación: library(svyVGAM) model_mul &lt;- svy_vglm( formula = Employment ~ Age + Sex + Region + Zone, design = diseno, crit = &quot;coef&quot;, family = multinomial(refLevel = &quot;Unemployed&quot;) ) summary(model_mul) ## svy_vglm.survey.design(formula = Employment ~ Age + Sex + Region + ## Zone, design = diseno, crit = &quot;coef&quot;, family = multinomial(refLevel = &quot;Unemployed&quot;)) ## Stratified 1 - level Cluster Sampling design (with replacement) ## With (238) clusters. ## Called via srvyr ## Sampling variables: ## - ids: PSU ## - strata: Stratum ## - weights: qw ## Data variables: HHID (chr), Stratum (chr), NIh (int), nIh (dbl), dI (dbl), ## PersonID (chr), PSU (chr), Zone (chr), Sex (chr), Age (int), MaritalST (fct), ## Income (dbl), Expenditure (dbl), Employment (fct), Poverty (fct), dki (dbl), ## dk (dbl), wk (dbl), Region (fct), CatAge (ord), Age2 (I&lt;dbl&gt;), qw (dbl), ## Pobreza (dbl), Desempleo (dbl) ## Coef SE z p ## (Intercept):1 2.41129 0.78316 3.08 0.00208 ## (Intercept):2 2.21128 0.63743 3.47 0.00052 ## Age:1 0.02160 0.01066 2.03 0.04275 ## Age:2 0.01823 0.00905 2.01 0.04406 ## SexMale:1 -2.22316 0.32064 -6.93 4.1e-12 ## SexMale:2 -0.59506 0.27908 -2.13 0.03299 ## RegionSur:1 -0.43199 0.71209 -0.61 0.54409 ## RegionSur:2 -0.27702 0.57377 -0.48 0.62923 ## RegionCentro:1 0.36911 0.63111 0.58 0.55864 ## RegionCentro:2 0.25684 0.54089 0.47 0.63490 ## RegionOccidente:1 0.25319 0.63483 0.40 0.69002 ## RegionOccidente:2 0.09176 0.51607 0.18 0.85888 ## RegionOriente:1 0.61915 0.67153 0.92 0.35653 ## RegionOriente:2 0.47226 0.61531 0.77 0.44277 ## ZoneUrban:1 -0.22478 0.41830 -0.54 0.59102 ## ZoneUrban:2 0.05425 0.37106 0.15 0.88376 La anterior salida muestra los coeficientes estimados para cada variable independiente en el modelo, así como sus errores estándar y valores p asociados. Los valores de los interceptos para las clases Employed e Inactive del resultado multinomial indican el valor logarítmico de la proporción de probabilidades de pertenecer a la clase correspondiente en comparación con la clase de referencia que es Unemployed. Luego, se muestran los coeficientes asociados con la variable Age para las clases Employed e Inactive, que indican cómo el logaritmo de la proporción de probabilidades de pertenecer a una clase en comparación con la clase de referencia cambia por una unidad de cambio en la edad. A continuación, vienen los coeficientes asociados con la variable binaria ‘SexMale’ para las clases Employed e Inactive, que indican cómo el logaritmo de la proporción de probabilidades de pertenecer a una clase en comparación con la clase de referencia cambia cuando la variable ‘SexMale’ cambia de categoría. El resto de coeficientes en la salida se interpreta análogamente. Las columnas SE, z y p proporcionan información sobre la significancia estadística de cada coeficiente. El valor p indica si el coeficiente es significativamente diferente de cero. En este caso, muchos de los coeficientes tienen valores de p bajos, lo que sugiere que son estadísticamente significativos. References "],["modelo-gamma-para-variables-continuas-positivas.html", "8.4 Modelo Gamma para variables continuas positivas", " 8.4 Modelo Gamma para variables continuas positivas Finalmente esta sección tratará sobre modelos para variables continuas que toman valores positivo. En particular, los modelos de regresión gamma constituyen una herramienta valiosa cuando se aborda la modelización de variables con distribuciones asimétricas y con heterocedasticidad, características comunes en datos relacionados con procesos de duración, costos y fenómenos de supervivencia. La regresión gamma se adapta de manera eficaz a situaciones donde la varianza no es constante y la relación entre las variables predictoras y la variable respuesta sigue una distribución gamma. La distribución gamma se caracteriza por tener una forma paramétrica que incluye un parámetro de forma y un parámetro de escala. Para garantizar que la variable de respuesta sea positiva, el vínculo inverso se introduce utilizando la función inversa multiplicativa. Este enfoque asegura que la predicción del modelo sea siempre positiva, ya que la distribución gamma se utiliza a menudo para modelar variables que representan valores, costos, duraciones o tiempos hasta eventos, y estas no pueden ser negativas. La elección del vínculo inverso en un modelo de regresión gamma es esencial para garantizar la coherencia con la naturaleza de los datos y para lograr resultados interpretables y significativos en la práctica. Por ende, la función de enlace \\(g(\\cdot)\\) para el MLG con una variable dependiente distribuida por un modelo Gamma es \\(\\frac{1}{\\mu_{i}}\\). Eso significa que el valor esperado de \\(y_i\\) observado, (\\(E(y_i) = \\mu_i\\)), está relacionado con sus variables de entrada como sigue: \\[ \\mu_{i} = \\frac{1}{\\boldsymbol{x}_i&#39;\\boldsymbol{\\beta}} \\] En R, es posible utilizar la función svyglm del paquete survey para ajustar un modelo de regresión gamma a datos de encuestas de hogares. En particular, suponga que la variable de interés es Income y que se quiere relacionar con las covariables Age, Sex, Region, Zone y Desempleo. Nótese que el argumento family especifica la distribución de la variable dependiente y la función de enlace a utilizar. En este caso, se está utilizando una distribución gamma y la función de enlace inverso. La notación Gamma(link = \"inverse\") indica que se está modelando la variable dependiente siguiendo una distribución gamma con una función de enlace inverso. modelo &lt;- svyglm( formula = Income ~ Age + Sex + Region + Zone + Desempleo, design = diseno, family = Gamma(link = &quot;inverse&quot;) ) broom::tidy(modelo) %&gt;% arrange(p.value) term estimate std.error statistic p.value (Intercept) 0.0021 2e-04 8.6649 0.0000 ZoneUrban -0.0009 2e-04 -4.6608 0.0000 Desempleo 0.0010 2e-04 3.8719 0.0002 Age 0.0000 0e+00 2.8444 0.0053 SexMale -0.0001 1e-04 -2.7589 0.0068 RegionOccidente 0.0002 3e-04 0.7849 0.4342 RegionSur -0.0001 3e-04 -0.4153 0.6787 RegionCentro 0.0001 2e-04 0.2335 0.8158 RegionOriente 0.0000 3e-04 -0.1131 0.9102 De la tabla anterior, el intercepto, la zona urbana, el estado de desempleo, la edad y el sexo masculino resultaron significativos para explicar el ingreso con los datos de la encuesta. Una vez estimado los coeficientes, se estiman los intervalos de confianza para la predicción como sigue: pred_IC &lt;- data.frame(confint(predict(modelo, type = &quot;response&quot;, se = T))) colnames(pred_IC) &lt;- c(&quot;Lim_Inf&quot;, &quot;Lim_Sup&quot;) pred_IC %&gt;% slice(1:6L) Lim_Inf Lim_Sup 361.4 523.1 349.6 502.1 364.6 547.4 385.0 587.0 347.0 496.1 365.0 549.0 "],["modelos-multinivel.html", "Capítulo 9 Modelos multinivel", " Capítulo 9 Modelos multinivel Los modelos multinivel, también conocidos como modelos de efectos mixtos o modelos jerárquicos, son una técnica estadística utilizada en el análisis de datos de encuestas de hogares para incorporar una estructura jerárquica o multinivel. En estas encuestas, los datos se recopilan a nivel individual (por ejemplo, sobre la edad, el género y la educación de cada miembro del hogar) y a nivel del hogar (por ejemplo, sobre el ingreso del hogar, la propiedad de la vivienda y la ubicación geográfica). Además, permiten analizar cómo los factores a nivel del hogar y a nivel individual influyen en las respuestas a las preguntas de la encuesta. Por ejemplo, un modelo multinivel podría utilizarse para investigar cómo el ingreso del hogar y la edad de los miembros del hogar influyen en el consumo de alimentos saludables. En los modelos multinivel, se trabajan con dos tipos de efectos: los efectos aleatorios y los efectos fijos. Los efectos fijos representan las relaciones promedio entre las variables, mientras que los efectos aleatorios modelan la variación en estas relaciones entre los hogares. De esta manera, los modelos multinivel permiten tener en cuenta la heterogeneidad en la población y obtener estimaciones más precisas de las variables de interés. Por tanto, los modelos multinivel son una herramienta valiosa en el análisis de datos de encuestas de hogares al permitir analizar cómo los factores a nivel del hogar y a nivel individual influyen en las respuestas a las preguntas de la encuesta y al tener en cuenta la estructura jerárquica de los datos. Algunas referencias bibliográficas relevantes sobre el uso de modelos multinivel en encuestas de hogares son Goldstein (2011), que es una referencia clásica para el análisis de datos multinivel, y aborda el uso de modelos jerárquicos en una variedad de contextos, incluyendo encuestas de hogares; Gelman y Hill (2019), que ofrece una introducción accesible a la teoría y la práctica de los modelos jerárquicos; Rabe-Hesketh y Skrondal (2012), el cual es una guía práctica para el análisis de datos multinivel y longitudinales utilizando el software estadístico Stata; Browne y Draper (2006) que compara los enfoques Bayesianos y frecuentistas basados en verosimilitud. Para iniciar este capítulo se cargan las librerías necesarias, la base de datos y el tema de la Cepal para realizar los gráficos: Cargue de librerías: knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, error = FALSE) options(digits = 4) options(tinytex.verbose = TRUE) library (survey) library(srvyr) library(convey) library(TeachingSampling) library(printr) library(stargazer) library(broom) library(jtools) library(modelsummary) library(patchwork) library(ggplot2) Cargue de la base de datos: encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) %&gt;% mutate(Age2 = I(Age ^ 2)) Creando el tema de la CEPAL para generar los gráficos en este capítulo: theme_cepal &lt;- function(...) theme_light(10) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), legend.position=&quot;bottom&quot;, legend.justification = &quot;left&quot;, legend.direction=&quot;horizontal&quot;, plot.title = element_text(size = 20, hjust = 0.5), ...) Para efectos de ejemplificar los conceptos que se presentarán en este capítulo, primero considere que la muestra tiene una naturaleza jerárquica, puesto que las viviendas fueron seleccionadas a partir de los estratos. Luego, el estrato será usado para analizar el comportamiento del fenómeno de interés. encuesta_plot &lt;- encuesta %&gt;% dplyr::select(HHID, Stratum) %&gt;% unique() %&gt;% group_by(Stratum) %&gt;% tally() %&gt;% arrange(desc(n)) %&gt;% dplyr::select(-n) %&gt;% slice(1:6L) %&gt;% inner_join(encuesta) %&gt;% filter(Expenditure &lt; 700) %&gt;% dplyr::select(Income, Expenditure, Stratum, Sex, Region, Zone) encuesta_plot %&gt;% slice(1:10L) Income Expenditure Stratum Sex Region Zone 697.3 296.1 idStrt017 Male Norte Rural 697.3 296.1 idStrt017 Female Norte Rural 697.3 296.1 idStrt017 Male Norte Rural 697.3 296.1 idStrt017 Female Norte Rural 526.8 294.8 idStrt017 Male Norte Rural 526.8 294.8 idStrt017 Female Norte Rural 526.8 294.8 idStrt017 Female Norte Rural 526.8 294.8 idStrt017 Male Norte Rural 526.8 294.8 idStrt017 Male Norte Rural 526.8 294.8 idStrt017 Female Norte Rural A modo introductorio, en el siguiente gráfico se ajusta un modelo lineal simple cuya variable de interés son los ingresos de los hogares con una sola variable explicativa correspondiente a los gastos de los hogares, sin considerar el efecto de los estratos en el diseño muestral. library(latex2exp) ggplot(data = encuesta_plot, aes(y = Income, x = Expenditure)) + geom_jitter() + theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) + geom_smooth(formula = y ~ x, method = &quot;lm&quot;, se = F) + ggtitle( latex2exp::TeX( &quot;$Ingreso_{i}\\\\sim\\\\hat{\\\\beta}_{0}+\\\\hat{\\\\beta}_{1}Gasto_{i}+\\\\epsilon_{i}$&quot; ) ) + theme_cepal() Como se ha explicado en capítulos anteriores, el modelo de regresión se basa en varios supuestos principales con respecto a la naturaleza de los datos en la población; más específicamente asume independencia de las observaciones. Naturalmente este supuesto no es válido, más aún cuando la selección de la muestra se hace de para cada estrato de muestreo y además, el comportamiento de los estratos muestrales es diferente entre ellos. Teniendo en cuenta lo anterior, se ajusta un modelo de regresión en donde el intercepto cambia de acuerdo con cada estrato. Por lo general, nos referimos a la estructura de datos descrita anteriormente como anidada, lo que significa que los puntos de datos individuales en un nivel (por ejemplo, la persona o el hogar) aparecen solo en un nivel de una variable de nivel superior, como el estrato de muestreo. Por lo tanto, las personas u hoagres están anidados dentro del estrato. References "],["modelo-con-intercepto-aleatorio.html", "9.1 Modelo con intercepto aleatorio", " 9.1 Modelo con intercepto aleatorio En el análisis de los modelos multinivel hay dos tipos de estimaciones que son relevantes. La primera asociada con los coeficientes de regresión, generalmente denominados como los parámetros fijos del modelo; la segunda con las estimaciones de la varianza, generalmente denominadas parámetros aleatorios del modelo. Cualquier análisis de regresión multinivel siempre debe comenzar con la estimación de la varianza de ambos niveles para la variable dependiente. El primer paso recomendado en el análisis de regresión multinivel consiste en una descomposición de la varianza de la variable dependiente en los diferentes niveles. Por ejemplo, asumiendo que la variable de interés es el ingreso de las personas y que existe una naturaleza jerárquica entre estas y el estrato, entonces la varianza del ingreso se puede descomponer en dos partes: la varianza dentro del estrato y la varianza entre los estratos. Estos dos componentes de varianza se pueden obtener en una regresión multinivel simple con un modelo nuelo dado por la siguiente expresión: \\[ y_{ij}=\\beta_{0j}+\\epsilon_{ij} \\] En donde, el intercepto depende de manera jerárquica de los estratos; así: \\[ \\beta_{0j}=\\gamma_{00}+\\tau_{0j} \\] En las anteriroes expresiones, \\(y_{ij}=\\) representa los ingresos de la persona \\(i\\) en el estrato \\(j\\); \\(\\beta_{0j}\\) es el intercepto en el estrato \\(j\\); \\(\\epsilon_{ij}\\) es el residual de la persona \\(i\\) en el estrato \\(j\\); \\(\\gamma_{00}\\) es el intercepto general y \\(\\tau_{0j}\\) es el efecto aleatorio para el intercepto. Para este modelo se asume que, \\[ \\tau_{0j}\\sim N\\left(0,\\sigma_{\\tau}^{2}\\right) \\] Además, \\[ \\epsilon_{ij}\\sim N\\left(0,\\sigma_{\\epsilon}^{2}\\right) \\]. Cai (2013) afirma que existe evidencia suficiente de que las ponderaciones de muestreo deben usarse en el modelado multinivel para obtener estimaciones insesgadas. Actualmente, diferentes autores recomiendan diferentes enfoques sobre cómo usar los pesos de muestreo en modelos jerárquicos. Por ejemplo, Pfeffermann et al. (1998) y Asparouhov (2006) aconsejan utilizar un enfoque de pseudomáxima verosimilitud para calcular estimaciones dentro y entre los diferentes niveles para obtener estimaciones insesgadas. A continuación se empezará a ejemplificar el ajuste de los modelos multinivel con encuestas complejas, iniciando con el ajuste de un modelo nulo. Para ajustar los modelos multinivel en R se usará la función lmer() de la librería lme4, usando para el ajuste la ponderación de Pfefferman, tal como se vio en los capítulos anteriores. modwk &lt;- lm(wk ~ Expenditure + Zone + Sex + Age2, data = encuesta) wkpred &lt;- predict(modwk) encuesta %&lt;&gt;% mutate(qw = wk / wkpred) El siguiente código en R utiliza la función lmer del paquete lme4 para ajustar un modelo multinivel nulo, en donde se quiere modelar el comportamiento de la variable Income en función de Stratum. El término (1 | Stratum) indica un efecto aleatorio para cada estrato de muestreo en la encuesta. Además, el argumento weights = qw especifica el vector de pesos de muestreo. library(lme4) mod_null &lt;- lmer(Income ~ (1 | Stratum), data = encuesta, weights = qw) El siguiente código devuelve los coeficientes de los efecto aleatorios para cada estrato en el modelo multinivel nulo ajustado. Estos coeficientes proporcionar información sobre cómo varían los ingresos para diferentes niveles de la variable categórica estrato head(coef(mod_null)$Stratum) (Intercept) idStrt001 639.4 idStrt002 508.4 idStrt003 485.3 idStrt004 963.4 idStrt005 520.1 idStrt006 440.9 Un concepto de interés en este tipo de análisis es la correlación intra-clásica (ICC, por sus siglas en inglés) que hace referencia a la proporción de la varianza total de una variable que se explica por las diferencias entre los grupos o niveles (estratos) en el modelo. En otras palabras, la ICC mide la similitud o correlación entre las observaciones dentro del mismo grupo o nivel en comparación con las observaciones de diferentes grupos. Esta cantidad se calcula como: \\[ \\rho=\\frac{\\sigma_{\\tau}^{2}}{\\sigma_{\\tau}^{2}+\\sigma_{\\epsilon}^{2}} \\] Una ICC alta indica que una gran proporción de la variación total de la variable se debe a las diferencias entre los grupos, lo que sugiere que los grupos son distintos entre sí y que los efectos de los grupos deben ser considerados en el modelo. Por otro lado, una ICC baja indica que la mayoría de la variación en la variable está dentro de los grupos y que los efectos de los grupos no son tan importantes para explicar la variabilidad en la variable. Para efectos de ejemplificar el cálculo de la correlación intraclases, se utiliza la función icc de la librería performance. El cálculo es el siguiente: performance::icc(mod_null) ICC_adjusted ICC_unadjusted optional 0.3366 0.3366 FALSE se puede observar que la correlación intraclase, utilizando el modelo nulo es de casi el 34%, porcentaje de varianza que se atribuye a la diferencia entre los estratos. Por otro lado, como el modelo que se está ajustando es el nulo, la predicción del ingreso para cualquier individuo en el mismo estrato será constante, como se muestra a continuación: (tab_pred &lt;- data.frame(Pred = predict(mod_null), Income = encuesta$Income, Stratum = encuesta$Stratum)) %&gt;% distinct() %&gt;% slice(1:6L) # Son las pendientes aleatorias Pred Income Stratum 1 639.4 409.87 idStrt001 6 639.4 823.75 idStrt001 10 639.4 90.92 idStrt001 13 639.4 135.33 idStrt001 18 639.4 336.19 idStrt001 22 639.4 1539.75 idStrt001 A continuación se muestra la estimación del ingreso en cada estrato de muestreo en relación con el ingreso. Se observa que la predicción en cada estrato es la misma (puntos del mismo color) y que existe una variación mayor a medida que el ingreso aumenta. ggplot(data = tab_pred, aes(x = Pred, y = Income, colour = Stratum)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = &quot;red&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) References "],["modelo-con-pendiente-aleatoria.html", "9.2 Modelo con pendiente aleatoria", " 9.2 Modelo con pendiente aleatoria Este tipo de modelos permiten que la relación entre la variable independiente y la variable dependiente cambie según alguna otra variable explicativa. En otras palabras, permite que la pendiente de la relación entre las variables sea diferente a medida que lo grupos o subconjuntos de datos. En un modelo de regresión lineal simple, la relación entre la variable independiente y la variable dependiente se modela como una línea recta con una pendiente fija. Sin embargo, en un modelo con pendiente aleatoria, se permite que la pendiente varíe según otra variable explicativa. En este tipo de modelos, la relación entre las variables puede suponer una curva con diferentes pendientes para diferentes subgrupos. Los modelos con pendiente aleatoria son útiles en situaciones donde se espera que la relación entre las variables cambie de manera no lineal o cuando se desea modelar diferencias en la pendiente entre subgrupos. Consideremos el siguiente modelo: \\[ Ingreso_{ij}=\\beta_{0}+\\beta_{1j}Gasto_{ij}+\\epsilon_{ij} \\] donde \\(\\beta_{1j}\\) esta dado como \\[ \\beta_{1j} = \\gamma_{10}+\\gamma_{11}Stratum_{j} + \\tau_{1j} \\] Para este caso en particular, la pendiente varía de acuerdo con los estratos de muestreo, no así el intercepto que seguirá fijo siempre. Para ajustar el modelo se utiliza la función lmer como se muestra a continuación: mod_Pend_Aleatoria &lt;- lmer(Income ~ 1 + (0 + Expenditure | Stratum), data = encuesta, weights = qw) Para cada estrato se tienen las siguientes estimaciones para las pendientes aleatorias \\(\\beta_{1j}\\): coef(mod_Pend_Aleatoria)$Stratum %&gt;% slice(1:8L) Expenditure (Intercept) idStrt001 1.8914 148.8 idStrt002 1.2666 148.8 idStrt003 1.1200 148.8 idStrt004 1.4717 148.8 idStrt005 0.8316 148.8 idStrt006 0.7942 148.8 idStrt007 0.8950 148.8 idStrt008 0.8070 148.8 Organizando los coeficientes en un gráfico se tiene: Coef_Estimado &lt;- inner_join( coef(mod_Pend_Aleatoria)$Stratum %&gt;% add_rownames(var = &quot;Stratum&quot;), encuesta_plot %&gt;% dplyr::select(Stratum) %&gt;% distinct() ) ggplot(data = encuesta_plot, aes(y = Income, x = Expenditure, colour = Stratum)) + geom_jitter() + theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) + geom_abline( data = Coef_Estimado, mapping = aes( slope = Expenditure, intercept = `(Intercept)`, colour = Stratum ) ) + theme_cepal() Se puede observar que la estimación de la pendiente varía de manera importante para cada uno de los estratos pero que, al imponer la restricción de un intercepto común para todos los estratos, no hay un buen ajuste en general. Por otro lado, la estimación de los ingresos para las unidades observadas usando este modelo se muestra a continuación: data.frame( Pred = predict(mod_Pend_Aleatoria), Income = encuesta$Income, Stratum = encuesta$Stratum ) %&gt;% distinct() %&gt;% slice(1:6L) Pred Income Stratum 1 803.8 409.87 idStrt001 6 890.6 823.75 idStrt001 10 288.8 90.92 idStrt001 13 307.2 135.33 idStrt001 18 640.8 336.19 idStrt001 22 1159.6 1539.75 idStrt001 Gráficamente se muestran las estimaciones versus los valores estimados de los ingresos y se logra observar que la predicción está más cerca a la línea de 45 grados que el modelo anterior. ggplot(data = tab_pred, aes(x = Pred, y = Income, colour = Stratum)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = &quot;red&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) "],["modelo-con-intercepto-y-pendiente-aleatoria.html", "9.3 Modelo con intercepto y pendiente aleatoria", " 9.3 Modelo con intercepto y pendiente aleatoria Los modelos con intercepto y pendiente aleatoria son un tipo de modelo estadístico que permite modelar la relación entre una variable de respuesta y una o más variables predictoras, teniendo en cuenta tanto efectos fijos como efectos aleatorios en donde el intercepto y la pendiente varían según el subgrupo de interés. En estos modelos, los coeficientes de la regresión (es decir, la pendiente y el intercepto) se consideran aleatorios en lugar de fijos. Esto significa que se asume que estos coeficientes pueden variar entre las unidades de análisis, que pueden ser individuos, grupos, regiones geográficas, etc. Estas variaciones se modelan como efectos aleatorios que se incorporan en la ecuación de regresión. Siguiendo con la encuesta de hogares, consideremos el siguiente modelo: \\[ Ingreso_{ij}=\\beta_{0j}+\\beta_{1j}Gasto_{ij}+\\epsilon_{ij} \\] En donde, \\[ \\beta_{0j} = \\gamma_{00}+\\gamma_{01}Stratum_{j} + \\tau_{0j} \\] Y, \\[ \\beta_{1j} = \\gamma_{10}+\\gamma_{11}Stratum_{j} + \\tau_{1j} \\] El ajuste del modelo se realiza utilizando la función lmer como se presenta a continuación: mod_IntPend_Aleatoria &lt;- lmer(Income ~ Expenditure + (1 + Expenditure | Stratum), data = encuesta, weights = qw) Los coeficientes del modelo son: coef(mod_IntPend_Aleatoria)$Stratum %&gt;% slice(1:10L) (Intercept) Expenditure idStrt001 -229.29 2.7555 idStrt002 36.19 1.6039 idStrt003 151.78 1.1637 idStrt004 219.82 1.3600 idStrt005 -87.91 1.2818 idStrt006 29.15 1.2178 idStrt007 40.63 1.0783 idStrt008 164.18 0.9288 idStrt009 20.03 0.8187 idStrt010 91.05 1.8226 Gráficamente, Coef_Estimado &lt;- inner_join( coef(mod_IntPend_Aleatoria)$Stratum %&gt;% add_rownames(var = &quot;Stratum&quot;), encuesta_plot %&gt;% dplyr::select(Stratum) %&gt;% distinct() ) ggplot(data = encuesta_plot, aes(y = Income, x = Expenditure, colour = Stratum)) + geom_jitter() + theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) + geom_abline( data = Coef_Estimado, mapping = aes( slope = Expenditure, intercept = `(Intercept)`, colour = Stratum ) ) + theme_cepal() Como se pudo observar en la gráfica anterior, el ajuste del modelo con intercepto y pendiente aleatoria se ajusta mejor a los datos que los otros dos modelos mostrados anteriormente. A continuación, se realizan las predicciones de los ingresos con el modelo: data.frame( Pred = predict(mod_IntPend_Aleatoria), Income = encuesta$Income, Stratum = encuesta$Stratum ) %&gt;% distinct() %&gt;% slice(1:6L) Pred Income Stratum 1 725.059 409.87 idStrt001 6 851.538 823.75 idStrt001 10 -25.189 90.92 idStrt001 13 1.594 135.33 idStrt001 18 487.643 336.19 idStrt001 22 1243.348 1539.75 idStrt001 Para poder ver qué tan buena son las predicciones, se realiza el siguiente gráfico: ggplot(data = tab_pred, aes(x = Pred, y = Income, colour = Stratum)) + geom_point() + geom_abline(intercept = 0, slope = 1, colour = &quot;red&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) Ahora bien, para robustecer el modelo, se ajusta nuevamente, pero agregando la variable zona como se muestra a continuación: \\[ Ingreso_{ij}=\\beta_{0j}+\\beta_{1j}Gasto_{ij}+\\beta_{2j}Zona_{ij} +\\epsilon_{ij} \\] Donde, \\[ \\beta_{0j} = \\gamma_{00}+\\gamma_{01}Stratum_{j} + \\gamma_{02}\\mu_{j} + \\tau_{0j} \\] Además, \\[ \\beta_{1j} = \\gamma_{10}+\\gamma_{11}Stratum_{j} + \\gamma_{12}\\mu_{j} + \\tau_{1j} \\] Y, \\[ \\beta_{2j} = \\gamma_{20}+\\gamma_{21}Stratum_{j} + \\gamma_{12}\\mu_{j} + \\tau_{2j} \\] donde \\(\\mu_{j}\\) es el gasto medio de los hogares en el estrato \\(j\\). En R el ajuste se hace de la siguiente manera: media_estrato &lt;- encuesta %&gt;% group_by(Stratum) %&gt;% summarise(mu = mean(Expenditure)) encuesta &lt;- inner_join(encuesta, media_estrato, by = &quot;Stratum&quot;) mod_IntPend_Aleatoria2 &lt;- lmer( Income ~ 1 + Expenditure + Zone + mu + (1 + Expenditure + Zone + mu | Stratum), data = encuesta, weights = qw ) calculando las predicciones de los ingresos de los hogares por estrato: data.frame( Pred = predict(mod_IntPend_Aleatoria2), Income = encuesta$Income, Stratum = encuesta$Stratum ) %&gt;% distinct() %&gt;% slice(1:6L) Pred Income Stratum 1 723.19 409.87 idStrt001 6 847.74 823.75 idStrt001 10 -15.60 90.92 idStrt001 13 10.78 135.33 idStrt001 18 489.40 336.19 idStrt001 22 1233.57 1539.75 idStrt001 "],["modelo-logístico-multinivel..html", "9.4 Modelo logístico multinivel.", " 9.4 Modelo logístico multinivel. Los modelos logísticos multinivel son una extensión de los modelos logísticos simples, que se utilizan para predecir la probabilidad de un resultado binario en función de una o varias variables explicativas. Sin embargo, en muchas situaciones, los datos se recogen de individuos que están agrupados en diferentes niveles o unidades de análisis, como escuelas, ciudades o países. En estos casos, los modelos logísticos simples pueden no ser suficientes para capturar la estructura jerárquica de los datos y la variación en las respuestas entre los diferentes grupos. Estos modelos resuelven este problema al permitir que los coeficientes del modelo varíen a través de los diferentes niveles de análisis. Además, los modelos logísticos multinivel permiten incluir tanto variables a nivel individual como variables a nivel de grupo, lo que aumenta la precisión de las estimaciones y la capacidad de explicar la variabilidad en las respuestas. También permiten estimar la varianza en las respuestas entre los diferentes grupos, lo que es útil para identificar las fuentes de variabilidad y para comparar la variabilidad entre grupos. Sea la variable \\(y_{ij} = 1\\) si el individuo \\(i\\) en el estrato \\(j\\) está por encima de la línea de pobreza y \\(y_{ij} = 0\\) en caso contrario, la variable \\(y_{ij}\\) se puede modelar mediante el modelo logístico: \\[ \\pi_{ij}=Pr\\left(y_{ij}\\right)=Pr\\left(y_{ij}=1\\mid \\boldsymbol{X}, \\boldsymbol{\\beta}\\right)=\\frac{\\exp\\left(\\boldsymbol{\\beta}_{j}\\boldsymbol{x}_{ij}\\right)}{1+\\exp\\left(\\boldsymbol{\\beta}_{j}\\boldsymbol{x}_{ij}\\right)} \\] Ó también, \\[ \\log\\left(\\frac{\\pi_{ij}}{1-\\pi_{ij}}\\right)=\\boldsymbol{\\beta}_{j}\\boldsymbol{x}_{ij} \\] Los modelos logísticos con intercepto y pendiente aleatoria son un tipo de modelo logístico multinivel que permiten que tanto el intercepto como la pendiente varíen aleatoriamente entre los diferentes grupos de observación. En los modelos logísticos básicos, la relación entre las variables predictoras y la variable de respuesta se modela mediante una función logística, donde la respuesta es la probabilidad de que el resultado binario ocurra. En los modelos con intercepto y pendiente aleatoria, la función logística se ajusta para cada grupo de observación, y tanto el intercepto como la pendiente son variables aleatorias que varían de un grupo a otro. Esto permite que los coeficientes del modelo, que representan la relación entre las variables predictoras y la respuesta, varíen según el grupo de observación. Por ejemplo, asuma que se quiere modelar la pobreza en términos del gasto y que se considera que el estrato es importante en la variación de este fenómeno. El modelo se define de la siguiente manera: \\[ logit(Pobreza_{ij})=\\beta_{0j}+\\beta_{1j}Gasto_{ij}+\\epsilon_{ij} \\] En donde, \\[ \\beta_{0j} = \\gamma_{00}+\\gamma_{01}Stratum_{j} + \\tau_{0j} \\] Además, \\[ \\beta_{1j} = \\gamma_{10}+\\gamma_{11}Stratum_{j} + \\tau_{1j} \\] En R, el ajuste se hace de la siguiente manera: encuesta &lt;- encuesta %&gt;% mutate(pobreza = ifelse(Poverty != &quot;NotPoor&quot;, 1, 0)) mod_logit_Pen_Aleatorio &lt;- glmer( pobreza ~ Expenditure + (1 + Expenditure | Stratum), data = encuesta, weights = qw, binomial(link = &quot;logit&quot;) ) Los coeficientes del modelo son: coef(mod_logit_Pen_Aleatorio)$Stratum %&gt;% slice(1:10L) (Intercept) Expenditure idStrt001 4.9568 -0.0256 idStrt002 9.7610 -0.0351 idStrt003 -1.1263 -0.0068 idStrt004 1.9918 -0.0158 idStrt005 8.1255 -0.0264 idStrt006 -1.1729 0.0089 idStrt007 0.9893 -0.0121 idStrt008 1.4810 -0.0057 idStrt009 3.6139 -0.0043 idStrt010 4.1611 -0.0206 Gráficamente el ajuste de los modelo se muestra a continuación: dat_pred &lt;- encuesta %&gt;% group_by(Stratum) %&gt;% summarise(Expenditure = list(seq(min(Expenditure), max(Expenditure), len = 100))) %&gt;% tidyr::unnest_legacy() dat_pred &lt;- mutate(dat_pred, Proba = predict(mod_logit_Pen_Aleatorio, newdata = dat_pred , type = &quot;response&quot;)) ggplot(data = dat_pred, aes(y = Proba, x = Expenditure, colour = Stratum)) + geom_line() + theme_bw() + geom_point(data = encuesta, aes(y = pobreza, x = Expenditure)) + theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) Las predicciones se muestran a continuación: (tab_pred &lt;- data.frame( Pred = predict(mod_logit_Pen_Aleatorio, type = &quot;response&quot;), pobreza = encuesta$pobreza, Stratum = encuesta$Stratum, qw = encuesta$qw)) %&gt;% distinct() %&gt;% slice(1:6L) Pred pobreza Stratum qw 0.0194 0 idStrt001 0.6193 0.0194 0 idStrt001 0.6058 0.0194 0 idStrt001 0.6089 0.0194 0 idStrt001 0.6242 0.0194 0 idStrt001 0.6096 0.0061 0 idStrt001 0.6079 La calidad de la predicción del modelo es muy buena como se muestra a continuación: tab_pred %&gt;% summarise(Pred = weighted.mean(Pred, qw), pobreza = weighted.mean(pobreza,qw)) Pred pobreza 0.3881 0.3895 "],["imputación-múltiple-en-encuestas-de-hogares.html", "Capítulo 10 Imputación múltiple en encuestas de hogares", " Capítulo 10 Imputación múltiple en encuestas de hogares La ausencia de respuesta es un fenómeno normal y común en las encuestas de hogares, más aún después de la pandemia por Covid-19. Las razones para la aparición de este fenómeno son múltiples; por ejemplo, una longitud extensa en los cuestionarios de las encuestas, la renuencia de los respondientes a entregar información sensible, por desastres naturales o violencia en los territorios que no permiten acceder a las áreas seleccionadas, entre muchas otras. Si la ausencia de respuesta en alguna variable de la encuesta es muy alta, puede poner en riesgo la calidad de las estimaciones que se obtienen utilizando los estimadores clásicos. El objetivo principal de este capítulo es abordar el problema de la falta de datos en encuestas de hogares, revisar las posibles causas, el impacto que tiene en la estimación de los indicadores, y mostrar algunas soluciones a la falta de datos en las encuesta. En este sentido, alguno de los avances en la teoría de los métodos en el contexto de muestras complejas son la imputación múltiple Raghunathan (2016) y la imputación fraccional Kim y Shao (2014). References "],["tres-patrones-para-la-ausencia-de-respuesta.html", "10.1 Tres patrones para la ausencia de respuesta", " 10.1 Tres patrones para la ausencia de respuesta Siguiendo las ideas anteriores, sea \\(\\boldsymbol{X}_{n \\times p} = x_{ij}\\) una matriz completa (sin valores perdidos) de tal forma que \\(X_{ij}\\) es el valor de la variable \\(j\\) con \\(j=1, \\dots, p\\) e \\(i\\) con \\(i=1, \\dots, n\\). Adicionalmente, se define \\(\\boldsymbol{M}_{n \\times p} = m_{ij}\\) una matriz indicadoradonde \\(m_{ij} = 1\\) si el valor de \\(x_{ij}\\) es un dato perdido y \\(m_{ij}=0\\) si \\(x_{ij}\\) está presente. Ahora bien, note que la matriz \\(M\\) describe un patrón de datos faltantes que la media marginal de columna puede ser interpretada como la probabilidad de que \\(x_{ij}\\) sea faltante A continuación, se describen alguna de las particularidades de la matriz \\(\\boldsymbol{M}_{n \\times p}\\): La matriz \\(\\boldsymbol{M}_{n \\times p}\\) presenta un comportamiento completamente al azar (MCAR, por sus siglas en inglés) si la probabilidad de respuesta es completamente independiente de las variables observadas y de las no observadas. En este caso, el mecanismo de respuesta es ignorable tanto para inferencias basadas en muestreo como en máxima verosimilitud. La matriz \\(\\boldsymbol{M}_{n \\times p}\\) presenta un comportamiento al azar (MAR, por sus siglas en inglés) si la probabilidad de respuesta es completamente independiente de las variables no observadas y no de las observadas. En este caso, el mecanismo de respuesta se considera ignorable para inferencias basadas en máxima verosimilitud. La matriz \\(\\boldsymbol{M}_{n \\times p}\\) presenta un comportamiento no al azar (MNAR, por sus siglas en inglés) si la probabilidad de respuesta no es completamente independiente de las variables no observadas y posiblemente también de las observadas. El mecanismo de respuesta es no ignorable. En las dos figuras siguientes, se ilustran los casos de observaciones perdidas de manera aleatoria y con un patrón identificado: Como se ha venido trabajando en los capítulos anteriores, primero carguemos la base de datos con la muestra seleccionada y con el fin de poder ejemplificar el tratamiento de datos faltantes, se incluirán manualmente valores perdidos. En este sentido, la lectura de la base se hará a continuación: encuesta &lt;- readRDS(&quot;Data/encuesta.rds&quot;) En primera instancia, se filtran encuestados mayores a 15 años y se calcula la proporción de la población desempleada, inactiva y empleada antes de generar los valores faltantes, ademas de cargar todas las librerías que se utilizarán en este capítulo: library (survey) library(srvyr) library(convey) library(TeachingSampling) library(printr) library(stargazer) library(broom) library(jtools) library(modelsummary) library(patchwork) library(ggplot2) encuesta &lt;- encuesta |&gt; filter(Age &gt;= 15) (tab_antes &lt;- prop.table(table(encuesta$Employment))) Unemployed Inactive Employed 0.0409792 0.373603 0.5854178 También se calcula el promedio de ingresos en la muestra: (med_antes &lt;- mean(encuesta$Income, na.rm = TRUE)) ## [1] 604.2494 Luego de los conteos anteriores, se genera un 20% de valores faltantes siguiendo un esquema MCAR. En R, la función sample_frac es parte del paquete dplyr y se utiliza para seleccionar una fracción específica de filas de un conjunto de datos; esta función es útil cuando se desea obtener una muestra aleatoria de un porcentaje específico de observaciones. set.seed(1234) encuesta_MCAR &lt;- sample_frac(encuesta, 0.8) dat_plot &lt;- bind_rows(list(encuesta_MCAR = encuesta_MCAR, encuesta = encuesta), .id = &quot;Caso&quot;) Ahora bien, para poder ver el efecto de la inclusión de datos faltantes de manera gráfica por zona y sexo para la variable ingreso, se realizan las siguientes gráficas: p1 &lt;- ggplot(dat_plot, aes(x = Zone, y = Income)) + geom_boxplot() + facet_grid(. ~ Caso) + theme_bw() + geom_hline(yintercept = mean(encuesta$Income), col = &quot;red&quot;) p2 &lt;- ggplot(dat_plot, aes(x = Sex, y = Income)) + geom_boxplot() + facet_grid(. ~ Caso) + theme_bw() + geom_hline(yintercept = mean(encuesta$Income), col = &quot;red&quot;) library(patchwork) p1 | p2 Como se puede observar en las gráficas anteriores, la distribución de los ingresos por Zona y Sexo se mantiene similar con o sin presencia de los datos faltantes. Esto se debe a que la no respuesta que se incluyó no depende de la variable de estudio. Ahora bien, analizando la variable de interés se observa que tampoco hay cambios distribucionales notables entre las distribuciones con y sin datos faltantes por sexo, como se puede ver a continuación: p1 &lt;- ggplot(dat_plot, aes(x = Income, fill = Caso)) + geom_density(alpha = 0.3) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) p2 &lt;- ggplot(dat_plot, aes(x = Income, fill = Caso)) + geom_density(alpha = 0.3) + facet_grid(. ~ Sex) + theme_bw() + geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) + theme(legend.position = &quot;none&quot;) (p1 / p2) Si graficamos ahora la variable gastos, se observan los mismos resultados que para ingresos. p1 &lt;- ggplot(dat_plot, aes(x = Expenditure, fill = Caso)) + geom_density(alpha = 0.3) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Expenditure), col = &quot;red&quot;) p2 &lt;- ggplot(dat_plot, aes(x = Expenditure, fill = Caso)) + geom_density(alpha = 0.3) + facet_grid(. ~ Sex) + theme_bw() + geom_vline(xintercept = mean(encuesta$Expenditure), col = &quot;red&quot;) + theme(legend.position = &quot;none&quot;) (p1 / p2) Por otro lado, simulemos ahora una pérdida de información al azar (MAR) que dependa de la Zona y del Sexo, como sigue: library(TeachingSampling) set.seed(1234) temp_estrato &lt;- paste0(encuesta$Zone, encuesta$Sex) table(temp_estrato) RuralFemale RuralMale UrbanFemale UrbanMale 481 428 531 439 sel &lt;- S.STSI( S = temp_estrato, Nh = c(481, 428, 531, 439), nh = c(20, 380, 20, 280) ) encuesta_MAR &lt;- encuesta[-sel, ] dat_plot2 &lt;- bind_rows(list(encuesta_MAR = encuesta_MAR, encuesta = encuesta), .id = &quot;Caso&quot;) El código anterior utiliza la librería TeachingSampling para realizar un muestreo aleatorio en cada cruce. Primero, se establece la semilla aleatoria en 1234 para asegurarse de que los resultados sean reproducibles. A continuación, se crea una variable llamada temp_estrato que combina dos variables de la encuesta Zone y Sex utilizando la función paste0 para crear grupos de estratos. La función table se usa para mostrar la frecuencia de cada estrato. Luego, se realiza el muestreo estratificado utilizando la función S.STSI cuyos argumentos son S, el vector de estratos creado anteriormente; Nh, el número de unidades en cada estrato (en este caso, 469, 411, 510 y 390); y nh, el tamaño de muestra deseado para cada estrato (en este caso, 20, 380, 20 y 280). El resultado del muestreo estratificado es un vector de índices de fila que corresponden a las observaciones seleccionadas para la muestra. Luego, se crea un nuevo conjunto de datos llamado encuesta_MAR que excluye las observaciones seleccionadas en la muestra. Finalmente, se usa la función bind_rows del paquete dplyr para unir los dos conjuntos de datos (encuesta y encuesta_MAR) en un solo conjunto de datos llamado dat_plot2, con una nueva variable llamada Caso, que indica el caso de cada observación en el conjunto de datos. Observemos gráficamente el efecto de la perdida de información en una encuesta en un esquema MAR: p1 &lt;- ggplot(dat_plot2, aes(x = Caso, y = Expenditure)) + geom_hline(yintercept = mean(encuesta$Expenditure), col = &quot;red&quot;) + geom_boxplot() + facet_grid(Zone ~ Sex) + theme_bw() p1 En el gráfico anterior se logra observar un cambio en la distribución de los datos en las distintas desagregaciones cuando en la encuesta no se tiene pérdida de información y cuando sí se tiene con un esquema MAR. Naturalmente, esto afectaría en las estimaciones finales que se hagan de los parámetros estudiados. Con mayor claridad, se puede ver el cambio distribucional en la siguiente gráfica: p1 &lt;- ggplot(dat_plot2, aes(x = Income, fill = Caso)) + geom_density(alpha = 0.3) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) p2 &lt;- ggplot(dat_plot2, aes(x = Income, fill = Caso)) + facet_grid(. ~ Sex) + geom_density(alpha = 0.3) + theme_bw() + theme(legend.position = &quot;none&quot;) + geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) p1 / p2 Este comportamiento es natural que suceda en un esquema MAR de datos faltantes puesto que la probabilidad de que los datos estén ausentes está relacionada con los valores observados en otras variables del conjunto de datos. La ventaja que tienen los mecanismos MCAR y MAR es que se puede predecir el valor de los datos faltantes utilizando la información de otras variables disponibles en el conjunto de datos. Esto puede mejorar la calidad de los resultados de los análisis y evitar la necesidad de descartar observaciones con datos faltantes. Otra gráfica en donde se evidencia el cambio de distribución de los gastos entre hombres y mujeres. p1 &lt;- ggplot(dat_plot2, aes(x = Expenditure, fill = Caso)) + geom_density(alpha = 0.3) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Expenditure), col = &quot;red&quot;) p2 &lt;- ggplot(dat_plot2, aes(x = Expenditure, fill = Caso)) + facet_grid(. ~ Sex) + geom_density(alpha = 0.3) + theme_bw() + theme(legend.position = &quot;none&quot;) + geom_vline(xintercept = mean(encuesta$Expenditure), col = &quot;red&quot;) p1 / p2 Para seguir con la ejemplificación de los esquemas de datos faltantes, generaremos un esquema de pérdida de información en una encuesta NMAR. Como se mencionó al inicio de este capítulo, en este tipo de esquema la probabilidad de que un dato falte está relacionada con el propio valor de ese dato. En otras palabras, en un esquema NMAR, la probabilidad de que falte un dato no es independiente del valor de ese dato, sino que está influenciada por algún factor que puede estar relacionado con el fenómeno que se está estudiando. Esto puede llevar a que los datos faltantes introduzcan un sesgo apreciable en la inferencia y análisis estadístico de las encuestas de hogares, lo que hace que el manejo adecuado de los datos faltantes en este tipo de esquemas sea particularmente importante en la investigación. encuesta_MNAR &lt;- encuesta %&gt;% arrange(Income) %&gt;% slice(1:1300L) dat_plot3 &lt;- bind_rows(list(encuesta_MNAR = encuesta_MNAR, encuesta = encuesta), .id = &quot;Caso&quot;) El código anterior tiene como objetivo crear un nuevo conjunto de datos llamado encuesta_MNAR que contiene las primeras 1300 observaciones del conjunto de datos original encuesta, ordenadas por la variable Income. Evidentemente, la perdida de respuesta no es al azar, e implica que las personas con mayores ingresos no están respondiendo a la encuesta. Posteriormente, el código une el conjunto de datos original encuesta con el conjunto de datos encuesta_MNAR usando la función bind_rows, y crea una nueva variable llamada Caso que indica la fuente de los datos. Ahora bien, para ver el efecto que tiene en una encuesta el tener datos faltante con esquema NMAR, se ilustran los siguientes gráficos: p1 &lt;- ggplot(dat_plot3, aes(x = Income, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_vline(xintercept = mean(encuesta_MNAR$Income), col = &quot;blue&quot;) p1 Como se puede observar en la gráfica anterior, la distribución de los ingresos cambia notablemente cuando se tienen datos faltantes con esquema NMAR, lo mismo sucede con la variable gastos, como se puede observar en la siguiente gráfica: p1 &lt;- ggplot(dat_plot3, aes(x = Expenditure, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Expenditure), col = &quot;red&quot;) + geom_vline(xintercept = mean(encuesta_MNAR$Expenditure), col = &quot;blue&quot;) p1 Para ver más al detalle el impacto que tiene la no respuesta con un esquema NMAR, a continuación se muestra una gráfica del ingreso discriminada por sexo y por zona. También se nota un cambio en la distribución de los ingresos significativos. p1 &lt;- ggplot(dat_plot3, aes(x = Caso, y = Income)) + geom_hline(yintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_boxplot() + facet_grid(Zone ~ Sex) + theme_bw() p1 "],["imputación-por-la-media-no-condicional..html", "10.2 Imputación por la media no condicional.", " 10.2 Imputación por la media no condicional. La imputación por la media no condicional consiste en reemplazar los valores faltantes con la media aritmética de la variable completa, sin tener en cuenta ninguna otra variable. Es decir, la media se calcula a partir de todos los valores disponibles en la variable en cuestión, independientemente de las características de los demás datos. Para efectos de ejemplificar la solución del problema a los datos faltantes en una encuesta de hogares con un patrón MCAR generemos la siguiente base de datos: encuesta &lt;- full_join( encuesta, encuesta_MCAR %&gt;% dplyr::select(HHID, PersonID, Income, Employment) %&gt;% mutate( Income_missing = Income, Employment_missin = Employment, Employment = NULL, Income = NULL ) ) El código anterior utiliza la función full_join de la librería dplyr de R para combinar los dos conjuntos de datos. La combinación se realiza mediante la unión completa, que devuelve todas las filas de ambas tablas, uniendo las filas con valores coincidentes y rellenando con valores faltantes para las columnas que no tienen una coincidencia en ambas tablas. Ahora bien, para tener como referencia el porcentaje de datos faltantes, se ejecuta el siguiente comando: encuesta %&gt;% group_by(Zone) %&gt;% summarise(Income = sum(is.na(Income_missing) / n())) Zone Income Rural 0.2079208 Urban 0.1927835 encuesta %&gt;% group_by(Sex) %&gt;% summarise(Income = sum(is.na(Income_missing) / n())) Sex Income Female 0.1837945 Male 0.2191465 La imputación por la media no condicional es un método bastante simple y rápido, y puede ser útil en ciertas situaciones, especialmente cuando la variable en cuestión no tiene una distribución muy sesgada o cuando los valores faltantes son relativamente pocos en comparación con el tamaño de la muestra. Sin embargo, el método de imputación por la media no condicional también tiene limitaciones y puede no ser adecuado en todas las situaciones, especialmente cuando hay sesgos o patrones en los datos faltantes o cuando los datos están altamente correlacionados. Adicionalmente, este método no afecta el promedio, pero si afecta la variabilidad, el sesgo y los percentiles. A continuación, se ejemplifica con los datos de ejemplo este método: promedio &lt;- mean(encuesta$Income_missing, na.rm = TRUE) encuesta %&lt;&gt;% dplyr::mutate(Income_imp = ifelse(is.na(Income_missing), promedio, Income_missing)) sum(is.na(encuesta$Income_imp)) ## [1] 0 En el código anterior la imputación se realiza utilizando la media aritmética de los valores no faltantes en Income_missing y se almacena en una nueva variable llamada Income_imp. La primera línea del código calcula la media aritmética de los valores no faltantes en la columna Income_missing y la almacena en una variable llamada promedio. La última línea cuenta el número de valores faltantes en la nueva columna Income_imp de la base de datos encuesta. Como es natural, después de la imputación, no existen valores faltantes. dat_plot4 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone, Sex, Income, Income_imp), key = &quot;Caso&quot;, value = &quot;Income2&quot;, -Zone, -Sex ) p1 &lt;- ggplot(dat_plot4, aes(x = Income2, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_vline(xintercept = mean(encuesta$Income_imp), col = &quot;blue&quot;) p1 En la gráfica anterior se puede observar que la distribución real y la imputada para la variable de interés cambia de manera significativa; esto muestra que este método, para este conjunto de datos no es el más apropiado dado lo sesgado de la distribución de la variable ingresos. Un caso similar al anterior ocurre si graficamos la variable ingreso por zona y sexo. A continuación, se muestran los diagramas de cajas para revisar la distribución de los datos, arrojando nuevamente las conclusiones obtenidas con el gráfico anterior: p1 &lt;- ggplot(dat_plot4, aes(x = Caso, y = Income2)) + geom_hline(yintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_boxplot() + facet_grid(Zone ~ Sex) + theme_bw() p1 "],["imputación-por-la-media-condicional.html", "10.3 Imputación por la media condicional", " 10.3 Imputación por la media condicional El método de imputación por la media condicional es una técnica utilizada en el análisis de datos para tratar valores faltantes o perdidos en una variable numérica. A diferencia del método de imputación por la media no condicional, el método de imputación por la media condicional tiene en cuenta otras variables en el conjunto de datos. Esta técnica se basa en la idea de que la media de una variable puede variar en función de los valores de otras variables. Por lo tanto, en lugar de simplemente reemplazar los valores faltantes con la media aritmética de la variable completa, se utiliza la media de la variable para grupos de observaciones que tienen valores similares en otras variables. El método de imputación por la media condicional puede ser más preciso que el método de imputación por la media no condicional en situaciones en las que las variables están correlacionadas o cuando hay patrones de valores faltantes en los datos. A continuación, se ejemplifica la técnica de imputación utilizando la variable estrato para hacer el cálculo de los promedios por cada uno de los estratos y así poder imputar los datos faltantes, asumiendo que hay una relación directa entre los estratos y los ingresos de los hogares: encuesta %&lt;&gt;% group_by(Stratum) %&gt;% mutate(Income_imp = ifelse( is.na(Income_missing), mean(Income_missing, na.rm = TRUE), Income_missing )) %&gt;% data.frame() sum(is.na(encuesta$Income_imp)) ## [1] 0 encuesta %&lt;&gt;% mutate(Income_imp = ifelse(is.na(Income_imp), promedio, Income_imp)) sum(is.na(encuesta$Income_imp)) ## [1] 0 El anterior código utiliza la función group_by para agrupar las observaciones de la base de datos de la encuesta por los niveles de la variable Stratum. Luego, se asigna el valor imputado a la columna Income_imp en la base de datos encuesta. Si un valor en la columna Income_missinges faltante, se reemplaza con la media aritmética de los valores no faltantes en Income_missing dentro del estrato correspondiente. Si no es NA, se mantiene el valor original. A continuación, se calculan las medias y desviaciones estándar tanto para los datos imputados como los originales y así poder comparar le efecto de la imputación realizada: encuesta %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp) ) Income_ Income_sd Income_imp_ Income_imp_sd 604.2494 513.1078 611.545 488.7209 En este escenario, el sesgo relativo para el promedio de los ingresos es menor al 1.5%. Ahora bien, siguiendo la misma idea, el sesgo relativo para la desviación es cercano al 5%. Si se realiza la imputación utilizando la media condicional, agrupando por la variable zona, se tienen los siguientes resultados: encuesta %&gt;% group_by(Zone) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp) ) Zone Income_ Income_sd Income_imp_ Income_imp_sd Rural 469.1217 336.5861 477.9042 305.5101 Urban 730.8793 609.0304 736.7815 585.6550 Realizando el mismo ejercicio anterior, se obtienen sesgos relativos para la media de los ingresos para la zona rural de 1.87% y para la zona urbana de 0.8%. En ambos casos se observa una buena imputación de los ingresos. Ahora bien, para observar la distribución de los datos imputados en comparación con los no imputados se realizan las siguientes gráficas: dat_plot5 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone, Sex, Income, Income_imp), key = &quot;Caso&quot;, value = &quot;Income2&quot;, -Zone, -Sex ) p1 &lt;- ggplot(dat_plot5, aes(x = Income2, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_vline(xintercept = mean(encuesta$Income_imp), col = &quot;blue&quot;) p1 Se puede observar que, de manera general, la distribución de las observaciones imputadas y originales tienen un comportamiento mejor que con la media no condicional. Si se observa ahora la distribución de los datos por zona y sexo, se puede observar también una buena imputación de las observaciones. p1 &lt;- ggplot(dat_plot5, aes(x = Caso, y = Income2)) + geom_hline(yintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_boxplot() + facet_grid(Zone ~ Sex) + theme_bw() p1 "],["imputación-por-hot-deck.html", "10.4 Imputación por Hot-Deck", " 10.4 Imputación por Hot-Deck Esta imputación consiste en reemplazar los valores faltantes de una o más variables para un no respondiente (llamado receptor) con valores observados de un respondiente (el donante) similar al no respondiente con respecto a las características observadas en ambos casos. La técnica se basa en la idea de que las unidades similares pueden tener valores similares en las variables de interés. En este enfoque, se selecciona una observación donante que sea similar a la observación receptora en términos de características relevantes (por ejemplo, edad, género, ubicación geográfica, etc.), y se utiliza su valor observado para imputar el valor faltante en la observación receptora. El término Hot-Deck hace referencia a una tarjeta perforada que se utilizaba en los primeros sistemas informáticos para almacenar y recuperar datos. Esta es una técnica relativamente simple y eficaz para imputar valores faltantes en conjuntos de datos pequeños o medianos, y se utiliza comúnmente en encuestas y estudios de investigación social. Sin embargo, puede ser menos efectiva en conjuntos de datos grandes o complejos, donde puede ser difícil encontrar observaciones similares o donde las características relevantes son difíciles de definir o medir de manera confiable. A continuación, se presenta un código computacional que ejemplifica, para los datos que estamos usando en el capítulo, el uso del método. donante &lt;- which(!is.na(encuesta$Income_missing)) receptor &lt;- which(is.na(encuesta$Income_missing)) encuesta$Income_imp &lt;- encuesta$Income_missing set.seed(1234) for (ii in receptor) { don_ii &lt;- sample(x = donante, size = 1) encuesta$Income_imp[ii] &lt;- encuesta$Income_missing[don_ii] } sum(is.na(encuesta$Income_imp)) ## [1] 0 En el código mostrado anteriormente se describe a continuación, la primera línea del código selecciona las observaciones que no tienen valores faltantes en la variable Income_missing, la segunda línea selecciona las observaciones que tienen valores faltantes en la variable Income_missing; luego se crea una nueva variable Income_imp para almacenar los valores imputados. Finalmente, se utiliza un bucle for para iterar a través de cada observación receptora. Dentro del bucle, se utiliza la función sample para seleccionar una observación donante aleatoria de entre las observaciones que no tienen valores faltantes en la variable Income_missing. Una vez realizada la imputación, se calcula la media y la desviación de los datos completos e imputados: encuesta %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp) ) Income_ Income_sd Income_imp_ Income_imp_sd 604.2494 513.1078 618.2937 528.2157 Como en los métodos anteriores, el sesgo relativo de la imputación fue de 2.3%. Haciendo el mismo ejercicio, pero esta vez desagregada por zona tenemos: encuesta %&gt;% group_by(Zone) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp) ) Zone Income_ Income_sd Income_imp_ Income_imp_sd Rural 469.1217 336.5861 503.7127 368.9137 Urban 730.8793 609.0304 725.6691 623.9875 El sesgo relativo de la estimación en la zona rural es de 7.4% y en al zona urbana es de 0.7%. El mismo ejercicio se puede realizar por sexo: encuesta %&gt;% group_by(Sex) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp) ) Sex Income_ Income_sd Income_imp_ Income_imp_sd Female 589.2330 504.3041 602.8075 503.0951 Male 621.7771 522.9428 636.3699 555.8522 A continuación, se muestra la gráfica de la distribución de los datos, tanto los completos como los imputados, observándose que la distribución de los datos imputados es muy similar a la de los datos no imputados: dat_plot6 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone, Sex, Income, Income_imp), key = &quot;Caso&quot;, value = &quot;Income2&quot;,-Zone,-Sex ) p1 &lt;- ggplot(dat_plot6, aes(x = Income2, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_vline(xintercept = mean(encuesta$Income_imp), col = &quot;blue&quot;) p1 "],["imputación-por-regresión.html", "10.5 Imputación por regresión", " 10.5 Imputación por regresión La imputación por regresión se basa en la construcción de un modelo de regresión a partir de las variables auxiliares disponibles en el conjunto de datos, que se utiliza para predecir los valores faltantes de la variable de interés. Para llevar a cabo la imputación por regresión, se selecciona una variable objetivo que tenga valores faltantes y se identifican las variables predictoras que tienen una correlación significativa con la variable objetivo. Se ajusta un modelo de regresión utilizando las variables predictoras y la variable objetivo disponible, y se utilizan los coeficientes del modelo para predecir los valores faltantes de la variable objetivo. Para ejemplificar, imputemos la variable ingreso tomando como covariables las variable zona, sexo y estado de empleo; se utiliza un modelo de regresión lineal múltiple, como se muestra a continuación: encuesta$Income_imp &lt;- encuesta$Income_missing encuesta_obs &lt;- filter(encuesta,!is.na(Income_missing)) encuesta_no_obs &lt;- filter(encuesta, is.na(Income_missing)) mod &lt;- lm(Income ~ Zone + Sex + Expenditure, data = encuesta_obs) Una vez ajustado el modelo, se realiza el proceso de predicción como se muestra a continuación: imp &lt;- predict(mod, encuesta_no_obs) encuesta_no_obs$Income_imp &lt;- imp encuesta &lt;- bind_rows(encuesta_obs, encuesta_no_obs) Ahora, se hace el cálculo de la variable ingreso completa e imputada: encuesta %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp) ) Income_ Income_sd Income_imp_ Income_imp_sd 604.2494 513.1078 611.7477 498.3293 Teniendo un sesgo relativo de 1.2%. Haciendo el mismo ejercicio por zona tenemos: encuesta %&gt;% group_by(Zone) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp) ) Zone Income_ Income_sd Income_imp_ Income_imp_sd Rural 469.1217 336.5861 476.1361 317.3999 Urban 730.8793 609.0304 738.8311 594.5319 Por último, los ejercicios gráficos se realizan a continuación: dat_plot7 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone, Sex, Income, Income_imp), key = &quot;Caso&quot;, value = &quot;Income2&quot;,-Zone,-Sex ) p1 &lt;- ggplot(dat_plot7, aes(x = Income2, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_vline(xintercept = mean(encuesta$Income_imp), col = &quot;blue&quot;) p1 "],["imputación-por-el-vecino-más-cercano.html", "10.6 Imputación por el vecino más cercano", " 10.6 Imputación por el vecino más cercano En la imputación por el vecino más cercano, los valores faltantes se reemplazan por valores de otras observaciones que son similares a la observación con valores faltantes. La imputación por el vecino más cercano se basa en la idea de que los registros similares tienden a tener valores similares para una determinada variable. La técnica consiste en encontrar los \\(k\\) registros más similares a la observación con valores faltantes en función de las variables disponibles en el conjunto de datos y utilizar los valores de estas observaciones para estimar el valor faltante. Para calcular la similitud entre observaciones, se pueden utilizar diferentes medidas de distancia, como la distancia euclidiana o la distancia de Manhattan. La técnica también permite ajustar el valor de \\(k\\), que representa el número de vecinos más cercanos utilizados para estimar el valor faltante. Es importante destacar que la imputación por el vecino más cercano es una técnica relativamente simple y fácil de implementar. Sin embargo, su eficacia puede verse limitada por la cantidad y la calidad de los datos disponibles, así como por la elección de los parámetros (como el valor de \\(k\\) y la medida de distancia) que pueden afectar significativamente los resultados obtenidos. Por lo tanto, es importante evaluar cuidadosamente la calidad de los datos y los resultados obtenidos antes de utilizar esta técnica. Teniendo en cuenta lo anterior, se presentan 3 pasos a tener en cuenta al momento de utilizar esta técnica: Definir una magnitud de distancia (Distancia euclidiana, k-media, K-Medioides, entre otras). Para la \\(i\\)-ésimo elemento identificar el donante, cual será el más cercano al receptor según la magnitud de distancia previamente definida. Se imputa el valor faltante con la información del donante identificado previamente. Para ejemplificar esta metodología, se va a imputar la variable ingresos utilizando como variable de apoyo los gastos del individuo. Se utilizará como distancia la euclidiana. encuesta$Income_imp &lt;- encuesta$Income_missing encuesta_obs &lt;- filter(encuesta,!is.na(Income_missing)) encuesta_no_obs &lt;- filter(encuesta, is.na(Income_missing)) for (ii in 1:nrow(encuesta_no_obs)) { Expen_ii &lt;- encuesta_no_obs$Expenditure[[ii]] don_ii &lt;- which.min(abs(Expen_ii - encuesta_obs$Expenditure)) encuesta_no_obs$Income_imp[[ii]] &lt;- encuesta_obs$Income_missing[[don_ii]] } encuesta &lt;- bind_rows(encuesta_obs, encuesta_no_obs) Haciendo el cálculo del promedio de los ingresos para los datos completos y estimados se tienen los siguientes resultados: encuesta %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp) ) Income_ Income_sd Income_imp_ Income_imp_sd 604.2494 513.1078 610.505 513.6812 Se observa que hay una diferencia de 6 unidades monetarias entre el promedio real y el estimado. Realizando este mismo ejercicio por zona tenemos: encuesta %&gt;% group_by(Zone) %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp) ) Zone Income_ Income_sd Income_imp_ Income_imp_sd Rural 469.1217 336.5861 477.9160 344.1316 Urban 730.8793 609.0304 734.7559 607.0266 Obteniéndose diferencias de 7 unidades monetarias en el ingreso para la zona rural y de 4 para la urbana. Además, se puede observar que la distribución de los datos imputados son muy próximos que los datos reales. dat_plot8 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone, Sex, Income, Income_imp), key = &quot;Caso&quot;, value = &quot;Income2&quot;,-Zone,-Sex ) p1 &lt;- ggplot(dat_plot8, aes(x = Income2, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_vline(xintercept = mean(encuesta$Income_imp), col = &quot;blue&quot;) p1 "],["imputación-por-el-vecino-más-cercano-con-regresión.html", "10.7 Imputación por el vecino más cercano con regresión", " 10.7 Imputación por el vecino más cercano con regresión A continuación, se presentan los pasos que se deben tener en cuenta para realizar la imputación utilizando el vecino más cercano con regresión: Ajustar un modelo de regresión entre la variable de interés y las covariables auxiliares. Realizar la predicción de los valores observados y no observados. Comparar las predicciones obtenidas para los valores observados y no observados. Para la \\(i\\)-ésima observación, identificar el donante con la menor distancia entre los valores predichos al receptor. Reemplazar el valor faltante con la información observada proveniente del donante. A continuación, se ejemplifica la técnica imputando los ingresos en los hogares realizando un modelo en el cual se toman como covariables el sexo, la zona y los gastos: encuesta$Income_imp &lt;- encuesta$Income_missing encuesta_obs &lt;- filter(encuesta,!is.na(Income_missing)) encuesta_no_obs &lt;- filter(encuesta, is.na(Income_missing)) mod &lt;- lm(Income ~ Zone + Sex + Expenditure, data = encuesta_obs) Luego, se predicen los valores observados y no observados con el modelo ajustado anteriormente y se imputa el valor faltante calculando las diferencias entre las predicciones de los datos observados y no observados: pred_Obs &lt;- predict(mod, encuesta_obs) pred_no_Obs &lt;- predict(mod, encuesta_no_obs) for (ii in 1:nrow(encuesta_no_obs)) { don_ii &lt;- which.min(abs(pred_no_Obs[ii] - pred_Obs)) encuesta_no_obs$Income_imp[[ii]] &lt;- encuesta_obs$Income_missing[[don_ii]] } encuesta &lt;- bind_rows(encuesta_obs, encuesta_no_obs) Por último, se calcula la media de los ingresos y su desviación estándar para los datos completos e imputado como se ha realizado anteriormente: encuesta %&gt;% summarise( Income_ = mean(Income), Income_sd = sd(Income), Income_imp_ = mean(Income_imp), Income_imp_sd = sd(Income_imp) ) Income_ Income_sd Income_imp_ Income_imp_sd 604.2494 513.1078 607.6411 515.432 De la anterior imputación se puede observar que, la diferencia entre los datos reales y los imputados es cercano a 4 unidades monetarias. A continuación, se presentan la gráfica distribucional del ingreso real y del ingreso imputado por el método del vecino más cercano mediante un modelo. Se puede observar que, las dos distribuciones con muy similares. dat_plot9 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone, Sex, Income, Income_imp), key = &quot;Caso&quot;, value = &quot;Income2&quot;,-Zone,-Sex ) p1 &lt;- ggplot(dat_plot9, aes(x = Income2, fill = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_vline(xintercept = mean(encuesta$Income), col = &quot;red&quot;) + geom_vline(xintercept = mean(encuesta$Income_imp), col = &quot;blue&quot;) p1 "],["imputación-múltiple..html", "10.8 Imputación múltiple.", " 10.8 Imputación múltiple. La imputación múltiple consiste en crear múltiples copias del conjunto de datos, donde los valores faltantes en cada copia son imputados utilizando modelos estadísticos. Estos modelos se basan en las relaciones entre las variables en el conjunto de datos y se utilizan para estimar los valores faltantes de manera plausible. Luego de que se han creado múltiples copias completas del conjunto de datos, se realizan análisis separados en cada copia para generar resultados. Los resultados de cada análisis se combinan para obtener un único resultado final que refleje la incertidumbre causada por la imputación de los valores faltantes. La imputación múltiple es una técnica poderosa para lidiar con los datos faltantes, ya que proporciona resultados más precisos y menos sesgados en comparación con otros métodos que simplemente eliminan las observaciones con valores faltantes. Sin embargo, la imputación múltiple es un proceso computacionalmente intensivo y requiere un conocimiento sólido de la teoría estadística para su implementación efectiva. Esta técnica de imputación fue propuesta por Rubin (1987) y consite en en generar \\(M &gt; 1\\) conjuntos de valores para los datos faltantes. En este sentido, suponga entonces que existe un conjunto de \\(n\\) datos que relaciona dos variables \\(X\\), \\(Y\\), a través del siguiente modelo de regresión simple: \\[y_i = \\beta x_i + \\varepsilon_i\\] Para todo individuo \\(i = 1, \\ldots, n.\\), de tal manera que los errores tienen distribución normal con \\(E(\\varepsilon) = 0\\) y \\(Var(\\varepsilon) = \\sigma ^2\\). Suponga que \\(Y_{Obs}\\) es un vector conteniendo los valores observados para un conjunto de individuos de tamaño \\(n_1\\), \\(Y_{NoObs}\\) es el vector de los valores no observados de tamaño \\(n_0\\), es decir, \\(n_1 + n_0 = n\\). Además, se asume que sí fue posible observar los valores de la covariable \\(X\\) para todos los individuos en la muestra. Al final, el valor imputado corresponderá al promedio de esos \\(M\\) valores. Dado que el interés es la estimación de la pendiente de la regresión lineal simple \\(\\beta\\), entonces la esperanza estimada al utilizar la metodología de imputación múltiple está dada por: \\[E(\\hat{\\beta} | Y_{obs}) = E(E(\\hat{\\beta} | Y_{obs}, Y_{mis}) | Y_{obs})\\] Esta expresión es estimada por el promedio de las \\(M\\) estimaciones puntuales de \\(\\hat{\\beta}\\) sobre las \\(M\\) imputaciones, dado por: \\[\\bar{\\hat{\\beta}} = \\frac{1}{M} \\sum_{m = 1} ^ M \\hat{\\beta}_m\\] La varianza estimada al utilizar la metodología de imputación múltiple está dada por la siguiente expresión: \\[ V(\\hat{\\beta} | Y_{obs}) = E(V(\\hat{\\beta} | Y_{obs}, Y_{mis}) | Y_{obs}) + V(E(\\hat{\\beta} | Y_{obs}, Y_{mis}) | Y_{obs}) \\] La primera parte de la anterior expresión se estima como el promedio de las varianzas muestrales de \\(\\hat{\\beta}\\) sobre las \\(M\\) imputaciones, dado por: \\[\\bar{U} = \\frac{1}{M} \\sum_{m = 1} ^ M Var(\\beta)\\] El segundo término se estima como la varianza muestral de las \\(M\\) estimaciones puntuales de \\(\\hat{\\beta}\\) sobre las \\(M\\) imputaciones, dada por: \\[B = \\frac{1}{M-1} \\sum_{m = 1} ^ M (\\hat{\\beta}_m - \\bar{\\hat{\\beta}})\\] Es necesario tener en cuenta un factor de corrección (puesto que \\(M\\) es finito). Por tanto, la estimación del segundo término viene dada por la siguiente expresión: \\[ (1 + \\frac{1}{M}) B \\] Por tanto, la varianza estimada es igual a: \\[\\hat{V}(\\hat{\\beta} | Y_{obs}) = \\bar{U} + (1 + \\frac{1}{M}) B\\] Se ejemplificará la técnica de imputación múltiple para los datos de la encuesta que se utiliza de ejemplo en este texto: encuesta$Income_imp &lt;- encuesta$Income_missing encuesta$Employment_imp &lt;- encuesta$Employment_missin encuesta_obs &lt;- filter(encuesta,!is.na(Income_missing)) encuesta_no_obs &lt;- filter(encuesta, is.na(Income_missing)) n0 &lt;- nrow(encuesta_no_obs) n1 &lt;- nrow(encuesta_obs) Inicialmente, se extraen los datos a imputar y se calculan los tamaños de los datos observados y no observados. M &lt;- 10 set.seed(1234) for (ii in 1:M) { vp &lt;- paste0(&quot;Income_vp_&quot;, ii) vp2 &lt;- paste0(&quot;Employment_vp_&quot;, ii) encuesta_temp &lt;- encuesta_obs %&gt;% sample_n(size = n1, replace = TRUE) mod &lt;- lm(Income ~ Zone + Sex + Expenditure, data = encuesta_temp) encuesta_no_obs[[vp]] &lt;- predict(mod, encuesta_no_obs) encuesta_obs[[vp]] &lt;- encuesta_obs$Income } El código anterior ajusta un modelo de regresión lineal con la variable Income como respuesta y las variables Zone, Sex y Expenditure como covariables. Este modelo se utiliza para predecir los valores de la variable Income en la base de datos encuesta_no_obs. Una vez corrido el código anterior, se seleccionan las variables de ingresos y sus valores plausibles (diferentes conjuntos de imputaciones) como se muestra a continuación: dplyr::select(encuesta_no_obs, Income, matches(&quot;Income_vp_&quot;))[1:10, 1:4] Income Income_vp_1 Income_vp_2 Income_vp_3 409.87 550.2195 566.0432 567.8296 409.87 561.1194 529.3457 541.8310 90.92 210.5682 225.7715 163.9913 90.92 221.4682 189.0739 137.9927 90.92 210.5682 225.7715 163.9913 135.33 222.6937 237.9191 178.4083 135.33 222.6937 237.9191 178.4083 1539.75 784.8579 801.1103 846.8098 336.00 507.8955 472.7913 439.7467 685.48 593.0111 558.0623 540.9473 A continuación, se grafica la distribución de los ingresos y los valores plausibles, observándose que las distribuciones son muy similares: encuesta &lt;- bind_rows(encuesta_obs, encuesta_no_obs) dat_plot10 &lt;- tidyr::gather( encuesta %&gt;% dplyr::select(Zone, Sex, matches(&quot;Income_vp_&quot;)), key = &quot;Caso&quot;, value = &quot;Income2&quot;,-Zone,-Sex ) p1 &lt;- ggplot(dat_plot10, aes(x = Income2, col = Caso)) + geom_density(alpha = 0.2) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + geom_density(data = encuesta , aes(x = Income), col = &quot;black&quot;, size = 1.2) p1 Con los valores plausibles enocntrados anteriormente, se procede a definir el diseño muestral utilizado en este ejemplo y así poder hacer la estimación de los parámetros. A continuación, se define el diseño muestral: library(srvyr) diseno &lt;- encuesta %&gt;% as_survey_design( strata = Stratum, ids = PSU, weights = wk, nest = T ) Con el diseño anterior, se estiman los ingresos medios para cada valor plausible junto ocn su varianza, como se muestra a continuación: estimacion_vp &lt;- diseno %&gt;% summarise( vp1 = survey_mean(Income_vp_1, vartype = c(&quot;var&quot;)), vp2 = survey_mean(Income_vp_2, vartype = c(&quot;var&quot;)), vp3 = survey_mean(Income_vp_3, vartype = c(&quot;var&quot;)), vp4 = survey_mean(Income_vp_4, vartype = c(&quot;var&quot;)), vp5 = survey_mean(Income_vp_5, vartype = c(&quot;var&quot;)), vp6 = survey_mean(Income_vp_6, vartype = c(&quot;var&quot;)), vp7 = survey_mean(Income_vp_7, vartype = c(&quot;var&quot;)), vp8 = survey_mean(Income_vp_8, vartype = c(&quot;var&quot;)), vp9 = survey_mean(Income_vp_9, vartype = c(&quot;var&quot;)), vp10 = survey_mean(Income_vp_10, vartype = c(&quot;var&quot;)) ) estimacion_vp vp1 vp1_var vp2 vp2_var vp3 vp3_var vp4 vp4_var vp5 vp5_var vp6 vp6_var vp7 vp7_var vp8 vp8_var vp9 vp9_var vp10 vp10_var 619.3525 845.706 617.6058 844.5521 617.4322 867.5133 617.748 856.7732 619.9729 857.805 617.1098 852.6605 618.3378 860.7981 619.2779 867.9485 616.9401 850.131 615.7853 870.0434 A continuación se presentan los datos anteriores discriminado por promedio y varianza: require(tidyr) ( estimacion_vp %&lt;&gt;% tidyr::gather() %&gt;% separate(key, c(&quot;vp&quot;, &quot;estimacion&quot;)) %&gt;% mutate(estimacion = ifelse(is.na(estimacion), &quot;promedio&quot;, &quot;var&quot;)) %&gt;% spread(estimacion, value) %&gt;% mutate(vp = 1:10) ) vp promedio var 1 619.3525 845.7060 2 615.7853 870.0434 3 617.6058 844.5521 4 617.4322 867.5133 5 617.7480 856.7732 6 619.9729 857.8050 7 617.1098 852.6605 8 618.3378 860.7981 9 619.2779 867.9485 10 616.9401 850.1310 Por último, para obtener la estimación de la media y su varianza utilizando la imputación múltiple, se realizan los siguientes cálculos que se derivan de las expresiones matemáticas antes mostradas: Media_vp &lt;- mean(estimacion_vp$promedio) (Ubar &lt;- mean(estimacion_vp$var)) ## [1] 857.3931 (B &lt;- var(estimacion_vp$promedio)) ## [1] 1.645758 var_vp &lt;- Ubar + (1 + 1 / M) (resultado &lt;- data.frame(Media_vp, Media_vp_se = sqrt(var_vp))) Media_vp Media_vp_se 617.9562 29.30005 estimacion_var_vp &lt;- diseno %&gt;% summarise_at(vars(matches(&quot;Income_vp&quot;)), survey_var, vartype = &quot;var&quot;) Por otro lado, otro parámetro de interés es la varianza de los ingresos. Este parámetro permite medir la variabilidad de los ingresos de los ciudadanos de la base de datos de ejemplo. La forma de estimarla es la misma que para el promedio de los ingresos y se utilizarán los mismo códigos mostrados anteriormente, cambiando el parámetro a estimar: ( estimacion_var_vp %&lt;&gt;% tidyr::gather() %&gt;% separate(key, c(&quot;A&quot;, &quot;B&quot;, &quot;vp&quot;, &quot;estimacion&quot;)) %&gt;% mutate( estimacion = ifelse(is.na(estimacion), &quot;promedio&quot;, &quot;var&quot;), A = NULL, B = NULL, vp = as.numeric(vp) ) %&gt;% spread(estimacion, value) ) vp promedio var 1 262689.8 3074674460 2 263092.0 3079895581 3 274370.1 3237991671 4 269127.4 3164842634 5 270450.0 3165285304 6 264992.6 3106582542 7 270916.0 3175865575 8 276069.5 3251836316 9 265060.8 3111426636 10 275462.0 3258214990 Por último, se utilizan las ecuaciones mostradas anteriormente: Media_var_vp &lt;- mean(estimacion_var_vp$promedio) Ubar &lt;- mean(estimacion_var_vp$var) B &lt;- var(estimacion_var_vp$promedio) var_var_vp &lt;- Ubar + (1 + 1 / M) * B resultado$var_vp &lt;- Media_var_vp resultado$var_vp_se &lt;- sqrt(var_var_vp) cbind(Media_var_vp, var_var_vp) Media_var_vp var_var_vp 269223 3191037330 References "],["estimación-de-parámetros-para-modelos-en-encuestas-complejas.html", "Capítulo 11 Estimación de parámetros para modelos en encuestas complejas", " Capítulo 11 Estimación de parámetros para modelos en encuestas complejas En la introducción de su excelente libro, Statistical Design for Researches, Leslie Kish afirma que el enunciado de la mayoría de libros de inferencia estadística abren con el siguiente enunciado: Dadas \\(n\\) variables aleatorias, seleccionadas de una población, independientes e idénticamente distribuidas y que cada palabra en el anterior enunciado es engañosa. ¿Quién le da a uno las muestras? ¿Existe algún sitio en dónde las repartan? Las muestras no son dadas, las muestras deben ser seleccionadas, asignadas o capturadas. El tamaño de la muestra no siempre es un número \\(n\\) fijo, en la mayoría de casos prácticos es una variable aleatoria. Los datos no siguen el supuesto de independencia ni de idéntica distribución; es más, en muchas ocasiones no existe una sola población, sino que la muestra seleccionada es el resultado de una selección de sub-poblaciones para las cuales se deben producir, no solo una sino muchas estimaciones. En la teoría de muestreo, se considera que las características de interés son parámetros y no constituyen realizaciones de variables aleatorias. Para reforzar esta idea haga lo siguiente: examine una moneda y obsérvela. Suponga que usted está observando la cara (o sello, da igual) de la moneda. Esa cara (o sello) no constituye una realización de una variable aleatoria. Para que se pueda hablar de una variable aleatoria, es necesario realizar un experimento, el cual induce el conjunto de todos los posibles resultados, el cual a su vez induce una sigma-álgebra que define a la variable aleatoria. Sería muy diferente si se crease un experimento con esa moneda. El más sencillo de todos sería lanzarla al aire y observar si la moneda cayó en cara o sello. De forma similar, es muy válido afirmar que, por ejemplo, el estado de la naturaleza de un individuo que está desempleado no constituye una realización de una variable aleatoria. Un ejemplo práctico se presenta a la hora de estimar la tasa de desempleo, se considera que, si un individuo está desempleado, pues está desempleado y punto. En otras palabras, el estado de la naturaleza del individuo al momento de la medición es “desempleado” y esta caracterización no corresponde a ninguna realización de algún evento aleatorio. Es por esto que, una vertiente de la inferencia en poblaciones finitas considera que el parámetro de interés será el número total de personas desempleadas dividido por el número total de personas en la fuerza laboral. Si se tuviese la oportunidad de medir a todos los integrantes de la fuerza laboral, mediante la realización de un censo, pues esa división correspondería al parámetro poblacional con el cual se tomarían decisiones y/o se cambiarían o reforzarían las políticas públicas de un país. El propósito de este capítulo es llevar a los lectores al correcto análisis de sus datos, preguntándose acerca del proceso de selección de la muestra. Más aún, en términos de muestreo, solo hay un único caso para el cual la teoría de la inferencia estadística es aplicable y se trata del muestreo aleatorio simple con reemplazo en donde si se tienen las propiedades de independencia y de idéntica distribución. Note que, en términos de selección de muestras, solo hay dos posibles escenarios generales. La selección con reemplazo y la selección sin reemplazo. "],["acerca-de-las-muestras-aleatorias-y-su-análisis.html", "11.1 Acerca de las muestras aleatorias y su análisis", " 11.1 Acerca de las muestras aleatorias y su análisis Hablemos primero de la selección sin reemplazo, en donde una muestra seleccionada está conformada por algunos elementos de la población que no se repiten. Para seleccionar una muestra sin reemplazo de tamaño \\(n=3\\), de una población de tamaño \\(N=5\\), el proceso de selección puede ser de la siguiente manera. Se escoge una unidad de las cinco posibles, luego se selecciona una unidad de las cuatro restantes, y por último, una unidad de las tres restantes. Esto hace que el proceso de selección de la muestra no se lleve a cabo de forma independiente. Por ejemplo, si el muestreo es aleatorio simple, la probabilidad de selección de la primera unidad es 1/5, la probabilidad de selección de la segunda unidad es 1/4 y así sucesivamente. Por otro lado, cuando el muestreo es con reemplazo, la selección se realiza de forma independiente puesto que se trata de realizar el mismo ensayo (seleccionar una unidad de cinco posibles) tres veces, sin importar que las unidades tengan diferentes probabilidades de selección. Por otra parte, es bien sabido que la teoría de muestreo establece que el valor de la característica de interés, \\(y_k\\), es eso, un valor; por tanto, no es aleatorio. Luego, es incorrecto decir que \\(y_k\\) es una variable aleatoria asociada con alguna distribución de probabilidad. Recuerde que en el muestreo lo único aleatorio en la inferencia es la muestra. Ahora, no significa que no podamos construir variables aleatorias en muestreo. Por ejemplo, construyamos la siguiente variable aleatoria \\(X_i\\) (\\(i=1,2,3\\)) definida como el valor de la característica de interés en el individuo \\(k\\)-ésimo, seleccionado en la \\(i\\)-ésima extracción. En este caso, existen tres variables aleatorias, puesto que la muestra es de tamaño tres. Si consideramos un muestreo aleatorio sin reemplazo, la primera variable aleatoria \\(X_1\\), podrá tomar cualquiera de los siguiente cinco valores: \\(y_1, y_2, y_3, y_4, y_5\\). La segunda variable aleatoria \\(X_2\\), solo podrá tomar cuatro valores, puesto que \\(X_1\\) ya fue realizada, y la tercera variable aleatoria \\(X_3\\) solo podrá tomar tres valores, puesto que \\(X_1\\) y \\(X_2\\) ya fueron realizadas. Esto hace que \\(X_1\\), \\(X_2\\) y \\(X_3\\) no constituya una sucesión de variables aleatorias independientes (puesto que la selección sin reemplazo no es un proceso independiente) ni idénticamente distribuidas (puesto que ni siquiera su espacio muestral es el mismo: \\(X_1\\) puede tomar cinco valores, \\(X_2\\) solo cuatro y \\(X_3\\) solo tres). Lo cual quiere decir que a partir de un muestreo sin reemplazo (ni siquiera el tan mencionado muestreo aleatorio simple) no es posible construir una muestra aleatoria, como las que aparecen en los libros de teoría estadística. Sin embargo, algo muy distinto sucede con el muestreo con reemplazo. Cuando construimos las variables aleatorias \\(X_1\\), \\(X_2\\) y \\(X_3\\), resulta ser que ellas sí conforman una sucesión de variables aleatorias independientes (puesto que el muestreo con reemplazo sí define un proceso de extracciones independientes) e idénticamente distribuidas (puesto que conservan el mismo espacio muestral y mantienen la probabilidad de selección). Es decir, \\(X_1\\) puede tomar los valores \\(y_1, \\ldots, y_5\\). La probabilidad de que \\(X_1=y_1\\) es \\(p_1\\), la probabilidad de selección del primer elemento; la probabilidad de que \\(X_1=y_2\\) es \\(p_2\\), la probabilidad de selección del segundo elemento y así sucesivamente hasta obtener que la probabilidad de que \\(X_1=y_5\\) es \\(p_5\\), la probabilidad de selección del primer elemento primer elemento. La misma distribución la tienen \\(X_2\\) y \\(X_3\\). Por lo tanto, \\(X_1\\), \\(X_2\\) y \\(X_3\\) conforman una muestra aleatoria, como las que aparecen en los libros clásicos de inferencia estadística. Entonces, hemos llegado a un punto sin retorno, en donde la conclusión es que, si la muestra fue seleccionada con reemplazo, entonces podemos inducir una muestra aleatoria. Sin embargo, existen muchas variantes en el muestreo con reemplazo. A continuación, vamos a dilucidar cuál de ellas es la indicada para analizar la muestra de acuerdo con la teoría de los libros de inferencia. En primera instancia, veamos que para que la esperanza (bajo el diseño de muestreo \\(p\\)) de cualquier variable aleatoria \\(X_i\\) sea igual a la media poblacional, es necesario que, para todos los individuos en la población, la probabilidad de selección sea idéntica e igual a \\(1/N\\), como se muestra a continuación: \\[ E_p(X_i)=\\sum_{k \\in U} y_k Pr(X_i = Y_k) = \\sum_{k \\in U} y_k p_k = \\frac{t_y}{N} = \\bar{y}_U=\\mu_N \\] De la misma manera, para que la varianza de cualquier variable aleatoria \\(X_i\\) sea igual a la varianza poblacional, se requiere la misma condición, puesto que: \\[ Var_p(X_i) = \\sum_{k \\in U} (y_k - \\bar{y}_U)^2 p_k = \\frac{1}{N}\\sum_{k \\in U} (y_k - \\bar{y}_U)^2 = S^2_{y_U} = \\sigma^2_N \\] Por lo tanto, la esperanza y la varianza de un estimador clásico como \\(\\bar{X}\\) solo coincidierón con los bien conocidos resultados de la inferencia clásica cuando el muestreo haya sido aleatorio simple con reemplazo. De otra forma, no se tienen las, bien conocidas, propiedades de esta estadística que implican que su esperanza es \\(E(\\bar{X}) = \\mu_N\\) y su varianza es \\(Var(\\bar{X}) = \\frac{\\sigma^2_N}{n}\\). Este razonamiento de aplicarse de la misma forma para pruebas de hipótesis, construcción de intervalos de confianza, modelos de regresión, y hasta diseño de experimentos. Ahora, para una encuesta cuyos datos no fueron extraídos de manera aleatoria simple con reemplazo, la manera correcta de analizarla confiadamente es incluir los pesos de muestreo en todas las técnicas y metodologías estadísticas, ya sean regresiones simples y logísticas o simples varianzas del promedio. "],["modelos-de-superpoblación.html", "11.2 Modelos de superpoblación", " 11.2 Modelos de superpoblación Suponga que la estimación de máxima verosimilitud es apropiada para muestras aleatorias simples. Por ejemplo, modelos de regresión simple, múltiple, regresión logística, entre otros. Bajo este esquema, se asume que la función de densidad poblacional es \\(f(y | \\theta)\\) donde \\(\\theta\\) es el parámetro de interés. Con una réplica del ejemplo que David Binder utiliza en un artículo del año 2011 (una excelente lectura para quienes ha seguido el trabajo de Ken Brewer), se introducen algunos conceptos que son de utilidad. Finalmente, todos los resultados se van a plasmar en simulaciones de Monte Carlo, algunas veces anidadas. Suponga que se generaron \\(N=100\\) realizaciones de variables aleatorias independientes distribuidas Bernoulli con parámetro de interés \\(\\theta=0.3\\). Los datos que se obtienen se muestran a continuación: 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 En esta población finita, que fue generada a partir de un modelo probabilístico (llamado modelo de superpoblación), hay 28 éxitos. 11.2.1 Primer proceso inferencial: el modelo En este apartado, es notable que la medida de probabilidad que rige la inferencia hasta el momento sea la inducida por la distribución binomial con parámetro 0.3. De esta manera, el estimador insesgado de mínima varianza (todas estas propiedades obtenidas con base en la distribución binomial) está dado por el promedio poblacional. Nótese que la inferencia utiliza todos los datos de la población. Ahora, para reproducirlo computacionalmente, basta con simular muchas poblaciones de 100 variables aleatorias independientes distribuidas Bernoulli con parámetro desconocido \\(\\theta\\)=0.3. Como es bien sabido, bajo la perspectiva de los modelos poblacionales y la inferencia estadística clásica, el estimador \\(\\bar{y}_U = \\frac{\\sum_U y_k}{N}\\) es insesgado. Para corroborarlo, es posible introducir la siguiente simulación de Monte Carlo. N = 100 theta = 0.3 nsim1 = 1000 Est0=rep(NA,nsim1) for(i in 1:nsim1){ y=rbinom(N, 1, theta) Est0[i]=mean(y) } Esp0 = mean(Est0) cbind(theta, Esp0) theta Esp0 0.3 0.29742 11.2.2 Segundo proceso inferencial: el muestreo En el primer proceso inferencial, se asume que las variables de estudio son realizaciones de variables aleatorias gobernadas por un modelo probabilístico. Sin embargo, un razonamiento muy válido es que en cualquier población finita en particular, los valores de la medición son fijos aunque desconocidos y no siguen ningún modelo probabilístico; es decir, no corresponden a realizaciones de variables aleatorias. Por ejemplo, suponga que para esa misma población del ejemplo anterior el dato uno corresponde a un individuo desempleado y el dato cero corresponde a un individuo empleado. Por otra parte, asuma que la población está subdividida en conglomerados, que pueden ser llamados hogares. De esta forma, nuestra población finita toma la siguiente caracterización, mediante una partición de \\(N_{I}=27\\) hogares: (1 1 0) (1 0) (0 0 0 0 0 0 1) (1 0) (0 0 0 0 0 0 1) (0 0 1) (0 0 0 0 0 0 0 1) (0 0 1) (0 0 0 1) (0 0 0 0 1) (0 0 0 0 0 0 0 1) (1 0) (1 0) (0 0 1) (1 0) (0 0 1) (1 0) (0 1) (0 0 0 1) (0 0 1) (1 1 0) (0 0 0 0 1) (0 1) (0 1) (0 0 0 0 0 0 0 0 0 1) (0 1) (0) El proceso de aglomeración en hogares es obviamente artificioso en este ejemplo, pero ilustra que en la vida real las poblaciones finitas siempre están aglomeradas. Suponga por otra parte que tomamos una muestra \\(S_{I}\\) de \\(n_{I}\\) hogares y en cada hogar seleccionado realizamos un censo; además la selección de los hogares se hará aleatoriamente, sin reemplazo y con probabilidades de inclusión \\(\\pi_{Ii}\\) proporcionales al tamaño del hogar \\(N_{i}\\). Siendo la característica de interés \\(y_{k}\\), el estado del individuo en la fuerza laboral (1, si está desempleado y 0, en otro caso); entonces es bien sabido que bajo este esquema de muestreo un estimador insesgado para la proporción de desempleados \\(\\bar{y}_{U}\\) es el siguiente: \\[ \\bar{y}_{\\pi S}=\\sum_{i\\in S_{I}}\\frac{t_{y_{i}}}{\\pi_{Ii}}=\\frac{\\sum_{i\\in S_{I}}\\bar{y}_{i}}{n_{I}} \\] En donde \\(\\bar{y}_{i}=\\frac{t_{y_{i}}}{N_{i}}\\) es la proporción de desempleados en el hogar \\(i\\)-ésimo, \\(t_{y_{i}}\\) es el total de desempleados en el hogar \\(i\\)-ésimo, \\(N_{i}\\) es el número de individuos en el hogar y \\(n_{I}\\) es el número de hogares seleccionados. Por otro lado, un estimador ingenuo, correspondiente a la proporción de desempleados en la muestra, que asume que el agrupamiento de los valores no interfiere en el proceso de inferencia e ignora el diseño de muestreo es el siguiente: \\[ \\bar{y}_{S}=\\frac{\\sum_{i\\in S_{I}}t_{y_{i}}}{\\sum_{i\\in S_{I}}N_{i}} \\] En términos generales el siguiente esquema trata de reproducir gráficamente este proceso de inferencia, en donde un gran número de muestras podrían haber sido extraídas siguiendo el diseño de muestreo. Con la siguiente simulación de Monte Carlo se comprueba fácilmente que es insesgado, mientras que es sesgado: library(TeachingSampling) N=100 theta=0.3 y=rbinom(N, 1, theta) theta_N=mean(y) nsim2=1000 Est1=Est2=rep(NA,nsim2) #-----Creación de los clusters--------- clus=c(0,which((y[-N]-y[-1])!=0)+1) NI=(length(clus)-1) Ind=matrix(0, nrow=N, ncol=NI) Tamaños=clus[-1]-clus[-(length(clus))] for(l in 1:(length(clus)-1)){ a=(clus[l]+1):clus[l+1] Ind[a,l]=a } #Tamaños nsim2=1000 nI=floor(NI*0.3) for(j in 1:nsim2){ res &lt;- S.piPS(nI,Tamaños) sam &lt;- res[,1] Ind.sam=Ind[,sam] Tamaños.sam=Tamaños[sam] #-------Espacio para las medias medias=matrix(NA) for(k in 1:ncol(Ind.sam)){ medias[k]=mean(y[Ind.sam[,k]]) } #------- Est1[j]=mean(medias) Est2[j]=sum(Tamaños.sam*medias)/sum(Tamaños) } Esp1=mean(Est1) Esp2=mean(Est2) cbind(theta_N, Esp1, Esp2) theta_N Esp1 Esp2 0.29 0.2928711 0.11186 Nótese que el primer estimador es insesgado (su esperanza equivale al parámetro de la población finita) porque es función del inverso de la probabilidad de inclusión de los elementos que son inducidas por la medida de probabilidad definida por el plan de muestreo. El segundo estimador es sesgado porque no tiene en cuenta el diseño de muestreo. 11.2.3 Inferencia doble: los modelos y el muestreo En último lugar, suponga que los valores de las variables de interés sí constituyen realizaciones de variables aleatorias que siguen un modelo probabilístico. Como una población finita está constituida por la realización particular de las variables aleatorias, condicionado a la realización de una población finita, se extrae una muestra aleatoria de elementos, mediante un diseño de muestreo complejo. Nótese que, en este tercer proceso inferencial, tanto el modelo como el diseño de muestreo como la medida de probabilidad que da origen a las superpoblaciones, constituyen dos medidas de probabilidad distintas que deben regir la inferencia del parámetro de interés. Al respecto, nótese que, dado que el diseño de muestreo es complejo, no es viable utilizar técnicas clásicas, como el método de máxima verosimilitud, puesto que los datos finales no constituyen una muestra aleatoria de variables independientes ni idénticamente distribuidas. Por lo anterior, la forma final de la función de verosimilitud, definida como la densidad conjunta de las variables en la muestra, será muy compleja, intratable e insoluble. Una solución a este problema de estimación es la técnica de máxima pseudo-verosimilitud, la cual induce estimadores que tienen en cuenta las ponderaciones del diseño de muestreo complejo. Para el ejemplo de las proporciones, el estimador \\(\\bar{y}_{\\pi S}\\) cumple la siguiente relación: \\[ E_{\\xi p}(\\bar{y}_{\\pi S})=E_{\\xi}E_{p}(\\bar{y}_{\\pi S}|Y)=E_{\\xi}(\\bar{y}_{U})=\\theta=0.3 \\] Con la siguiente simulación de Monte Carlo se comprueba fácilmente que \\(\\bar{y}_{\\pi S}\\) es insesgado, mientras que es \\(\\bar{y}_{S}\\) sesgado: library(TeachingSampling) N=100 theta=0.3 nsim1=100 Esp1=Esp2=rep(NA,nsim1) for(i in 1:nsim1){ y=rbinom(N, 1, theta) #-----Creación de los clusters--------- clus=c(0,which((y[-N]-y[-1])!=0)+1) NI=(length(clus)-1) Ind=matrix(0, nrow=N, ncol=NI) Tamaños=clus[-1]-clus[-(length(clus))] for(l in 1:(length(clus)-1)){ a=(clus[l]+1):clus[l+1] Ind[a,l]=a } Ind Tamaños nsim2=100 nI=floor(NI*0.3) Est1=Est2=rep(NA,nsim2) for(j in 1:nsim2){ res &lt;- S.piPS(nI,Tamaños) sam &lt;- res[,1] sam Ind.sam=Ind[,sam] Tamaños.sam=Tamaños[sam] #-------Espacio para las medias medias=matrix(0) for(k in 1:ncol(Ind.sam)){ medias[k]=mean(y[Ind.sam[,k]]) } Est1[j]=mean(medias) Est2[j]=sum(Tamaños.sam*medias)/sum(Tamaños) } Esp1[i]=mean(Est1) Esp2[i]=mean(Est2) } cbind(theta, mean(Esp1), mean(Esp2)) theta 0.3 0.3120952 0.1156181 Por supuesto que, dado que el proceso de inferencia es doble, entonces este ejercicio de Monte Carlo debe ser anidado. Es decir, muchas simulaciones dentro de una simulación. Nótese que en primer lugar se debe generar todas las poblaciones finitas y para cada una de ellas se debe generar las posibles muestras. Los métodos que se explicarán en este capítulo serán la estimación por Máxima Verosimilitud (MV) y Máxima Pseudo Verosimilitud (MPV) para modelos de regresión. El primer método se basa en estimar un parámetro desconocido suponiendo que las variables de interés constituyen una muestra aleatoria de variables independiente e idénticamente distribuidas (IID) para poder hacer inferencia sobre la población de interés. Por otra parte el método de Máxima Pseudo Verosimilitud sigue un razonamiento parecido, pero con la gran diferencia de que la variable de interés se rige por un diseño muestral específico, lo cual induce una probabilidad de inclusión del individuo que debe ser tenida en cuenta al momento de realizar cualquier tipo de inferencia. "],["método-de-máxima-verosimilitud.html", "11.3 Método de Máxima Verosimilitud", " 11.3 Método de Máxima Verosimilitud Uno de los métodos más utilizados en la estadística para estimar parámetros es el método de Máxima Verosimilitud, para utilizar este método debemos conocer la función de distribución de las variables de interés. Luego, si \\(y_{1},y_{2},\\ldots,y_{N}\\) una muestra aleatoria de las variable de interés que siguen una distribución \\(f(y;\\theta)\\). Por lo tanto, la función de verosimilitud está dada por: \\[ L(\\theta)=\\prod_{i=1}^{n}f(y_{i},\\theta) \\] Para un mejor manejo de esta función se sugiere aplicar propiedades de los logaritmos generando la siguiente función \\[ l(\\theta)=\\sum_{i=1}^{n}\\ln[f(y_{i},\\theta)] \\] Calculando las derivadas con respecto a \\(\\theta\\) e igualando a cero tenemos el siguiente sistema de ecuaciones \\[ \\sum_{i=1}^{N}\\frac{\\partial}{\\partial\\theta}\\ln[f(y_{i},\\theta)]=0 \\] Ahora, definiendo a \\(u_{i}=\\frac{\\partial}{\\partial\\theta}\\ln[f(y_{i},\\theta)]\\), entonces el sistema de ecuaciones tendría la siguiente forma: \\[ \\sum_{i=1}^{N}u_{i}(\\theta)=0 \\] \\(u_{i}\\) es conocido el puntaje o de la unidad \\(i\\)-ésima. La solución de este sistema de ecuaciones, notada como \\(\\hat{\\theta}_{MV}\\), es conocida como el Estimador de Máxima Verosimilitud. Una bondad de este método es que podemos obtener una varianza asintótica del modelo \\(\\xi\\), de la siguiente manera \\[ V_{\\xi}(\\hat{\\theta}_{MV})\\cong[J(\\theta)]^{-1} \\] donde, \\[ J(\\theta)=\\sum_{i=1}^{N}\\partial u_{i}(\\theta)/\\partial\\theta \\] Como el anterior término depende del parámetro, un estimador consistente estaría dado por: \\[ \\hat{V}_{\\xi}(\\hat{\\theta}_{MV})=[J(\\hat{\\theta}_{MV})]^{-1} \\] donde, \\[ J(\\hat{\\theta}_{MV})=J(\\theta)\\mid_{\\theta=\\hat{\\theta}_{MV}} \\] 11.3.1 MV para una distribución Bernoulli En el ejemplo introductorio que sirvió como punto de partida para esta discusión, se habló de que los datos de naturaleza \\(\\{0,1\\}\\) pueden ser modelados mediante una distribución Bernoulli, con parámetro de éxito \\(\\theta\\). De esta forma, la función de verosimilitud está dada por: \\[ L(\\theta) =\\prod_{i=1}^{N}\\theta^{y_{i}}(1-\\theta)^{1-y_{i}} \\] Luego, aplicando logaritmo, se tiene que: \\[ l(\\theta) =\\sum_{i=1}^{N}\\left[{y_{i}}\\ln(\\theta)+(1-y_{i})\\ln(1-\\theta)\\right] \\] Por lo tanto, las ecuaciones de verosimilitud, definidas en función de las variables de puntaje () son: \\[ \\sum_{i=1}^{N}\\frac{\\partial}{\\partial\\theta}\\left[{y_{i}}\\ln(\\theta)+(1-y_{i})\\ln(1-\\theta)\\right]=\\sum_{i=1}^{N}u_{i}(\\theta) \\] En donde, \\(u_{i}(\\theta)=\\frac{y_{i}-\\theta}{\\theta(1-\\theta)}\\). Por tanto, igualando a cero, se obtiene que \\[ \\frac{\\partial}{\\partial\\theta}\\ln(\\theta)\\bar{y}_{U}+\\frac{\\partial}{\\partial\\theta}\\ln(1-\\theta)(n-\\bar{y}_{U})=0 \\] De lo cual se obtiene el estimador de máxima verosimilitud dado por: \\[ \\hat{\\theta}_{MV}=\\bar{y}_{U}=P_{d} \\] Con varianza estimada dada por: \\[ \\hat{V}_{\\xi}(\\hat{\\theta}_{MV})=[J(\\hat{\\theta}_{MV})]^{-1} \\] En donde, \\[ J(\\hat{\\theta}_{MV})=\\sum_{i=1}^{N}\\frac{\\partial}{\\partial\\theta}u_{i}(\\theta)=\\frac{N}{\\bar{y}_{U}(1-\\bar{y}_{U})}=\\frac{N}{P_{d}(1-P_{d})} \\] Es decir que la estimación de la varianza para \\(\\hat{\\theta}_{MV}=P_{d}\\) es \\(\\hat{Var}_{\\xi}(\\hat{\\theta}_{MV})=P_{d}Q_{d}/N\\). En donde, \\(Q_{d}=1-P_{d}\\). 11.3.2 MV para una distribución normal Ahora se ilustrará el método de Máxima Verosimilitud suponiendo la siguiente función de distribución de un variable aleatoria con distribución normal \\[ f(y;\\theta)=\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2}\\left(\\dfrac{y_{i}-\\theta^{2}}{\\sigma^{2}}\\right)\\right] \\] Conociendo la función de distribución llegamos a la probabilidad conjunta \\[ L(\\theta)=\\prod_{i=1}^{N}\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2}\\left(\\dfrac{y_{i}-\\theta^{2}}{\\sigma^{2}}\\right)\\right] \\] Con un poco de álgebra llegamos a esta expresión \\[ L(\\theta)=(2\\pi\\sigma^{2})^{-N/2}\\exp[(-\\dfrac{1}{2\\sigma^{2}}\\sum_{i=1}^{N}(y_{i}-\\theta^{2})] \\] Aplicando logaritmos \\[ l(\\theta)=ln(2\\pi\\sigma^{2})^{-N/2}[-\\dfrac{1}{2\\sigma^{2}}\\sum_{i=1}^{N}(y_{i}-\\theta^{2})] \\] Maximizando la anterior expresión llegamos a obtener el score \\(u_{i}\\) \\[ u_{i}=\\partial l(\\theta)/\\partial\\theta=\\dfrac{1}{\\sigma^{2}}\\sum_{i=1}^{N}(y_{i}-\\theta^{2})=0 \\] igualando a cero despejamos \\(\\theta\\) y tenemos \\[ \\theta=\\dfrac{\\sum_{i=1}^{N}y_{i}}{N}=\\bar{Y} \\] Llegamos a que una estimación por el método de Máxima Verosimilitud, para la función \\(\\theta\\) que sigue una función de distribución normal, es el promedio poblacional \\(\\bar{Y}\\). 11.3.3 MV para una regresión lineal múltiple En un entorno matricial se puede tener en cuenta más de una variable predictora llevándonos a un modelo de regresión múltiple donde no solamente las variables \\(y_{i}\\) son continuas, sino que también pueden ser categóricas. A continuación, se presenta la estimación de parámetros del modelo. El modelo adopta la forma \\(X&#39;\\beta\\), disponemos de \\(X\\) como una matriz de dimensión \\(N\\times i\\), donde \\(n\\) es el tamaño de muestra e \\(i\\) es el número de variables predictoras, también se define un vector \\(Y\\) de tamaño \\(n\\) como la variable de interés y, por último, un vector \\(\\beta\\) de tamaño \\(i\\). Suponiendo que \\(X\\) sigue una distribución normal tenemos la siguiente función: \\[ f(Y;X\\beta)=\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2\\sigma^{2}}(Y-X\\beta)&#39;(Y-X\\beta)\\right] \\] Conociendo la anterior función de distribución, llegaremos a la probabilidad conjunta de \\(f\\) \\[ L(Y;X\\beta)=\\prod_{i=1}^{n}\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2\\sigma^{2}}(Y-X\\beta)&#39;(Y-X\\beta)\\right] \\] Con un poco de algebra matricial se llega a: \\[ L(Y;X\\beta)=(2\\pi\\sigma^{2})^{-n/2}-\\exp\\left[\\dfrac{1}{2\\sigma^{2}}(Y&#39;Y-Y&#39;X\\beta-(X\\beta)&#39;Y+(X\\beta)&#39;X\\beta)\\right] \\] Aplicando propiedades de logaritmos nos queda la siguiente expresión: \\[ l(Y;X\\beta)=ln(2\\pi\\sigma^{2})^{-n/2}-\\dfrac{1}{2\\sigma^{2}}(Y&#39;Y-Y&#39;X\\beta-(X\\beta)&#39;Y+(X\\beta)&#39;X\\beta) \\] Maximizando el anterior resultado podemos llegar al score \\(u_{i}\\): \\[ \\dfrac{\\partial l(Y;X\\beta)}{\\partial\\beta}=-\\dfrac{1}{2\\sigma^{2}}(-2X&#39;Y+2X&#39;X\\beta) \\] Igualando a cero la derivada y despejando llegamos a la estimación de \\(\\beta\\): \\[ \\beta=(X&#39;X)^{-1}(X&#39;Y) \\] El anterior resultado es la estimación general de \\(\\beta\\) en una regresión múltiple, obtenida bajo el método de Máxima Verosimilitud. "],["método-de-máxima-pseudo-verosimilitud.html", "11.4 Método de Máxima Pseudo-Verosimilitud", " 11.4 Método de Máxima Pseudo-Verosimilitud El anterior método tiene la particularidad de que \\(y_{i}\\) son IID, en la vida real muchas veces no es posible poder cumplir ese supuesto. En los procedimientos actuales se recurre a obtener una muestra compleja mediante la realización de conglomerados que tengan alguna relación particular de aglomeración para luego estratificarlos y llegar al individuo de interés que nos proporcione información sobre el estudio. Con esa muestra compleja cumplimos con que todos los individuos tienen una probabilidad de inclusión desigual sin la necesidad de utilizar un marco muestral. A partir de eso Pfeffermann(1993) discutió la posibilidad de hacer inferencia en la población partiendo de la información de una muestra, para esto se propuso crear un pseudo-parámetro que tenga en cuenta el diseño muestral, es decir, que el score \\(u_{i}\\) sea ponderado por el inverso de la probabilidad de inclusión que denominaremos \\(u_{i}\\). Este método es conocido como Máxima Pseudo Verosimilitud(MPV). \\[ L(\\theta)=\\prod_{i=1}^{n}w_{i}f(y_{i},\\theta) \\] Para un mejor manejo de esta función se sugiere aplicar propiedades de los logaritmos generando la siguiente función: \\[ l(\\theta)=\\sum_{i=1}^{n}\\ln[w_{i}f(y_{i},\\theta)] \\] Calculando las derivadas parciales de \\(L(\\theta)\\) con respecto a \\(\\theta\\) e igualando a cero tenemos un sistema de ecuaciones como sigue: \\[ \\dfrac{\\partial l(\\theta)}{\\partial\\theta}=\\sum_{i=1}^{n}w_{i}u_{i}(\\theta)=0 \\] donde \\(ui=\\partial\\ln[f(y_{i},\\theta)]/\\partial\\theta\\) es el vector de “score” de elementos \\(i,i\\in n\\) ponderado por \\(w_{i}\\), ahora definiremos \\(T\\) como: \\[ T=\\sum_{i=1}^{n}w_{i}u_{i}(\\theta)=0 \\] Mediante la linealización de Taylor y considerando los resultados de Binder(1983), podemos obtener una varianza asintóticamente insesgada de la siguiente forma: \\[ V_{p}(\\hat{\\theta}_{M}PV)\\cong[J(\\theta_{U})]^{-1}V_{p}(T)[J(\\theta_{U})]^{-1} \\] donde, \\[ J(\\theta_{U})=\\sum_{i\\in U}\\dfrac{\\partial u_{i}(\\theta)}{\\partial(\\theta)}\\mid_{\\theta=\\theta_{U}} \\] La estimación de la varianza anterior está definida por: \\[ \\hat{V}_{p}(\\hat{\\theta}_{MPV})\\cong[\\hat{J}(\\hat{\\theta}_{MPV})]^{-1}\\hat{V}_{p}(T)[\\hat{J}(\\hat{\\theta}_{MPV})]^{-1} \\] donde, \\[ \\hat{J}(\\theta_{MPV})=\\sum_{i\\in U}w_{i}\\dfrac{\\partial u_{i}(\\theta)}{\\partial(\\theta)}\\mid_{\\theta=\\theta_{MPV}} \\] 11.4.1 MPV para una distribución Bernoulli Las ecuaciones de verosimilitud dadas anteriormente, conllevan a aplicar la técnica de pseudo-verosimilitud, para la cual, en primer lugar, se definen: \\[ u_{k}(\\theta)=\\frac{y_{k}-\\theta}{\\theta(1-\\theta)} \\] Luego, las ecuaciones de pseudo-verosimilitud son: \\[ \\sum_{k=1}^{n}w_{k}u_{k}(\\theta)=\\sum_{k=1}^{n}w_{k}\\frac{y_{k}-\\theta}{\\theta(1-\\theta)} \\] Por lo tanto, al igualar a cero, se tiene que: \\[ \\sum_{k=1}^{n}w_{k}y_{k}-\\theta\\sum_{k=1}^{n}w_{k}=0 \\] Por lo anterior, al despejar, se tiene que el estimador de máxima pseudo-verosimilitud, está dado por: \\[ \\hat{\\theta}_{MPV}=\\frac{\\sum_{k=1}^{n}w_{k}y_{k}}{\\sum_{k=1}^{n}w_{k}}=\\frac{\\hat{t}_{y,\\pi}}{\\hat{N}}=\\tilde{y}_{S}=\\tilde{p}_{d} \\] Luego, el estimador de la varianza de \\(\\hat{\\theta}_{MPV}\\) es: \\[ \\hat{V}_{p}(\\hat{\\theta}_{MPV})\\cong[\\hat{J}(\\hat{\\theta}_{MPV})]^{-1}\\hat{V}_{p}(\\hat{t}_{u\\pi})[\\hat{J}(\\hat{\\theta}_{MPV})]^{-1} \\] donde \\[ \\hat{J}(\\theta_{MPV})=\\sum_{i\\in U}w_{i}\\dfrac{\\partial u_{i}(\\theta)}{\\partial(\\theta)}\\mid_{\\theta=\\hat{\\theta_{MPV}}}=\\frac{\\hat{N}}{\\tilde{y}_{S}(1-\\tilde{y}_{S})}=\\frac{\\hat{N}}{\\tilde{p}_{d}(1-\\tilde{p}_{d})} \\] Por ejemplo, bajo un muestreo aleatorio simple sin reemplazo, se tiene que el estimador de máxima pseudo-verosimilitud es \\(\\hat{\\theta}_{MPV}=\\bar{y}_{S}\\). Además, la estimación de su varianza es: \\[ \\hat{V}_{MAS}(\\hat{t}_{u\\pi})=\\frac{N^{2}}{n}\\left(1-\\frac{n}{N}\\right)S_{\\hat{u}_{S}}^{2}=\\frac{N^{2}}{n}\\left(1-\\frac{n}{N}\\right)\\frac{1}{n-1}\\sum_{k=1}^{n}(\\hat{u}_{k}-\\bar{\\hat{u}})^{2} \\] Luego, teniendo en cuenta que bajo este diseño de muestreo, se tiene que \\(\\bar{\\hat{u}}=0\\) y que \\(\\hat{N}=N\\), entonces el estimador de la varianza de \\(\\hat{\\theta}_{MPV}\\) es: \\[ \\hat{V}_{MAS}(\\hat{\\theta}_{MPV})\\cong\\frac{1}{n}\\left(1-\\frac{n}{N}\\right)S_{y_{S}}^{2} \\] Nótese que la anterior expresión, coincide plenamente con la estimación de la varianza de la media muestral, es decir \\(\\hat{V}_{MAS}(\\hat{\\theta}_{MPV})=\\hat{V}_{MAS}(\\bar{y}_{S})\\). 11.4.2 MPV para una distribución normal Siguiendo el mismo orden de la sección de Máxima Verosimilitud, se ilustrará el método de Máxima Pseudo Verosimilitud, suponga que \\(f(y;\\theta)\\) sigue una función de distribución normal. \\[ f(y;\\theta)=\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2}\\left(\\dfrac{y_{i}-\\theta^{2}}{\\sigma^{2}}\\right)w_{i}\\right] \\] Aplicaremos la productoria para llegar a la probabilidad conjunta: \\[ L(\\theta)=\\prod_{i=1}^{n}\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2}\\left(\\dfrac{y_{i}-\\theta^{2}}{\\sigma^{2}}\\right)w_{i}\\right] \\] Con algo de algebra llegamos a: \\[ L(\\theta)=(2\\pi\\sigma^{2})^{-n/2}\\exp[(-\\dfrac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(y_{i}-\\theta^{2})w_{i}] \\] Utilizamos logaritmos tenemos: \\[ l(\\theta)=ln(2\\pi\\sigma^{2})^{-n/2}[-\\dfrac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(y_{i}-\\theta^{2})w_{i}] \\] Maximizamos la anterior expresión con derivadas parciales tenemos: \\[ \\partial l(\\theta)/\\partial\\theta=\\dfrac{1}{\\sigma^{2}}\\sum_{i=1}^{n}(y_{i}-\\theta^{2})w_{i}=0 \\] Despejando \\(\\theta\\), se llega a un resultado interesante: \\[ \\theta=\\dfrac{\\sum_{i=1}^{n}y_{i}}{\\sum_{i=1}^{n}w_{i}}=\\dfrac{\\hat{t}_{y\\pi}}{\\hat{N}}=\\tilde{Y} \\] Esto nos conlleva que, para la función \\(\\theta\\) una estimación es el promedio muestral ponderado. 11.4.3 MPV para una regresión múltiple Con el modelo de la forma \\(X&#39;\\beta\\) se tiene una matriz \\(X\\) de dimensión \\(n\\times i\\), donde \\(n\\) es el tamaño de la muestra e \\(i\\) es el número de variables predictoras, también una matriz \\(W\\) diagonal, con los \\(w_{i}\\), de tamaño \\(n\\times n\\), y, por último, se define dos vectores, uno \\(Y\\) de tamaño \\(n\\) como la variable de interés y otro \\(\\beta\\) de tamaño \\(i\\). Con estas condiciones se puede definir una función de verosimilitud de la siguiente manera. Conociendo la función de distribución normal de \\(X\\) \\[ f(Y;X\\beta)=\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2\\sigma^{2}}(Y-X\\beta)&#39;W(Y-X\\beta)\\right] \\] Se halla la probabilidad conjunta matricialmente: \\[ L(Y;X\\beta)=\\prod_{i=1}^{n}\\dfrac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left[-\\dfrac{1}{2\\sigma^{2}}(Y-X\\beta)&#39;W(Y-X\\beta)\\right] \\] Simplificando la anterior expresión se llega a: \\[ L(Y;X\\beta)=(2\\pi\\sigma^{2})^{-N/2}-\\exp\\left[\\dfrac{1}{2\\sigma^{2}}(Y&#39;WY-Y&#39;WX\\beta-(X\\beta)&#39;WY+(X\\beta)&#39;WX\\beta)\\right] \\] Para poder derivar mejor, se aplica propiedades de los logaritmos: \\[ l(Y;X\\beta)=ln(2\\pi\\sigma^{2})^{-N/2}-\\dfrac{1}{2\\sigma^{2}}(Y&#39;WY-Y&#39;WX\\beta-(X\\beta)&#39;WY+(X\\beta)&#39;WX\\beta) \\] Maximizando el anterior resultado conoceremos el score \\(T\\): \\[ T=\\dfrac{\\partial l(Y;X\\beta)}{\\partial\\beta}=-\\dfrac{1}{2\\sigma^{2}}(-2X&#39;WY+2X&#39;WX\\beta) \\] Despejando \\(\\beta\\) tenemos el siguiente resultado: \\[ \\beta=(X&#39;WX)^{-1}(X&#39;Y) \\] Con este \\(\\beta\\) podemos estimar un modelo partiendo de una muestra probabilística compleja. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
